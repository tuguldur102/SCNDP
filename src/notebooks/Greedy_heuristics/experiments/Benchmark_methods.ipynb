{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BAri8EBVCB9q",
        "sng7Cbjdxrij",
        "BLqks9c_wBaT",
        "L4Cpj6RfxCw1",
        "M37DBjcexELc",
        "Tv8-rfGkwJEA",
        "tErgDZwTwOXg",
        "4qHJgTYZwc7R",
        "Y4PpC802wvkt",
        "2I4MqiHJwQJp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Assessing Attack Vulnerability in Networks with Uncertainty\n",
        "My Thai. Paper implementation - Benchmark methods"
      ],
      "metadata": {
        "id": "BAri8EBVCB9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Checks"
      ],
      "metadata": {
        "id": "EnE2jnjFxoVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "sng7Cbjdxrij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import linprog\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----- Core Sampling and Estimation -----\n",
        "def sample_realization(G: nx.Graph) -> nx.Graph:\n",
        "    H = nx.Graph()\n",
        "    H.add_nodes_from(G.nodes())\n",
        "    for u, v in G.edges():\n",
        "        if random.random() < G.edges[u, v]['p']:\n",
        "            H.add_edge(u, v)\n",
        "    return H\n",
        "\n",
        "def pairwise_connectivity(H: nx.Graph) -> float:\n",
        "    return sum(len(c) * (len(c) - 1) / 2 for c in nx.connected_components(H))\n",
        "\n",
        "def estimate_epc(G: nx.Graph, N: int) -> float:\n",
        "    n = G.number_of_nodes()\n",
        "    total = 0.0\n",
        "    for _ in range(N):\n",
        "        u = random.choice(list(G.nodes()))\n",
        "        visited = {u}\n",
        "        queue = [u]\n",
        "        while queue:\n",
        "            v = queue.pop(0)\n",
        "            for w in G.neighbors(v):\n",
        "                if w not in visited and random.random() < G.edges[v, w]['p']:\n",
        "                    visited.add(w)\n",
        "                    queue.append(w)\n",
        "        total += (len(visited) - 1)\n",
        "    return (n * total) / (2 * N)\n",
        "\n",
        "# ----- Heuristic Removals -----\n",
        "def remove_k_betweenness(G: nx.Graph, k: int) -> nx.Graph:\n",
        "    bc = nx.betweenness_centrality(G)\n",
        "    topk = sorted(bc, key=bc.get, reverse=True)[:k]\n",
        "    H = G.copy()\n",
        "    H.remove_nodes_from(topk)\n",
        "    return H\n",
        "\n",
        "def remove_k_pagerank_edges(G: nx.Graph, k: int) -> nx.Graph:\n",
        "    L = nx.line_graph(G)\n",
        "    pr = nx.pagerank(L)\n",
        "    topk = sorted(pr, key=pr.get, reverse=True)[:k]\n",
        "    H = G.copy()\n",
        "    H.remove_edges_from(topk)\n",
        "    return H\n",
        "\n",
        "# ----- Sample-Average Approximation (SAA) -----\n",
        "def sample_average_objective(G: nx.Graph, S: set, T: int) -> float:\n",
        "    total = 0.0\n",
        "    for _ in range(T):\n",
        "        H = sample_realization(G)\n",
        "        H.remove_nodes_from(S)\n",
        "        total += pairwise_connectivity(H)\n",
        "    return total / T\n",
        "\n",
        "def greedy_initial_SAA(G: nx.Graph, k: int, T: int) -> set:\n",
        "    S = set()\n",
        "    candidates = set(G.nodes())\n",
        "    for _ in range(k):\n",
        "        best_node, best_obj = None, float('inf')\n",
        "        for u in candidates:\n",
        "            obj = sample_average_objective(G, S | {u}, T)\n",
        "            if obj < best_obj:\n",
        "                best_node, best_obj = u, obj\n",
        "        S.add(best_node)\n",
        "        candidates.remove(best_node)\n",
        "    return S\n",
        "\n",
        "def SAA(G: nx.Graph, k: int, T: int) -> set:\n",
        "    S = greedy_initial_SAA(G, k, T)\n",
        "    improved = True\n",
        "    while improved:\n",
        "        improved = False\n",
        "        current_obj = sample_average_objective(G, S, T)\n",
        "        for u in list(S):\n",
        "            for v in set(G.nodes()) - S:\n",
        "                newS = (S - {u}) | {v}\n",
        "                new_obj = sample_average_objective(G, newS, T)\n",
        "                if new_obj < current_obj:\n",
        "                    S, improved, current_obj = newS, True, new_obj\n",
        "                    break\n",
        "            if improved:\n",
        "                break\n",
        "    return S\n",
        "\n",
        "# ----- LP Relaxation for REGA -----\n",
        "def solve_lp_relaxation(G: nx.Graph, D: set, k: int) -> dict:\n",
        "    nodes = list(G.nodes())\n",
        "    n = len(nodes)\n",
        "    edges = list(G.edges())\n",
        "    m = len(edges)\n",
        "\n",
        "    idx_s = {nodes[i]: i for i in range(n)}\n",
        "    idx_z = {edges[j]: n + j for j in range(m)}\n",
        "\n",
        "    bounds = [(0,1)] * (n + m)\n",
        "    for u in D:\n",
        "        bounds[idx_s[u]] = (1,1)\n",
        "\n",
        "    A_eq = np.zeros((1, n + m))\n",
        "    for u in nodes:\n",
        "        A_eq[0, idx_s[u]] = 1\n",
        "    b_eq = [k]\n",
        "\n",
        "    A_ub, b_ub = [], []\n",
        "    for (u, v) in edges:\n",
        "        iu, iv = idx_s[u], idx_s[v]\n",
        "        iz = idx_z[(u, v)]\n",
        "        row = np.zeros(n + m); row[iu] = 1; row[iz] = 1\n",
        "        A_ub.append(row); b_ub.append(1)\n",
        "        row = np.zeros(n + m); row[iv] = 1; row[iz] = 1\n",
        "        A_ub.append(row); b_ub.append(1)\n",
        "        row = np.zeros(n + m); row[iz] = -1; row[iu] = -1; row[iv] = -1\n",
        "        A_ub.append(row); b_ub.append(-1)\n",
        "    A_ub = np.array(A_ub); b_ub = np.array(b_ub)\n",
        "\n",
        "    c = np.zeros(n + m)\n",
        "    for j, (u, v) in enumerate(edges):\n",
        "        c[n + j] = G.edges[u, v]['p']\n",
        "\n",
        "    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq,\n",
        "                  bounds=bounds, method='highs')\n",
        "    if not res.success:\n",
        "        raise RuntimeError(\"LP infeasible: \" + res.message)\n",
        "    return {nodes[i]: float(res.x[idx_s[nodes[i]]]) for i in range(n)}\n",
        "\n",
        "def REGA_with_LP(G: nx.Graph, k: int, T_inner: int, R: int, alpha: float) -> set:\n",
        "    def sample_avg(S):\n",
        "        total = 0.0\n",
        "        for _ in range(T_inner):\n",
        "            H = sample_realization(G)\n",
        "            H.remove_nodes_from(S)\n",
        "            total += pairwise_connectivity(H)\n",
        "        return total / T_inner\n",
        "\n",
        "    best_S, best_obj = None, float('inf')\n",
        "    nodes = set(G.nodes())\n",
        "\n",
        "    for _ in range(R):\n",
        "        D = set()\n",
        "        for _ in range(k):\n",
        "            s_vals = solve_lp_relaxation(G, D, k)\n",
        "            rem = list(nodes - D)\n",
        "            sorted_nodes = sorted(rem, key=lambda u: s_vals[u], reverse=True)\n",
        "            m = max(1, int(alpha * len(sorted_nodes)))\n",
        "            D.add(random.choice(sorted_nodes[:m]))\n",
        "        current_obj = sample_avg(D)\n",
        "        improved = True\n",
        "        while improved:\n",
        "            improved = False\n",
        "            for u in list(D):\n",
        "                for v in nodes - D:\n",
        "                    newS = (D - {u}) | {v}\n",
        "                    val = sample_avg(newS)\n",
        "                    if val < current_obj:\n",
        "                        D, current_obj, improved = newS, val, True\n",
        "                        break\n",
        "                if improved:\n",
        "                    break\n",
        "        if current_obj < best_obj:\n",
        "            best_S, best_obj = D.copy(), current_obj\n",
        "    return best_S\n",
        "\n",
        "# ----- Experiment Runner including ALL algorithms -----\n",
        "def run_experiments(models, ps, k,\n",
        "                    T_saa, T_inner_rega, R_rega, alpha_rega, N_eval):\n",
        "    records = []\n",
        "    for name, G0 in tqdm(models.items(), desc=\"Running experiments\", total=len(models)):\n",
        "        for p in tqdm(ps, desc=f\"Model {name} with p\", total=len(ps)):\n",
        "            G = G0.copy()\n",
        "            for u, v in G.edges():\n",
        "                G.edges[u, v]['p'] = p\n",
        "\n",
        "            # Betweenness\n",
        "            t0 = time.perf_counter()\n",
        "            G_bc = remove_k_betweenness(G, k)\n",
        "            t_bc = time.perf_counter() - t0\n",
        "\n",
        "            epc_bc = estimate_epc(G_bc, N_eval)\n",
        "\n",
        "            # PageRank\n",
        "            t0 = time.perf_counter()\n",
        "            G_pr = remove_k_pagerank_edges(G, k)\n",
        "            t_pr = time.perf_counter() - t0\n",
        "\n",
        "            epc_pr = estimate_epc(G_pr, N_eval)\n",
        "\n",
        "            #SAA\n",
        "            t0 = time.perf_counter()\n",
        "            S_saa = SAA(G, k, T_saa)\n",
        "            t_saa = time.perf_counter() - t0\n",
        "\n",
        "            G_saa = G.copy(); G_saa.remove_nodes_from(S_saa)\n",
        "            epc_saa = estimate_epc(G_saa, N_eval)\n",
        "\n",
        "            # REGA-LP\n",
        "            t0 = time.perf_counter()\n",
        "            S_rega = REGA_with_LP(G, k, T_inner_rega, R_rega, alpha_rega)\n",
        "            t_rega = time.perf_counter() - t0\n",
        "\n",
        "            G_rega = G.copy(); G_rega.remove_nodes_from(S_rega)\n",
        "            epc_rega = estimate_epc(G_rega, N_eval)\n",
        "\n",
        "            for algo, t, e in [\n",
        "                ('Betweenness', t_bc, epc_bc),\n",
        "                ('PageRank',    t_pr, epc_pr),\n",
        "                ('SAA',         t_saa, epc_saa),\n",
        "                ('REGA-LP',     t_rega, epc_rega)\n",
        "            ]:\n",
        "                records.append({\n",
        "                    'model': name, 'p': p, 'algo': algo,\n",
        "                    'time': t, 'epc': e\n",
        "                })\n",
        "    return pd.DataFrame(records)"
      ],
      "metadata": {
        "id": "zQwKwyfc3Fc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ----- Define models and parameters -----\n",
        "models = {\n",
        "    'ER': nx.gnm_random_graph(50, 100, seed=42),\n",
        "    'BA': nx.barabasi_albert_graph(50, 2, seed=42),\n",
        "    'SW': nx.watts_strogatz_graph(50, 4, 0.3, seed=42),\n",
        "}\n",
        "\n",
        "ps = np.arange(0.0, 1.2, 0.2)\n",
        "k = 10\n",
        "T_saa = 30\n",
        "T_inner_rega = 1000    # inner-sample size for REGA\n",
        "R_rega = 5            # restarts for REGA\n",
        "alpha_rega = 0.2\n",
        "N_eval = 10000         # final EPC sample count (use 1e5 for paper-quality)\n",
        "\n",
        "# Execute experiments\n",
        "df = run_experiments(models, ps, k, T_saa, T_inner_rega, R_rega, alpha_rega, N_eval)\n",
        "\n",
        "# Plot EPC vs p and Time vs p\n",
        "for name in models:\n",
        "    plt.figure()\n",
        "    for algo in df.algo.unique():\n",
        "        sub = df[(df.model == name) & (df.algo == algo)]\n",
        "        plt.plot(sub.p, sub.epc, label=algo)\n",
        "    plt.title(f\"{name} — EPC vs p\")\n",
        "    plt.xlabel(\"p\"); plt.ylabel(\"EPC\"); plt.grid(True); plt.legend()\n",
        "    plt.savefig(f\"{name}_epc_vs_p.png\")\n",
        "\n",
        "    plt.figure()\n",
        "    for algo in df.algo.unique():\n",
        "        sub = df[(df.model == name) & (df.algo == algo)]\n",
        "        plt.plot(sub.p, sub.time, label=algo)\n",
        "    plt.title(f\"{name} — Time vs p\")\n",
        "    plt.xlabel(\"p\"); plt.ylabel(\"Time (s)\"); plt.grid(True); plt.legend()\n",
        "    plt.savefig(f\"{name}_time_vs_p.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RIt72Bd0v4lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Empty-set Greedy Heuristics"
      ],
      "metadata": {
        "id": "BLqks9c_wBaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check"
      ],
      "metadata": {
        "id": "L4Cpj6RfxCw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all sanity-check cases for the stochastic CNDP implementation\n",
        "# This block re‑defines the minimal functions needed, so it is fully\n",
        "# self‑contained (no need for an external stochastic_cndp.py file).\n",
        "\n",
        "import importlib, subprocess, sys, math, itertools, random, time, json, os, collections\n",
        "\n",
        "# install networkx & matplotlib if missing\n",
        "try:\n",
        "    import networkx as nx\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"networkx\"])\n",
        "    import networkx as nx\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"numpy\"])\n",
        "    import numpy as np\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"matplotlib\"])\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------- Helper functions ----------------\n",
        "def pairwise_cost_det(G_det):\n",
        "    cost = 0\n",
        "    for comp in nx.connected_components(G_det):\n",
        "        s = len(comp)\n",
        "        cost += s * (s - 1) // 2\n",
        "    return cost\n",
        "\n",
        "def expected_pairwise_connectivity(G, S=set(), num_samples=10000, rng=None):\n",
        "    if rng is None:\n",
        "        rng = random.Random()\n",
        "    remaining = set(G.nodes()) - set(S)\n",
        "    if not remaining:\n",
        "        return 0.0\n",
        "    total = 0\n",
        "    edges = list(G.edges(data=True))\n",
        "    for _ in range(num_samples):\n",
        "        H = nx.Graph()\n",
        "        H.add_nodes_from(remaining)\n",
        "        for u, v, d in edges:\n",
        "            if u in S or v in S:\n",
        "                continue\n",
        "            if rng.random() < d[\"prob\"]:\n",
        "                H.add_edge(u, v)\n",
        "        total += pairwise_cost_det(H)\n",
        "    return total / num_samples\n",
        "\n",
        "def greedy_cndp(G, K, num_samples=2000, seed=None):\n",
        "    rng = random.Random(seed)\n",
        "    S = set()\n",
        "    sigmas = [expected_pairwise_connectivity(G, S, num_samples, rng)]\n",
        "    for _ in range(K):\n",
        "        best_v, best_sigma = None, float(\"inf\")\n",
        "        for v in set(G.nodes()) - S:\n",
        "            sigma_v = expected_pairwise_connectivity(G, S | {v}, num_samples, rng)\n",
        "            if sigma_v < best_sigma:\n",
        "                best_sigma, best_v = sigma_v, v\n",
        "        S.add(best_v)\n",
        "        sigmas.append(best_sigma)\n",
        "    return S, sigmas\n",
        "\n",
        "# ------------------ Check 1: Edge‑count distribution ------------------\n",
        "print(\"CHECK 1: Edge‑count distribution in live‑edge samples\")\n",
        "# G_small = nx.complete_graph(5)\n",
        "G_small = nx.path_graph(4)\n",
        "for u, v in G_small.edges():\n",
        "    G_small[u][v][\"prob\"] = 0.7   # constant probability\n",
        "\n",
        "rng = random.Random(0)\n",
        "edge_counts = []\n",
        "for _ in range(20000):\n",
        "    H = nx.Graph()\n",
        "    H.add_nodes_from(G_small.nodes())\n",
        "    for u, v, d in G_small.edges(data=True):\n",
        "        if rng.random() < d[\"prob\"]:\n",
        "            H.add_edge(u, v)\n",
        "    edge_counts.append(H.number_of_edges())\n",
        "\n",
        "mean_edges = np.mean(edge_counts)\n",
        "expected_mean = 0.3 * G_small.number_of_edges()\n",
        "print(f\"  Sample mean edges = {mean_edges:.3f} (expected {expected_mean:.3f})\")\n",
        "\n",
        "# plot histogram (single plot as required)\n",
        "plt.figure()\n",
        "plt.hist(edge_counts, bins=range(0, G_small.number_of_edges() + 2), rwidth=0.9)\n",
        "plt.xlabel(\"edges in one live‑edge sample\")\n",
        "plt.ylabel(\"frequency (20 000 samples)\")\n",
        "plt.title(\"Check 1: Edge‑count distribution\")\n",
        "plt.show()\n",
        "\n",
        "# ------------------ Check 2: Exact vs Monte‑Carlo σ ------------------\n",
        "print(\"\\nCHECK 2: Exact σ vs Monte‑Carlo σ on tiny graph\")\n",
        "def sigma_exact(G, S=set()):\n",
        "    rem = set(G.nodes()) - set(S)\n",
        "    edges = list(G.edges())\n",
        "    total = 0.0\n",
        "    for mask in range(1 << len(edges)):\n",
        "        H = nx.Graph(); H.add_nodes_from(rem)\n",
        "        p_scenario = 1.0\n",
        "        for bit, (u, v) in enumerate(edges):\n",
        "            p = G[u][v][\"prob\"]\n",
        "            chosen = (mask >> bit) & 1\n",
        "            if chosen and u not in S and v not in S:\n",
        "                H.add_edge(u, v)\n",
        "                p_scenario *= p\n",
        "            else:\n",
        "                p_scenario *= (1 - p)\n",
        "        total += p_scenario * pairwise_cost_det(H)\n",
        "    return total\n",
        "\n",
        "exact_sigma = sigma_exact(G_small, set())\n",
        "mc_sigma = expected_pairwise_connectivity(G_small, set(), num_samples=20000, rng=rng)\n",
        "print(f\"  Exact σ = {exact_sigma:.4f}\")\n",
        "print(f\"  Monte‑Carlo σ (20 000 samples) = {mc_sigma:.4f}\")\n",
        "print(f\"  Relative error = {(mc_sigma - exact_sigma) / exact_sigma * 100:.2f}%\")\n",
        "\n",
        "# ------------------ Check 3: Deterministic extremes ------------------\n",
        "print(\"\\nCHECK 3: Deterministic extremes (all prob = 0 or 1)\")\n",
        "G_zero = nx.path_graph(6)\n",
        "for u, v in G_zero.edges():\n",
        "    G_zero[u][v][\"prob\"] = 0.0\n",
        "sigma_zero = expected_pairwise_connectivity(G_zero, set(), 5000, rng)\n",
        "print(f\"  All probs = 0 ⇒ σ = {sigma_zero} (should be 0)\")\n",
        "\n",
        "G_one = nx.path_graph(6)\n",
        "for u, v in G_one.edges():\n",
        "    G_one[u][v][\"prob\"] = 1.0\n",
        "sigma_one_mc = expected_pairwise_connectivity(G_one, set(), 5000, rng)\n",
        "sigma_one_exact = pairwise_cost_det(G_one)\n",
        "print(f\"  All probs = 1 ⇒ MC σ ≈ {sigma_one_mc:.1f}, exact deterministic σ = {sigma_one_exact}\")\n",
        "\n",
        "# ------------------ Check 4: Greedy monotonicity ------------------\n",
        "print(\"\\nCHECK 4: Greedy monotonicity (σ must not increase)\")\n",
        "G_test = nx.erdos_renyi_graph(40, 0.07, seed=1)\n",
        "for u, v in G_test.edges():\n",
        "    G_test[u][v][\"prob\"] = random.uniform(0.1, 0.9)\n",
        "S_sel, sigma_seq = greedy_cndp(G_test, K=6, num_samples=1000, seed=1)\n",
        "monotone = all(sigma_seq[i] >= sigma_seq[i+1] for i in range(len(sigma_seq)-1))\n",
        "print(f\"  σ sequence: {['{:.0f}'.format(s) for s in sigma_seq]}\")\n",
        "print(f\"  Monotone non‑increasing? {monotone}\")\n",
        "\n",
        "# ------------------ Check 5: Replicability with fixed seed ------------------\n",
        "print(\"\\nCHECK 5: Replicability (same seed ⇒ same result)\")\n",
        "S1, sig1 = greedy_cndp(G_test, K=4, num_samples=500, seed=123)\n",
        "S2, sig2 = greedy_cndp(G_test, K=4, num_samples=500, seed=123)\n",
        "print(f\"  Run 1 selected nodes: {sorted(S1)}\")\n",
        "print(f\"  Run 2 selected nodes: {sorted(S2)}\")\n",
        "print(f\"  Identical selections? {sorted(S1)==sorted(S2)}\")\n",
        "print(f\"  σ sequences identical? {np.allclose(sig1, sig2)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rtaEayBXxGO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "M37DBjcexELc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "stochastic_cndp.py\n",
        "------------------\n",
        "Heuristic solver for the Stochastic Critical-Node Detection Problem (CNDP)\n",
        "with *edge* uncertainty, as defined in Stochastic_CNDP.pdf.\n",
        "\n",
        "Implements:\n",
        "  • Algorithm 1  – Monte-Carlo estimator of σ(S)\n",
        "  • Algorithm 2  – greedy attacker using that estimator\n",
        "Supports:\n",
        "  • Erdős–Rényi G(n,p) graphs\n",
        "  • Watts–Strogatz Watts–Strogatz(n, k, β) graphs\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import random, itertools, math, time\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1.  GRAPH BUILDING UTILITIES\n",
        "# ------------------------------------------------------------\n",
        "def er_prob_graph(n: int, p_edge: float,\n",
        "                  p_low: float=0.1, p_high: float=0.9,\n",
        "                  seed: int|None=None) -> nx.Graph:\n",
        "    \"\"\"G(n,p_edge) with iid edge-existence probs U(p_low,p_high).\"\"\"\n",
        "    rng = random.Random(seed)\n",
        "    G = nx.erdos_renyi_graph(n, p_edge, seed=seed)\n",
        "    for u, v in G.edges():\n",
        "        G[u][v][\"prob\"] = rng.uniform(p_low, p_high)\n",
        "    return G\n",
        "\n",
        "\n",
        "def ws_prob_graph(n: int, k: int, beta: float,\n",
        "                  p_low: float=0.1, p_high: float=0.9,\n",
        "                  seed: int|None=None) -> nx.Graph:\n",
        "    \"\"\"Watts–Strogatz small-world graph with iid edge probs.\"\"\"\n",
        "    rng = random.Random(seed)\n",
        "    G = nx.watts_strogatz_graph(n, k, beta, seed=seed)\n",
        "    for u, v in G.edges():\n",
        "        G[u][v][\"prob\"] = rng.uniform(p_low, p_high)\n",
        "    return G\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2.  PAIRWISE CONNECTIVITY ON ONE *SCENARIO*\n",
        "# ------------------------------------------------------------\n",
        "def pairwise_cost_det(G_det: nx.Graph) -> int:\n",
        "    \"\"\"Σ_{components} |C|·(|C|-1)/2 – deterministic definition\n",
        "    used inside the Monte-Carlo loop.\"\"\"\n",
        "    cost = 0\n",
        "    for comp in nx.connected_components(G_det):\n",
        "        s = len(comp)\n",
        "        cost += s * (s - 1) // 2\n",
        "    return cost\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3.  ALGORITHM 1 –  MONTE-CARLO ESTIMATE  σ(S)\n",
        "# ------------------------------------------------------------\n",
        "def expected_pairwise_connectivity(\n",
        "    G: nx.Graph,\n",
        "    S: set[int] | set[str],\n",
        "    num_samples: int = 10_000,\n",
        "    rng: random.Random | None = None,\n",
        "    show_bar: bool = True,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Monte-Carlo estimator of σ(S) from Algorithm 1​ :contentReference[oaicite:0]{index=0}\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = random.Random()\n",
        "\n",
        "    remaining_nodes = set(G.nodes()) - S\n",
        "    if not remaining_nodes:\n",
        "        return 0.0\n",
        "\n",
        "    total_cost = 0\n",
        "    iterator = range(num_samples)\n",
        "    if show_bar:\n",
        "        iterator = tqdm(iterator, desc=\"MC-samples\", leave=False)\n",
        "\n",
        "    for _ in iterator:\n",
        "        # sample a *live-edge* scenario\n",
        "        H = nx.Graph()\n",
        "        H.add_nodes_from(remaining_nodes)\n",
        "\n",
        "        for u, v, data in G.edges(data=True):\n",
        "            if u in S or v in S:\n",
        "                continue\n",
        "            if rng.random() < data[\"prob\"]:\n",
        "                H.add_edge(u, v)\n",
        "\n",
        "        total_cost += pairwise_cost_det(H)\n",
        "\n",
        "    return total_cost / num_samples\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4.  ALGORITHM 2 –  GREEDY ATTACKER\n",
        "# ------------------------------------------------------------\n",
        "def greedy_cndp(\n",
        "    G: nx.Graph,\n",
        "    K: int,\n",
        "    num_samples: int = 2_000,\n",
        "    seed: int | None = None,\n",
        ") -> tuple[set[int], list[float]]:\n",
        "    \"\"\"Return (selected_set, σ values after each pick).\"\"\"\n",
        "    rng = random.Random(seed)\n",
        "    S: set[int] = set()\n",
        "    sigmas: list[float] = []\n",
        "\n",
        "    current_sigma = expected_pairwise_connectivity(G, S, num_samples, rng)\n",
        "    sigmas.append(current_sigma)\n",
        "\n",
        "    for _ in range(K):\n",
        "        best_node, best_sigma = None, float(\"inf\")\n",
        "\n",
        "        for v in (set(G.nodes()) - S):\n",
        "            sigma_v = expected_pairwise_connectivity(G, S | {v},\n",
        "                                                     num_samples, rng, False)\n",
        "            if sigma_v < best_sigma:\n",
        "                best_sigma, best_node = sigma_v, v\n",
        "\n",
        "        S.add(best_node)                # exploit 1-step look-ahead\n",
        "        current_sigma = best_sigma\n",
        "        sigmas.append(current_sigma)\n",
        "        print(f\"● Picked {best_node:>3};   σ = {current_sigma:.1f}\")\n",
        "\n",
        "    return S, sigmas\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5.  QUICK DRIVER FOR EXPERIMENTS\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # import argparse, json, pathlib\n",
        "\n",
        "    # parser = argparse.ArgumentParser(\n",
        "    #     description=\"Greedy heuristic for stochastic CNDP\")\n",
        "    # parser.add_argument(\"--model\", choices=[\"er\", \"ws\"], default=\"er\")\n",
        "    # parser.add_argument(\"-n\", type=int, default=100,\n",
        "    #                     help=\"number of nodes\")\n",
        "    # parser.add_argument(\"--p\", type=float, default=0.05,\n",
        "    #                     help=\"edge probability for ER (or rewiring β for WS)\")\n",
        "    # parser.add_argument(\"--k\", type=int, default=4,\n",
        "    #                     help=\"nearest-neighbour degree in WS\")\n",
        "    # parser.add_argument(\"--budget\", \"-K\", type=int, default=5)\n",
        "    # parser.add_argument(\"--samples\", type=int, default=2000)\n",
        "    # parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    model = \"er\"\n",
        "    n = 30\n",
        "    p = 0.2\n",
        "    seed = 42\n",
        "    budget = 10\n",
        "    samples = 10000\n",
        "    k = 4  # only used for WS model\n",
        "\n",
        "    if model == \"er\":\n",
        "        G = er_prob_graph(n, p, seed=seed)\n",
        "    else:\n",
        "        G = ws_prob_graph(n, k, p, seed=seed)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    S_star, sigmas = greedy_cndp(G, budget,\n",
        "                                 num_samples=samples,\n",
        "                                 seed=seed)\n",
        "    elapsed = time.perf_counter() - start\n",
        "    print(\"\\n=====  RESULT  =====\")\n",
        "    print(f\"Removed nodes: {sorted(S_star)}\")\n",
        "    print(\"σ after each pick:\", [round(x, 1) for x in sigmas])\n",
        "    print(f\"Elapsed: {elapsed:.1f} s\")\n"
      ],
      "metadata": {
        "id": "zNHLdvvIwH3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prof. Ashwin algorithm"
      ],
      "metadata": {
        "id": "Tv8-rfGkwJEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check"
      ],
      "metadata": {
        "id": "tErgDZwTwOXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, itertools, math, networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- Helper functions ----------\n",
        "\n",
        "def threshold_graph(G_prob, tau=0.5):\n",
        "    \"\"\"Return deterministic skeleton keeping only edges whose 'prob' >= tau.\"\"\"\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(G_prob.nodes())\n",
        "    for u, v, data in G_prob.edges(data=True):\n",
        "        if data[\"prob\"] >= tau:\n",
        "            G.add_edge(u, v)\n",
        "    return G\n",
        "\n",
        "def greedy_mis(G):\n",
        "    \"\"\"Simple degree-ascending greedy maximal independent set.\"\"\"\n",
        "    mis = set()\n",
        "    for v in sorted(G.nodes(), key=G.degree):\n",
        "        if all(n not in mis for n in G.neighbors(v)):\n",
        "            mis.add(v)\n",
        "    return mis\n",
        "\n",
        "def pairwise_cost_det(G):\n",
        "    \"\"\"Deterministic connectivity cost Σ σ(σ-1)/2 over components.\"\"\"\n",
        "    cost = 0\n",
        "    for comp in nx.connected_components(G):\n",
        "        s = len(comp)\n",
        "        cost += s*(s-1)//2\n",
        "    return cost\n",
        "\n",
        "def arulselvan_cndp(G_det, k):\n",
        "    \"\"\"Return deletion set of size k using MIS + greedy augmentation.\"\"\"\n",
        "    mis = greedy_mis(G_det)\n",
        "    # Greedy augmentation\n",
        "    while len(mis) != G_det.number_of_nodes() - k:\n",
        "        best_phi, best_v = math.inf, None\n",
        "        for v in set(G_det.nodes()) - mis:\n",
        "            # compute phi if we keep v (delete others)\n",
        "            temp_keep = mis | {v}\n",
        "            H = G_det.subgraph(temp_keep)\n",
        "            phi = pairwise_cost_det(H)\n",
        "            if phi < best_phi:\n",
        "                best_phi, best_v = phi, v\n",
        "        mis.add(best_v)\n",
        "    deletion_set = set(G_det.nodes()) - mis\n",
        "    return deletion_set\n",
        "\n",
        "def expected_pairwise_cost(G_prob, removed_nodes=None, T=200, seed=0):\n",
        "    \"\"\"Monte Carlo estimate of expected pairwise connectivity after removing nodes.\"\"\"\n",
        "    if removed_nodes is None:\n",
        "        removed_nodes = set()\n",
        "    rng = random.Random(seed)\n",
        "    nodes_kept = [n for n in G_prob.nodes() if n not in removed_nodes]\n",
        "    if not nodes_kept:   # no nodes remain\n",
        "        return 0.0\n",
        "    acc = 0.0\n",
        "    edges_with_prob = [(u, v, data[\"prob\"]) for u, v, data in G_prob.edges(data=True)]\n",
        "    for _ in range(T):\n",
        "        Gs = nx.Graph()\n",
        "        Gs.add_nodes_from(nodes_kept)\n",
        "        for u, v, p in edges_with_prob:\n",
        "            if u in removed_nodes or v in removed_nodes:\n",
        "                continue\n",
        "            if rng.random() < p:\n",
        "                Gs.add_edge(u, v)\n",
        "        acc += pairwise_cost_det(Gs)\n",
        "    return acc / T\n",
        "\n",
        "# ---------- 1. Toy Example (6 nodes) -------------\n",
        "\n",
        "# manual edge list with probabilities\n",
        "toy_edges = [\n",
        "    (1, 2, 0.80),\n",
        "    (1, 3, 0.48),\n",
        "    (2, 3, 0.70),\n",
        "    (2, 4, 0.25),\n",
        "    (3, 4, 0.60),\n",
        "    (3, 5, 0.30),\n",
        "    (4, 5, 0.65),\n",
        "    (4, 6, 0.90),\n",
        "    (5, 6, 0.35),\n",
        "]\n",
        "G_toy_prob = nx.Graph()\n",
        "G_toy_prob.add_nodes_from(range(1, 7))\n",
        "for u, v, p in toy_edges:\n",
        "    G_toy_prob.add_edge(u, v, prob=p)\n",
        "\n",
        "tau = 0.5\n",
        "k_toy = 2\n",
        "G_toy_det = threshold_graph(G_toy_prob, tau)\n",
        "del_toy = arulselvan_cndp(G_toy_det, k_toy)\n",
        "\n",
        "epc_before_toy = expected_pairwise_cost(G_toy_prob, removed_nodes=set(), T=500, seed=42)\n",
        "epc_after_toy  = expected_pairwise_cost(G_toy_prob, removed_nodes=del_toy, T=500, seed=42)\n",
        "\n",
        "# ---------- 2. Medium Example (100 nodes, 200 edges) -------------\n",
        "\n",
        "random.seed(1)\n",
        "n, m, k_medium = 50, 100, 15\n",
        "G_med_det_base = nx.gnm_random_graph(n, m, seed=2)  # deterministic backbone\n",
        "\n",
        "# assign heterogeneous per-edge reliability ~ Uniform[0.1,1]\n",
        "G_med_prob = nx.Graph()\n",
        "G_med_prob.add_nodes_from(G_med_det_base.nodes())\n",
        "for u, v in G_med_det_base.edges():\n",
        "    G_med_prob.add_edge(u, v, prob=random.uniform(0.1, 1.0))\n",
        "\n",
        "tau_med = 0.5\n",
        "G_med_det = threshold_graph(G_med_prob, tau_med)\n",
        "del_med = arulselvan_cndp(G_med_det, k_medium)\n",
        "\n",
        "epc_before_med = expected_pairwise_cost(G_med_prob, set(), T=200, seed=24)\n",
        "epc_after_med  = expected_pairwise_cost(G_med_prob, del_med, T=200, seed=24)\n",
        "\n",
        "# ---------- Summaries -------------\n",
        "\n",
        "summary_toy = {\n",
        "    \"Example\": [\"Toy (n=6)\"],\n",
        "    \"Threshold τ\": [tau],\n",
        "    \"k deletions\": [k_toy],\n",
        "    \"Deletion set\": [sorted(del_toy)],\n",
        "    \"EPC before\": [round(epc_before_toy, 2)],\n",
        "    \"EPC after\": [round(epc_after_toy, 2)]\n",
        "}\n",
        "\n",
        "summary_med = {\n",
        "    \"Example\": [\"Medium (n=100)\"],\n",
        "    \"Threshold τ\": [tau_med],\n",
        "    \"k deletions\": [k_medium],\n",
        "    \"Deletion set\": [sorted(del_med)[:5] + [\"...\"]],  # truncate for display\n",
        "    \"EPC before\": [round(epc_before_med, 1)],\n",
        "    \"EPC after\": [round(epc_after_med, 1)]\n",
        "}\n",
        "\n",
        "df = pd.concat([pd.DataFrame(summary_toy), pd.DataFrame(summary_med)], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "fzSdJccPwqGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, itertools, math, networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- Helper functions ----------\n",
        "\n",
        "def threshold_graph(G_prob, tau=0.5):\n",
        "    \"\"\"Return deterministic skeleton keeping only edges whose 'prob' >= tau.\"\"\"\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(G_prob.nodes())\n",
        "    for u, v, data in G_prob.edges(data=True):\n",
        "        if data[\"prob\"] >= tau:\n",
        "            G.add_edge(u, v)\n",
        "    return G\n",
        "\n",
        "def greedy_mis(G):\n",
        "    \"\"\"Simple degree-ascending greedy maximal independent set.\"\"\"\n",
        "    mis = set()\n",
        "    for v in sorted(G.nodes(), key=G.degree):\n",
        "        if all(n not in mis for n in G.neighbors(v)):\n",
        "            mis.add(v)\n",
        "    return mis\n",
        "\n",
        "def pairwise_cost_det(G):\n",
        "    \"\"\"Deterministic connectivity cost Σ σ(σ-1)/2 over components.\"\"\"\n",
        "    cost = 0\n",
        "    for comp in nx.connected_components(G):\n",
        "        s = len(comp)\n",
        "        cost += s*(s-1)//2\n",
        "    return cost\n",
        "\n",
        "def arulselvan_cndp(G_det, k):\n",
        "    \"\"\"Return deletion set of size k using MIS + greedy augmentation.\"\"\"\n",
        "    mis = greedy_mis(G_det)\n",
        "    # Greedy augmentation\n",
        "    while len(mis) != G_det.number_of_nodes() - k:\n",
        "        best_phi, best_v = math.inf, None\n",
        "        for v in set(G_det.nodes()) - mis:\n",
        "            # compute phi if we keep v (delete others)\n",
        "            temp_keep = mis | {v}\n",
        "            H = G_det.subgraph(temp_keep)\n",
        "            phi = pairwise_cost_det(H)\n",
        "            if phi < best_phi:\n",
        "                best_phi, best_v = phi, v\n",
        "        mis.add(best_v)\n",
        "    deletion_set = set(G_det.nodes()) - mis\n",
        "    return deletion_set\n",
        "\n",
        "def expected_pairwise_cost(G_prob, removed_nodes=None, T=200, seed=0):\n",
        "    \"\"\"Monte Carlo estimate of expected pairwise connectivity after removing nodes.\"\"\"\n",
        "    if removed_nodes is None:\n",
        "        removed_nodes = set()\n",
        "    rng = random.Random(seed)\n",
        "    nodes_kept = [n for n in G_prob.nodes() if n not in removed_nodes]\n",
        "    if not nodes_kept:   # no nodes remain\n",
        "        return 0.0\n",
        "    acc = 0.0\n",
        "    edges_with_prob = [(u, v, data[\"prob\"]) for u, v, data in G_prob.edges(data=True)]\n",
        "    for _ in range(T):\n",
        "        Gs = nx.Graph()\n",
        "        Gs.add_nodes_from(nodes_kept)\n",
        "        for u, v, p in edges_with_prob:\n",
        "            if u in removed_nodes or v in removed_nodes:\n",
        "                continue\n",
        "            if rng.random() < p:\n",
        "                Gs.add_edge(u, v)\n",
        "        acc += pairwise_cost_det(Gs)\n",
        "    return acc / T\n",
        "\n",
        "# ---------- 1. Toy Example (6 nodes) -------------\n",
        "\n",
        "# manual edge list with probabilities\n",
        "toy_edges = [\n",
        "    (1, 2, 0.80),\n",
        "    (1, 3, 0.48),\n",
        "    (2, 3, 0.70),\n",
        "    (2, 4, 0.25),\n",
        "    (3, 4, 0.60),\n",
        "    (3, 5, 0.30),\n",
        "    (4, 5, 0.65),\n",
        "    (4, 6, 0.90),\n",
        "    (5, 6, 0.35),\n",
        "]\n",
        "G_toy_prob = nx.Graph()\n",
        "G_toy_prob.add_nodes_from(range(1, 7))\n",
        "for u, v, p in toy_edges:\n",
        "    G_toy_prob.add_edge(u, v, prob=p)\n",
        "\n",
        "tau = 0.5\n",
        "k_toy = 2\n",
        "G_toy_det = threshold_graph(G_toy_prob, tau)\n",
        "del_toy = arulselvan_cndp(G_toy_det, k_toy)\n",
        "\n",
        "epc_before_toy = expected_pairwise_cost(G_toy_prob, removed_nodes=set(), T=500, seed=42)\n",
        "epc_after_toy  = expected_pairwise_cost(G_toy_prob, removed_nodes=del_toy, T=500, seed=42)\n",
        "\n",
        "# ---------- 2. Medium Example (100 nodes, 200 edges) -------------\n",
        "\n",
        "random.seed(1)\n",
        "n, m, k_medium = 50, 100, 15\n",
        "G_med_det_base = nx.gnm_random_graph(n, m, seed=2)  # deterministic backbone\n",
        "\n",
        "# assign heterogeneous per-edge reliability ~ Uniform[0.1,1]\n",
        "G_med_prob = nx.Graph()\n",
        "G_med_prob.add_nodes_from(G_med_det_base.nodes())\n",
        "for u, v in G_med_det_base.edges():\n",
        "    G_med_prob.add_edge(u, v, prob=random.uniform(0.1, 1.0))\n",
        "\n",
        "tau_med = 0.5\n",
        "G_med_det = threshold_graph(G_med_prob, tau_med)\n",
        "del_med = arulselvan_cndp(G_med_det, k_medium)\n",
        "\n",
        "epc_before_med = expected_pairwise_cost(G_med_prob, set(), T=200, seed=24)\n",
        "epc_after_med  = expected_pairwise_cost(G_med_prob, del_med, T=200, seed=24)\n",
        "\n",
        "# ---------- Summaries -------------\n",
        "\n",
        "summary_toy = {\n",
        "    \"Example\": [\"Toy (n=6)\"],\n",
        "    \"Threshold τ\": [tau],\n",
        "    \"k deletions\": [k_toy],\n",
        "    \"Deletion set\": [sorted(del_toy)],\n",
        "    \"EPC before\": [round(epc_before_toy, 2)],\n",
        "    \"EPC after\": [round(epc_after_toy, 2)]\n",
        "}\n",
        "\n",
        "summary_med = {\n",
        "    \"Example\": [\"Medium (n=100)\"],\n",
        "    \"Threshold τ\": [tau_med],\n",
        "    \"k deletions\": [k_medium],\n",
        "    \"Deletion set\": [sorted(del_med)[:5] + [\"...\"]],  # truncate for display\n",
        "    \"EPC before\": [round(epc_before_med, 1)],\n",
        "    \"EPC after\": [round(epc_after_med, 1)]\n",
        "}\n",
        "\n",
        "df = pd.concat([pd.DataFrame(summary_toy), pd.DataFrame(summary_med)], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "xmtzNHWCwMV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "Lftv98bjx1LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Centrality-based heuristics"
      ],
      "metadata": {
        "id": "4qHJgTYZwc7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check"
      ],
      "metadata": {
        "id": "bvlzyqX5x5_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "URU23nacx7uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Analysis: Grid-fixed probability scenario (My Thai.) vs Draws from uniform[0.0, 1.0]"
      ],
      "metadata": {
        "id": "Y4PpC802wvkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. tiny 5-node ER graph, every p_uv = 0.3\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = random.Random(seed)\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from([0, 1, 2, 3])  # include node 3 as it's used in an edge\n",
        "G.add_edge(0, 1, prob=0.7)\n",
        "G.add_edge(1, 2, prob=0.7)\n",
        "G.add_edge(2, 3, prob=0.7)\n",
        "\n",
        "\n",
        "\n",
        "# G = nx.complete_graph(4)\n",
        "# G = nx.path_graph(4)  # 0-1-2-3\n",
        "# for u, v in G.edges():\n",
        "#     G[u][v][\"prob\"] = rng.uniform(0.0, 1.0)        # constant prob ⇒ analytic mean = 0.3 * 10 = 3\n",
        "    # G[u][v][\"prob\"] = 0.7\n",
        "\n",
        "print(G.nodes())\n",
        "print(G.edges(data=True))\n",
        "\n",
        "rng = random.Random(0)\n",
        "edge_counts = []\n",
        "for _ in range(20_000):\n",
        "    H = nx.Graph()\n",
        "    H.add_nodes_from(G.nodes())\n",
        "    for u, v, d in G.edges(data=True):\n",
        "        if rng.random() < d[\"prob\"]:\n",
        "            H.add_edge(u, v)\n",
        "    edge_counts.append(H.number_of_edges())\n",
        "\n",
        "print(\"Sample mean edges:\", np.mean(edge_counts))  # ~ 2.99 → close to 3\n",
        "\n",
        "plt.hist(edge_counts, bins=range(11), rwidth=0.9)\n",
        "plt.xlabel(\"edges in one sample\"); plt.ylabel(\"frequency\"); plt.title(\"Live-edge distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "joCSQhr0w35a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ground-truth by enumeration (only 10 edges → 1024 scenarios)\n",
        "def sigma_exact(G, S):\n",
        "    rem = set(G.nodes()) - S\n",
        "    total = 0\n",
        "    E = list(G.edges())\n",
        "    for mask in range(1 << len(E)):\n",
        "        H = nx.Graph(); H.add_nodes_from(rem)\n",
        "        p = 1.0\n",
        "        for bit, (u, v) in enumerate(E):\n",
        "            q = G[u][v][\"prob\"]\n",
        "            choose = (mask >> bit) & 1\n",
        "            if choose and u not in S and v not in S:\n",
        "                H.add_edge(u, v)\n",
        "                p *= q\n",
        "            else:\n",
        "                p *= (1-q)\n",
        "        total += p * pairwise_cost_det(H)\n",
        "    return total\n",
        "\n",
        "exact = sigma_exact(G, S=set())\n",
        "mc    = expected_pairwise_connectivity(G, S=set(), num_samples=100000, rng=rng)\n",
        "print(\"Exact σ:\", exact, \"   Monte-Carlo σ:\", mc)\n"
      ],
      "metadata": {
        "id": "ifFHhVNPw68m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. All benchmark methods comparision"
      ],
      "metadata": {
        "id": "2I4MqiHJwQJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments_with_threshold(models, ps, k,\n",
        "                                   T_saa, T_rega, R_rega, alpha,\n",
        "                                   tau, N_eval):\n",
        "    records = []\n",
        "    for name, G0 in tqdm(models.items(), desc=\"Running experiments\", total=len(models)):\n",
        "        for p in tqdm(ps, desc=f\"Model {name} with p\", total=len(ps)):\n",
        "            # 1. build probabilistic graph\n",
        "            G = G0.copy()\n",
        "            nx.set_edge_attributes(G, {e: p for e in G.edges()}, 'p')\n",
        "\n",
        "            # 2. Betweenness\n",
        "            t0 = time.perf_counter()\n",
        "            H_bc = remove_k_betweenness(G, k)\n",
        "            t_bc = time.perf_counter() - t0\n",
        "            epc_bc = estimate_epc(H_bc, N_eval)\n",
        "\n",
        "            # 3. PageRank\n",
        "            t0 = time.perf_counter()\n",
        "            H_pr = remove_k_pagerank_edges(G, k)\n",
        "            t_pr = time.perf_counter() - t0\n",
        "            epc_pr = estimate_epc(H_pr, N_eval)\n",
        "\n",
        "            # 4. SAA\n",
        "            t0 = time.perf_counter()\n",
        "            S_saa = SAA(G, k, T_saa)\n",
        "            t_saa = time.perf_counter() - t0\n",
        "            H_saa = G.copy(); H_saa.remove_nodes_from(S_saa)\n",
        "            epc_saa = estimate_epc(H_saa, N_eval)\n",
        "\n",
        "            # 5. REGA\n",
        "            t0 = time.perf_counter()\n",
        "            S_rega = REGA_with_LP(G, k, T_rega, R_rega, alpha)\n",
        "            t_rega = time.perf_counter() - t0\n",
        "            H_rega = G.copy(); H_rega.remove_nodes_from(S_rega)\n",
        "            epc_rega = estimate_epc(H_rega, N_eval)\n",
        "\n",
        "            # 6. Threshold + MIS (Arulselvan et al.)\n",
        "            t0 = time.perf_counter()\n",
        "            G_det = threshold_graph(G, tau)\n",
        "            del_th = arulselvan_cndp(G_det, k)\n",
        "            t_th = time.perf_counter() - t0\n",
        "            H_th = G.copy(); H_th.remove_nodes_from(del_th)\n",
        "            epc_th = estimate_epc(H_th, N_eval)\n",
        "\n",
        "            # 7. collect records for all 5\n",
        "            for algo, t, epc in [\n",
        "                ('Betweenness', t_bc, epc_bc),\n",
        "                ('PageRank',    t_pr, epc_pr),\n",
        "                ('SAA',         t_saa, epc_saa),\n",
        "                ('REGA',        t_rega, epc_rega),\n",
        "                ('Thresh-MIS',  t_th,  epc_th),\n",
        "            ]:\n",
        "                records.append({\n",
        "                    'model': name,\n",
        "                    'p':      p,\n",
        "                    'algo':   algo,\n",
        "                    'time':   t,\n",
        "                    'epc':    epc\n",
        "                })\n",
        "    return pd.DataFrame(records)"
      ],
      "metadata": {
        "id": "r0GRokeowWEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'ER': nx.gnm_random_graph(20, 40, seed=42),\n",
        "    'BA': nx.barabasi_albert_graph(20, 2, seed=42),\n",
        "    'SW': nx.watts_strogatz_graph(20, 4, 0.3, seed=42),\n",
        "}\n",
        "\n",
        "df = run_experiments_with_threshold(\n",
        "    models,\n",
        "    ps=[0.1,0.2,0.5,0.7 ,1.0],\n",
        "    k=10,\n",
        "    T_saa=30,\n",
        "    T_rega=1000,\n",
        "    R_rega=5,\n",
        "    alpha=0.2,\n",
        "    tau=0.5,\n",
        "    N_eval=10000\n",
        ")"
      ],
      "metadata": {
        "id": "Lx35hicvwXUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in models:\n",
        "    plt.figure()\n",
        "    for algo in df.algo.unique():\n",
        "        sub = df[(df.model == name) & (df.algo == algo)]\n",
        "        plt.plot(sub.p, sub.epc, label=algo)\n",
        "    plt.title(f\"{name} — EPC vs p\")\n",
        "    plt.xlabel(\"p\"); plt.ylabel(\"EPC\"); plt.grid(True); plt.legend()\n",
        "    plt.savefig(f\"{name}_epc_vs_p.png\")\n",
        "\n",
        "    plt.figure()\n",
        "    for algo in df.algo.unique():\n",
        "        sub = df[(df.model == name) & (df.algo == algo)]\n",
        "        plt.plot(sub.p, sub.time, label=algo)\n",
        "    plt.title(f\"{name} — Time vs p\")\n",
        "    plt.xlabel(\"p\"); plt.ylabel(\"Time (s)\"); plt.grid(True); plt.legend()\n",
        "    plt.savefig(f\"{name}_time_vs_p.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YyAXhh3HwZnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}