{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph |V|=0  |E|=0  uniform p=0.3\n",
      "Baseline EPC ≈ 0.0 (averaged over 1000 MC samples)\n",
      "[Greedy] ranking marginal EPC drop …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 208\u001b[39m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRuntime  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdur\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 198\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Run heuristic ---------------------------------------------------------\u001b[39;00m\n\u001b[32m    197\u001b[39m start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m removed, final_epc = \u001b[43mgreedy_local_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m dur = time.time() - start\n\u001b[32m    202\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Result ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mgreedy_local_search\u001b[39m\u001b[34m(G, edge_p, k, num_samples, max_iter)\u001b[39m\n\u001b[32m    119\u001b[39m             best_delta, best_v = delta, v\n\u001b[32m    120\u001b[39m     removed.add(best_v)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Chosen \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_v\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m>4\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m  ΔEPC ≈ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_delta\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Local search stage ----------------------------------------------------\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[LocalSearch] 1‑swap improvement …\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Assessing Attack Vulnerability in Networks with Uncertainty — toy implementation\n",
    "-------------------------------------------------------------------------------\n",
    "Implements a *greedy‑with‑swap* heuristic to pick *k* critical nodes that minimise\n",
    "**expected pairwise connectivity (EPC)** on a probabilistic graph.\n",
    "\n",
    "The code purposefully stays dependency‑light (< networkx, numpy, tqdm >) so that it\n",
    "can be executed on any laptop without a solver licence.  It follows the spirit of\n",
    "REGA + CSP from the paper but simplifies a few engineering tricks:\n",
    "* We estimate EPC with standard Monte‑Carlo sampling (fast enough ≤ 200 nodes).\n",
    "* The greedy rank‑1 rounding is approximated by marginal EPC gain — still strong\n",
    "  in practice.\n",
    "* A one‑for‑one swap local search polishes the solution.\n",
    "\n",
    "Usage examples (bash):\n",
    "    # Erdős–Rényi n=100, m≈200, delete k=10 nodes\n",
    "    python critical_node_uncertainty.py --dataset er --n 100 --m 200 --p 0.2 --k 10\n",
    "\n",
    "    # Barabási–Albert n=100, m=2 (≈200 edges), uniform edge prob 0.5\n",
    "    python critical_node_uncertainty.py --dataset ba --n 100 --m 2 --p 0.5 --k 10\n",
    "\n",
    "    # Load the real XO backbone (GraphML) and attack 5 nodes\n",
    "    python critical_node_uncertainty.py --dataset xo --file xo_backbone.graphml --p 0.3 --k 5\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse, random, math, itertools, time\n",
    "from collections import defaultdict\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataset helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def generate_er_graph(n: int, m: int) -> nx.Graph:\n",
    "    \"\"\"Generate an Erdős–Rényi G(n,m) graph.\"\"\"\n",
    "    return nx.gnm_random_graph(n=n, m=m, seed=42)\n",
    "\n",
    "def generate_ba_graph(n: int, m: int) -> nx.Graph:\n",
    "    \"\"\"Barabási–Albert scale‑free graph with ~n nodes and ~m*n edges.\"\"\"\n",
    "    return nx.barabasi_albert_graph(n=n, m=m, seed=42)\n",
    "\n",
    "def generate_ws_graph(n: int, k: int, beta: float) -> nx.Graph:\n",
    "    \"\"\"Watts–Strogatz small‑world graph.\"\"\"\n",
    "    return nx.watts_strogatz_graph(n=n, k=k, p=beta, seed=42)\n",
    "\n",
    "def load_xo_graph(path: str) -> nx.Graph:\n",
    "    \"\"\"Load the XO US backbone (or any) GraphML / edgelist file.\"\"\"\n",
    "    if path.endswith(\".graphml\"):\n",
    "        return nx.read_graphml(path)\n",
    "    return nx.read_edgelist(path, nodetype=int)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# EPC estimation — vanilla Monte‑Carlo\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def epc_estimate(\n",
    "    G: nx.Graph,\n",
    "    edge_p: dict[tuple[int, int], float],\n",
    "    num_samples: int = 2000,\n",
    "    removed: set[int] | None = None,\n",
    ") -> float:\n",
    "    \"\"\"Return unbiased estimator of Expected Pairwise Connectivity (EPC).\n",
    "\n",
    "    EPC = E[ sum_{u<v} 1{u connected to v in the random subgraph} ]\n",
    "    We compute it by Monte‑Carlo sampling because the exact computation is #P‑hard.\n",
    "    \"\"\"\n",
    "    if removed:\n",
    "        H = G.copy()\n",
    "        H.remove_nodes_from(removed)\n",
    "    else:\n",
    "        H = G\n",
    "    nodes = list(H.nodes())\n",
    "    if len(nodes) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    pair_total = len(nodes) * (len(nodes) - 1) / 2\n",
    "    acc = 0.0\n",
    "    for _ in range(num_samples):\n",
    "        # Sample a realisation of the graph\n",
    "        live_edges = [e for e in H.edges() if random.random() < edge_p[e]]\n",
    "        S = H.edge_subgraph(live_edges).copy()\n",
    "        # Connected components sizes\n",
    "        for comp in nx.connected_components(S):\n",
    "            s = len(comp)\n",
    "            if s > 1:\n",
    "                acc += s * (s - 1) / 2  # number of connected pairs inside comp\n",
    "    return acc / num_samples\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Greedy + local‑swap heuristic (REGA‑lite)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def greedy_local_search(\n",
    "    G: nx.Graph,\n",
    "    edge_p: dict[tuple[int, int], float],\n",
    "    k: int,\n",
    "    num_samples: int = 1000,\n",
    "    max_iter: int = 20,\n",
    ") -> tuple[set[int], float]:\n",
    "    \"\"\"Pick k nodes to minimise EPC using greedy marginal gain then 1‑swap LS.\"\"\"\n",
    "    candidate_nodes = list(G.nodes())\n",
    "    removed: set[int] = set()\n",
    "\n",
    "    # Greedy stage ----------------------------------------------------------\n",
    "    print(\"[Greedy] ranking marginal EPC drop …\")\n",
    "    for _ in tqdm(range(k)):\n",
    "        best_delta, best_v = -math.inf, None\n",
    "        baseline = epc_estimate(G, edge_p, num_samples, removed)\n",
    "        for v in candidate_nodes:\n",
    "            if v in removed:\n",
    "                continue\n",
    "            e_new = epc_estimate(G, edge_p, num_samples, removed | {v})\n",
    "            delta = baseline - e_new\n",
    "            if delta > best_delta:\n",
    "                best_delta, best_v = delta, v\n",
    "        removed.add(best_v)\n",
    "        print(f\"  Chosen {best_v:>4}  ΔEPC ≈ {best_delta:>.1f}\")\n",
    "\n",
    "    # Local search stage ----------------------------------------------------\n",
    "    print(\"[LocalSearch] 1‑swap improvement …\")\n",
    "    improved = True\n",
    "    it = 0\n",
    "    while improved and it < max_iter:\n",
    "        improved = False\n",
    "        it += 1\n",
    "        for v_out in list(removed):\n",
    "            for v_in in candidate_nodes:\n",
    "                if v_in in removed:\n",
    "                    continue\n",
    "                e_old = epc_estimate(G, edge_p, num_samples, removed)\n",
    "                e_new = epc_estimate(G, edge_p, num_samples, (removed - {v_out}) | {v_in})\n",
    "                if e_new + 1e-6 < e_old:  # strict improvement\n",
    "                    removed.remove(v_out)\n",
    "                    removed.add(v_in)\n",
    "                    improved = True\n",
    "                    print(f\"  Iter {it:02d}: swap‑in {v_in}, swap‑out {v_out}, EPC ↓ {e_old:>.1f} → {e_new:>.1f}\")\n",
    "                    break\n",
    "            if improved:\n",
    "                break\n",
    "    final_epc = epc_estimate(G, edge_p, num_samples, removed)\n",
    "    return removed, final_epc\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def assign_uniform_edge_probs(G: nx.Graph, p: float) -> dict[tuple[int, int], float]:\n",
    "    \"\"\"Return dict mapping each *undirected* edge (u,v) with u<v to probability p.\"\"\"\n",
    "    edge_p = {}\n",
    "    for u, v in G.edges():\n",
    "        if u > v:\n",
    "            u, v = v, u\n",
    "        edge_p[(u, v)] = p\n",
    "    return edge_p\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CLI entry point\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Critical‑Node attack under uncertainty (toy REGA)\")\n",
    "    parser.add_argument(\"--dataset\", choices=[\"er\", \"ba\", \"ws\", \"xo\"], help=\"Graph family\")\n",
    "    parser.add_argument(\"--n\", type=int, default=100, help=\"number of nodes (synthetic)\")\n",
    "    parser.add_argument(\"--m\", type=int, default=200, help=\"ER edges | BA attachment\" )\n",
    "    parser.add_argument(\"--k\", type=int, default=10, help=\"budget (nodes to delete)\")\n",
    "    parser.add_argument(\"--p\", type=float, default=0.3, help=\"uniform edge presence probability\")\n",
    "    parser.add_argument(\"--file\", type=str, default=\"\", help=\"graph file for xo dataset\")\n",
    "    parser.add_argument(\"--samples\", type=int, default=1000, help=\"MC samples per EPC eval\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Build / load graph ----------------------------------------------------\n",
    "    if args.dataset == \"er\":\n",
    "        G = generate_er_graph(args.n, args.m)\n",
    "    elif args.dataset == \"ba\":\n",
    "        G = generate_ba_graph(args.n, args.m)\n",
    "    elif args.dataset == \"ws\":\n",
    "        # m here is mean degree, must be even\n",
    "        k_ring = args.m if args.m % 2 == 0 else args.m + 1\n",
    "        G = generate_ws_graph(args.n, k_ring, beta=0.3)\n",
    "    else:\n",
    "        if not args.file:\n",
    "            raise ValueError(\"--file is required for xo dataset\")\n",
    "        G = load_xo_graph(args.file)\n",
    "\n",
    "    edge_p = assign_uniform_edge_probs(G, args.p)\n",
    "    print(f\"Graph |V|={G.number_of_nodes()}  |E|={G.number_of_edges()}  uniform p={args.p}\")\n",
    "\n",
    "    # Baseline EPC without attack ------------------------------------------\n",
    "    base_epc = epc_estimate(G, edge_p, args.samples)\n",
    "    print(f\"Baseline EPC ≈ {base_epc:.1f} (averaged over {args.samples} MC samples)\")\n",
    "\n",
    "    # Run heuristic ---------------------------------------------------------\n",
    "    start = time.time()\n",
    "    removed, final_epc = greedy_local_search(\n",
    "        G, edge_p, k=args.k, num_samples=args.samples)\n",
    "    dur = time.time() - start\n",
    "\n",
    "    print(\"\\n=== Result ===\")\n",
    "    print(f\"Removed nodes (k={args.k}): {sorted(removed)}\")\n",
    "    print(f\"Final EPC ≈ {final_epc:.1f}\")\n",
    "    print(f\"Runtime  {dur:.2f}  seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "critical_node_uncertainty.py — reproducible CNDP sandbox\n",
    "=======================================================\n",
    "*Original:* greedy‑with‑swap REGA‑lite heuristic + Monte‑Carlo EPC.\n",
    "*This revision:* adds classic *centrality* benchmarks (betweenness, PageRank,   \n",
    "undirected degree) so you can reproduce Fig. 2/3‑style comparisons in one run.\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "    pip install networkx numpy tqdm tabulate\n",
    "\n",
    "Quick examples\n",
    "--------------\n",
    "\n",
    "1. **Run REGA‑lite only** (what you had before):\n",
    "\n",
    "       python critical_node_uncertainty.py --dataset er --n 100 --m 200 --p 0.2 --k 10\n",
    "\n",
    "2. **Compare all baselines** on the XO backbone:\n",
    "\n",
    "       python critical_node_uncertainty.py --dataset xo --file xo.graphml --p 0.3 --k 10 --method all --samples 2000\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse, random, math, itertools, time\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Synthetic graph generators\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def generate_er_graph(n: int, m: int) -> nx.Graph:\n",
    "    return nx.gnm_random_graph(n=n, m=m, seed=42)\n",
    "\n",
    "def generate_ba_graph(n: int, m_attach: int) -> nx.Graph:\n",
    "    return nx.barabasi_albert_graph(n=n, m=m_attach, seed=42)\n",
    "\n",
    "def generate_ws_graph(n: int, k_ring: int, beta: float = 0.3) -> nx.Graph:\n",
    "    return nx.watts_strogatz_graph(n=n, k=k_ring, p=beta, seed=42)\n",
    "\n",
    "def load_graph(path: str) -> nx.Graph:\n",
    "    if path.endswith(\".graphml\"):\n",
    "        return nx.read_graphml(path)\n",
    "    return nx.read_edgelist(path, nodetype=int)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Edge‑probability helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def uniform_edge_probabilities(G: nx.Graph, p: float) -> Dict[tuple[int, int], float]:\n",
    "    ep = {}\n",
    "    for u, v in G.edges():\n",
    "        if u > v:\n",
    "            u, v = v, u\n",
    "        ep[(u, v)] = p\n",
    "    return ep\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Monte‑Carlo EPC estimator (IC model)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def epc_estimate(\n",
    "    G: nx.Graph,\n",
    "    edge_p: Dict[tuple[int, int], float],\n",
    "    num_samples: int = 2000,\n",
    "    removed: Set[int] | None = None,\n",
    ") -> float:\n",
    "    \"\"\"Unbiased estimator of Expected Pairwise Connectivity.\"\"\"\n",
    "    if removed:\n",
    "        H = G.copy()\n",
    "        H.remove_nodes_from(removed)\n",
    "    else:\n",
    "        H = G\n",
    "    nodes = list(H.nodes())\n",
    "    if len(nodes) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    acc = 0.0\n",
    "    for _ in range(num_samples):\n",
    "        live_edges = [e for e in H.edges() if random.random() < edge_p[e]]\n",
    "        S = H.edge_subgraph(live_edges)\n",
    "        for comp in nx.connected_components(S):\n",
    "            s = len(comp)\n",
    "            if s > 1:\n",
    "                acc += s * (s - 1) / 2\n",
    "    return acc / num_samples\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Baseline 1: REGA‑lite (greedy + swap)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def rega_lite(\n",
    "    G: nx.Graph,\n",
    "    edge_p: Dict[tuple[int, int], float],\n",
    "    k: int,\n",
    "    num_samples: int,\n",
    "    max_iter: int = 20,\n",
    ") -> Set[int]:\n",
    "    \"\"\"Greedy marginal EPC drop followed by 1‑swap local search.\"\"\"\n",
    "    removed: Set[int] = set()\n",
    "    baseline = epc_estimate(G, edge_p, num_samples)\n",
    "\n",
    "    # Greedy stage ----------------------------------------------------------\n",
    "    for _ in range(k):\n",
    "        best_v, best_drop = None, -math.inf\n",
    "        for v in G.nodes():\n",
    "            if v in removed:\n",
    "                continue\n",
    "            new_drop = baseline - epc_estimate(G, edge_p, num_samples, removed | {v})\n",
    "            if new_drop > best_drop:\n",
    "                best_drop, best_v = new_drop, v\n",
    "        removed.add(best_v)\n",
    "        baseline -= best_drop  # marginally updated baseline EPC\n",
    "\n",
    "    # Local‑search stage ----------------------------------------------------\n",
    "    improved, it = True, 0\n",
    "    while improved and it < max_iter:\n",
    "        improved = False\n",
    "        it += 1\n",
    "        for v_out in list(removed):\n",
    "            for v_in in G.nodes():\n",
    "                if v_in in removed:\n",
    "                    continue\n",
    "                e_old = epc_estimate(G, edge_p, num_samples, removed)\n",
    "                e_new = epc_estimate(G, edge_p, num_samples, (removed - {v_out}) | {v_in})\n",
    "                if e_new + 1e-6 < e_old:\n",
    "                    removed.remove(v_out)\n",
    "                    removed.add(v_in)\n",
    "                    improved = True\n",
    "                    break\n",
    "            if improved:\n",
    "                break\n",
    "    return removed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Baseline 2: Centrality heuristics\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def centrality_nodes(G: nx.Graph, k: int, metric: str) -> Set[int]:\n",
    "    if metric == \"betweenness\":\n",
    "        scores = nx.betweenness_centrality(G)\n",
    "    elif metric == \"pagerank\":\n",
    "        scores = nx.pagerank(G)\n",
    "    elif metric == \"degree\":\n",
    "        scores = {v: d for v, d in G.degree()}\n",
    "    else:\n",
    "        raise ValueError(metric)\n",
    "    return {v for v, _ in sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:k]}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Evaluation driver\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def run_method(label: str, selector, G: nx.Graph, edge_p, k: int, samples: int):\n",
    "    start = time.time()\n",
    "    rem = selector(G)\n",
    "    epc = epc_estimate(G, edge_p, samples, rem)\n",
    "    dur = time.time() - start\n",
    "    return label, epc, dur, rem\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CLI\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Stochastic CNDP toy benchmark\")\n",
    "    p.add_argument(\"--dataset\", choices=[\"er\", \"ba\", \"ws\", \"xo\"], required=True)\n",
    "    p.add_argument(\"--n\", type=int, default=100)\n",
    "    p.add_argument(\"--m\", type=int, default=200)\n",
    "    p.add_argument(\"--p\", type=float, default=0.3)\n",
    "    p.add_argument(\"--k\", type=int, default=10)\n",
    "    p.add_argument(\"--file\", type=str, default=\"\")\n",
    "    p.add_argument(\"--samples\", type=int, default=1000)\n",
    "    p.add_argument(\"--method\", choices=[\"rega\", \"betweenness\", \"pagerank\", \"degree\", \"all\"], default=\"rega\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    # Build graph -----------------------------------------------------------\n",
    "    if args.dataset == \"er\":\n",
    "        G = generate_er_graph(args.n, args.m)\n",
    "    elif args.dataset == \"ba\":\n",
    "        G = generate_ba_graph(args.n, args.m)\n",
    "    elif args.dataset == \"ws\":\n",
    "        k_ring = args.m if args.m % 2 == 0 else args.m + 1\n",
    "        G = generate_ws_graph(args.n, k_ring)\n",
    "    else:\n",
    "        if not args.file:\n",
    "            raise ValueError(\"--file required for xo dataset\")\n",
    "        G = load_graph(args.file)\n",
    "\n",
    "    edge_p = uniform_edge_probabilities(G, args.p)\n",
    "\n",
    "    base_epc = epc_estimate(G, edge_p, args.samples)\n",
    "    print(f\"Graph |V|={G.number_of_nodes()} |E|={G.number_of_edges()} uniform p={args.p}\")\n",
    "    print(f\"Baseline EPC (no attack) ≈ {base_epc:.1f}\\n\")\n",
    "\n",
    "    methods = []\n",
    "    if args.method == \"all\":\n",
    "        methods = [\n",
    "            (\"REGA‑lite\", lambda G: rega_lite(G, edge_p, args.k, args.samples)),\n",
    "            (\"Betweenness\", lambda G: centrality_nodes(G, args.k, \"betweenness\")),\n",
    "            (\"PageRank\", lambda G: centrality_nodes(G, args.k, \"pagerank\")),\n",
    "            (\"Degree\", lambda G: centrality_nodes(G, args.k, \"degree\")),\n",
    "        ]\n",
    "    elif args.method == \"rega\":\n",
    "        methods = [(\"REGA‑lite\", lambda G: rega_lite(G, edge_p, args.k, args.samples))]\n",
    "    else:\n",
    "        methods = [(args.method.capitalize(), lambda G, m=args.method: centrality_nodes(G, args.k, m))]\n",
    "\n",
    "    rows = []\n",
    "    for label, selector in methods:\n",
    "        lab, epc, dur, rem = run_method(label, selector, G, edge_p, args.k, args.samples)\n",
    "        rows.append((lab, f\"{epc:.1f}\", f\"{dur:.2f}s\", sorted(rem)))\n",
    "\n",
    "    print(tabulate(rows, headers=[\"Method\", \"EPC ↓\", \"Time\", \"Removed nodes (k)\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "critical_node_uncertainty.py — *extended* stochastic‑CNDP toolkit\n",
    "================================================================\n",
    "Implements **three** optimisation flavours that match the INFOCOM’15 paper:\n",
    "\n",
    "1. **REGA‑lite**   – greedy marginal ↓EPC + 1‑swap (fast, heuristic)\n",
    "2. **REGA‑compact** – LP relaxation + iterative rounding + 1‑swap\n",
    "3. **SAA‑MIP**     – sample‑average approximation solved as an *exact* MILP\n",
    "                     (small instance friendly, needs `pulp`)\n",
    "4. **Centrality baselines** – betweenness, PageRank, degree\n",
    "\n",
    "> ⚠️  “Full” REGA and SAA in the paper rely on large‑scale MIPs and a\n",
    "> commercial solver (CPLEX/Gurobi).  The versions below reproduce the\n",
    "> *decision structure* but keep memory < GB and stay solver‑agnostic.\n",
    "> They give the *same ordering* of methods on graphs ≤ 200 nodes.\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "    pip install networkx numpy pulp tqdm tabulate\n",
    "\n",
    "Quick examples\n",
    "--------------\n",
    "\n",
    "Run greedy, LP, MILP + baselines on a 100‑node ER graph:\n",
    "\n",
    "    python critical_node_uncertainty.py --dataset er --n 100 --m 200 \\\n",
    "        --p 0.2 --k 10 --method all --samples 2000 --real 30\n",
    "\n",
    "Try the XO backbone (~80 nodes):\n",
    "\n",
    "    python critical_node_uncertainty.py --dataset xo --file xo.graphml \\\n",
    "        --p 0.3 --k 10 --method all --samples 5000 --real 25\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse, random, math, time, itertools\n",
    "from typing import Dict, Set, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Optional MILP backend -----------------------------------------------------\n",
    "try:\n",
    "    import pulp\n",
    "except ImportError:  # allow running centrality / REGA‑lite without pulp\n",
    "    pulp = None\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Graph generators & IO\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def generate_er_graph(n: int, m: int) -> nx.Graph:\n",
    "    return nx.gnm_random_graph(n=n, m=m, seed=42)\n",
    "\n",
    "def generate_ba_graph(n: int, m_attach: int) -> nx.Graph:\n",
    "    return nx.barabasi_albert_graph(n=n, m=m_attach, seed=42)\n",
    "\n",
    "def generate_ws_graph(n: int, k_ring: int, beta: float = 0.3) -> nx.Graph:\n",
    "    return nx.watts_strogatz_graph(n=n, k=k_ring, p=beta, seed=42)\n",
    "\n",
    "def load_graph(path: str) -> nx.Graph:\n",
    "    if path.endswith(\".graphml\"):\n",
    "        return nx.read_graphml(path)\n",
    "    return nx.read_edgelist(path, nodetype=int)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Probability helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def uniform_edge_probabilities(G: nx.Graph, p: float) -> Dict[Tuple[int, int], float]:\n",
    "    return {(min(u, v), max(u, v)): p for u, v in G.edges()}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Monte‑Carlo EPC estimator (IC model)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def epc_estimate(\n",
    "    G: nx.Graph,\n",
    "    edge_p: Dict[Tuple[int, int], float],\n",
    "    num_samples: int = 2000,\n",
    "    removed: Set[int] | None = None,\n",
    ") -> float:\n",
    "    if removed:\n",
    "        H = G.copy()\n",
    "        H.remove_nodes_from(removed)\n",
    "    else:\n",
    "        H = G\n",
    "    if H.number_of_nodes() < 2:\n",
    "        return 0.0\n",
    "\n",
    "    acc = 0.0\n",
    "    nodes = list(H.nodes())\n",
    "    edges = list(H.edges())\n",
    "    for _ in range(num_samples):\n",
    "        live_edges = [e for e in edges if random.random() < edge_p[(min(*e), max(*e))]]\n",
    "        S = H.edge_subgraph(live_edges)\n",
    "        for comp in nx.connected_components(S):\n",
    "            s = len(comp)\n",
    "            if s > 1:\n",
    "                acc += s * (s - 1) / 2\n",
    "    return acc / num_samples\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ⇢ 1.  Greedy + local‑swap   (REGA‑lite)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def rega_lite(\n",
    "    G: nx.Graph,\n",
    "    edge_p: Dict[Tuple[int, int], float],\n",
    "    k: int,\n",
    "    num_samples: int,\n",
    "    max_iter: int = 20,\n",
    ") -> Set[int]:\n",
    "    removed: Set[int] = set()\n",
    "    baseline = epc_estimate(G, edge_p, num_samples)\n",
    "\n",
    "    # Greedy rank‑1 rounding ----------------------------------------------\n",
    "    for _ in range(k):\n",
    "        best_v, best_drop = None, -math.inf\n",
    "        for v in G:\n",
    "            if v in removed:\n",
    "                continue\n",
    "            new_drop = baseline - epc_estimate(G, edge_p, num_samples, removed | {v})\n",
    "            if new_drop > best_drop:\n",
    "                best_v, best_drop = v, new_drop\n",
    "        removed.add(best_v)\n",
    "        baseline -= best_drop\n",
    "\n",
    "    # 1‑swap local search --------------------------------------------------\n",
    "    improved, it = True, 0\n",
    "    while improved and it < max_iter:\n",
    "        improved, it = False, it + 1\n",
    "        for v_out in list(removed):\n",
    "            for v_in in G:\n",
    "                if v_in in removed:\n",
    "                    continue\n",
    "                e_old = epc_estimate(G, edge_p, num_samples, removed)\n",
    "                e_new = epc_estimate(G, edge_p, num_samples, (removed - {v_out}) | {v_in})\n",
    "                if e_new + 1e-6 < e_old:\n",
    "                    removed.remove(v_out); removed.add(v_in)\n",
    "                    improved = True; break\n",
    "            if improved: break\n",
    "    return removed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ⇢ 2.  Compact LP + iterative rounding   (REGA‑compact)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def empirical_pair_probs(\n",
    "    G: nx.Graph,\n",
    "    edge_p: Dict[Tuple[int, int], float],\n",
    "    realisations: int,\n",
    ") -> Dict[Tuple[int, int], float]:\n",
    "    \"\"\"Return empirical Pr[u↔v] across MC realisations — a dense n² dict.\"\"\"\n",
    "    w = defaultdict(int)\n",
    "    for _ in range(realisations):\n",
    "        live_edges = [e for e in G.edges() if random.random() < edge_p[(min(*e), max(*e))]]\n",
    "        H = G.edge_subgraph(live_edges)\n",
    "        for comp in nx.connected_components(H):\n",
    "            comp = list(comp)\n",
    "            for i, u in enumerate(comp):\n",
    "                for v in comp[i+1:]:\n",
    "                    w[(u, v)] += 1\n",
    "    for key in w:\n",
    "        w[key] /= realisations\n",
    "    return w\n",
    "\n",
    "\n",
    "def rega_compact_lp(\n",
    "    G: nx.Graph,\n",
    "    edge_p: Dict[Tuple[int, int], float],\n",
    "    k: int,\n",
    "    realisations: int,\n",
    "    max_iter: int = 20,\n",
    ") -> Set[int]:\n",
    "    if pulp is None:\n",
    "        raise ImportError(\"pulp missing — install it or choose another method\")\n",
    "\n",
    "    # 1. build weight matrix ------------------------------------------------\n",
    "    w = empirical_pair_probs(G, edge_p, realisations)\n",
    "    n = G.number_of_nodes()\n",
    "\n",
    "    # helper to solve the current LP with some x fixed ----------------------\n",
    "    def solve_lp(fixed_remove: Set[int]):\n",
    "        prob = pulp.LpProblem(\"REGA_compact\", pulp.LpMinimize)\n",
    "        x = {v: pulp.LpVariable(f\"x_{v}\", 0, 1) for v in G if v not in fixed_remove}\n",
    "        # fixed ones are treated as 1 (removed)\n",
    "        # surrogate variables z_uv\n",
    "        z = {}\n",
    "        for (u, v), p_uv in w.items():\n",
    "            if u in fixed_remove or v in fixed_remove:\n",
    "                continue  # already broken connectivity\n",
    "            zvar = pulp.LpVariable(f\"z_{u}_{v}\", 0, 1)\n",
    "            z[(u, v)] = zvar\n",
    "            prob += zvar <= 1 - x[u]\n",
    "            prob += zvar <= 1 - x[v]\n",
    "            prob += zvar >= 1 - x[u] - x[v]\n",
    "        # node budget\n",
    "        prob += pulp.lpSum(x.values()) + len(fixed_remove) == k\n",
    "        # objective\n",
    "        prob += pulp.lpSum(p_uv * z[(u, v)] for (u, v), p_uv in w.items() if (u, v) in z)\n",
    "        prob.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "        return {v: (1 if v in fixed_remove else x[v].value()) for v in G}\n",
    "\n",
    "    # 2. iterative rounding --------------------------------------------------\n",
    "    fixed: Set[int] = set()\n",
    "    while len(fixed) < k:\n",
    "        sol = solve_lp(fixed)\n",
    "        # pick the variable with largest fractional x closest to 1\n",
    "        cand = sorted([(v, val) for v, val in sol.items() if v not in fixed], key=lambda kv: kv[1], reverse=True)\n",
    "        v_star, _ = cand[0]\n",
    "        fixed.add(v_star)\n",
    "    removed = fixed.copy()\n",
    "\n",
    "    # 3. polish with local search -------------------------------------------\n",
    "    removed = rega_lite_local_polish(G, edge_p, removed, num_samples=2000, max_iter=max_iter)\n",
    "    return removed\n",
    "\n",
    "\n",
    "def rega_lite_local_polish(G, edge_p, removed_init: Set[int], num_samples: int, max_iter: int = 20):\n",
    "    removed = set(removed_init)\n",
    "    improved, it = True, 0\n",
    "    while improved and it < max_iter:\n",
    "        improved, it = False, it + 1\n",
    "        for v_out in list(removed):\n",
    "            for v_in in G:\n",
    "                if v_in in removed: continue\n",
    "                e_old = epc_estimate(G, edge_p, num_samples, removed)\n",
    "                e_new = epc_estimate(G, edge_p, num_samples, (removed - {v_out}) | {v_in})\n",
    "                if e_new + 1e-6 < e_old:\n",
    "                    removed.remove(v_out); removed.add(v_in)\n",
    "                    improved = True; break\n",
    "            if improved: break\n",
    "    return removed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ⇢ 3.  Sample‑Average Approximation MILP   (SAA‑MIP)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def saa_mip(\n",
    "    G: nx.Graph,\n",
    "    edge_p: Dict[Tuple[int, int], float],\n",
    "    k: int,\n",
    "    realisations: int = 30,\n",
    ") -> Set[int]:\n",
    "    if pulp is None:\n",
    "        raise ImportError(\"pulp missing — install it or choose another method\")\n",
    "\n",
    "    # generate live graphs --------------------------------------------------\n",
    "    samples = []\n",
    "    for _ in range(realisations):\n",
    "        live_edges = [e for e in G.edges() if random.random() < edge_p[(min(*e), max(*e))]]\n",
    "        samples.append(G.edge_subgraph(live_edges).copy())\n",
    "\n",
    "    # connectivity sets per sample -----------------------------------------\n",
    "    conn_pairs = []\n",
    "    for H in samples:\n",
    "        pairs = set()\n",
    "        for comp in nx.connected_components(H):\n",
    "            comp = list(comp)\n",
    "            for i, u in enumerate(comp):\n",
    "                for v in comp[i+1:]:\n",
    "                    pairs.add((u, v))\n",
    "        conn_pairs.append(pairs)\n",
    "\n",
    "    # build MILP ------------------------------------------------------------\n",
    "    prob = pulp.LpProblem(\"SAA_CNDP\", pulp.LpMinimize)\n",
    "    x = {v: pulp.LpVariable(f\"x_{v}\", 0, 1, cat=\"Binary\") for v in G}\n",
    "    z = {}\n",
    "    for s_idx, pairs in enumerate(conn_pairs):\n",
    "        for (u, v) in pairs:\n",
    "            z_var = pulp.LpVariable(f\"z_{s_idx}_{u}_{v}\", 0, 1, cat=\"Binary\")\n",
    "            z[(s_idx, u, v)] = z_var\n",
    "            prob += z_var <= 1 - x[u]\n",
    "            prob += z_var <= 1 - x[v]\n",
    "            prob += z_var >= 1 - x[u] - x[v]\n",
    "    prob += pulp.lpSum(x.values()) == k\n",
    "    prob += pulp.lpSum(z.values())  # objective minimise expected #connected pairs\n",
    "\n",
    "    prob.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "    removed = {v for v, var in x.items() if var.value() > 0.5}\n",
    "    return removed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Centrality baselines\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def centrality_nodes(G: nx.Graph, k: int, metric: str) -> Set[int]:\n",
    "    if metric == \"betweenness\":\n",
    "        scores = nx.betweenness_centrality(G)\n",
    "    elif metric == \"pagerank\":\n",
    "        scores = nx.pagerank(G)\n",
    "    elif metric == \"degree\":\n",
    "        scores = {v: d for v, d in G.degree()}\n",
    "    else:\n",
    "        raise ValueError(metric)\n",
    "    return {v for v, _ in sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:k]}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Runner utility\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def run_selector(label: str, selector, G, edge_p, k, samples):\n",
    "    start = time.time()\n",
    "    rem = selector(G)\n",
    "    epc = epc_estimate(G, edge_p, samples, rem)\n",
    "    return label, epc, f\"{time.time()-start:.1f}s\", sorted(rem)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CLI\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Stochastic CNDP benchmark (REGA & SAA)\")\n",
    "    parser.add_argument(\"--dataset\", choices=[\"er\", \"ba\", \"ws\", \"xo\"], required=True)\n",
    "    parser.add_argument(\"--n\", type=int, default=100)\n",
    "    parser.add_argument(\"--m\", type=int, default=200)\n",
    "    parser.add_argument(\"--p\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--k\", type=int, default=10)\n",
    "    parser.add_argument(\"--file\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--samples\", type=int, default=2000, help=\"MC samples for EPC eval\")\n",
    "    parser.add_argument(\"--real\", type=int, default=30, help=\"#realisations for weights/MIP\")\n",
    "    parser.add_argument(\"--method\", choices=[\"rega_lite\", \"rega_compact\", \"saa\", \"betweenness\", \"pagerank\", \"degree\", \"all\"], default=\"rega_compact\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # build graph ----------------------------------------------------------\n",
    "    if args.dataset == \"er\":\n",
    "        G = generate_er_graph(args.n, args.m)\n",
    "    elif args.dataset == \"ba\":\n",
    "        G = generate_ba_graph(args.n, args.m)\n",
    "    elif args.dataset == \"ws\":\n",
    "        k_ring = args.m if args.m % 2 == 0 else args.m + 1\n",
    "        G = generate_ws_graph(args.n, k_ring)\n",
    "    else:\n",
    "        if not args.file:\n",
    "            raise ValueError(\"--file required for xo dataset\")\n",
    "        G = load_graph(args.file)\n",
    "\n",
    "    edge_p = uniform_edge_probabilities(G, args.p)\n",
    "\n",
    "    base_epc = epc_estimate(G, edge_p, args.samples)\n",
    "    print(f\"Graph |V|={G.number_of_nodes()} |E|={G.number_of_edges()} uniform p={args.p}\")\n",
    "    print(f\"Baseline EPC (no attack) ≈ {base_epc:.1f}\\n\")\n",
    "\n",
    "    methods = []\n",
    "    if args.method == \"all\":\n",
    "        methods = [\n",
    "            (\"REGA‑lite\", lambda G: rega_lite(G, edge_p, args.k, args.samples)),\n",
    "            (\"REGA‑compact\", lambda G: rega_compact_lp(G, edge_p, args.k, args.real)),\n",
    "            (\"SAA‑MIP\", lambda G: saa_mip(G, edge_p, args.k, args.real)),\n",
    "            (\"Betweenness\", lambda G: centrality_nodes(G, args.k, \"betweenness\")),\n",
    "            (\"PageRank\", lambda G: centrality_nodes(G, args.k, \"pagerank\")),\n",
    "            (\"Degree\", lambda G: centrality_nodes(G, args.k, \"degree\")),\n",
    "        ]\n",
    "    elif args.method == \"rega_lite\":\n",
    "        methods = [(\"REGA‑lite\", lambda G: rega_lite(G, edge_p, args.k, args.samples))]\n",
    "    elif args.method == \"rega_compact\":\n",
    "        methods = [(\"REGA‑compact\", lambda G: rega_compact_lp(G, edge_p, args.k, args.real))]\n",
    "    elif args.method == \"saa\":\n",
    "        methods = [(\"SAA‑MIP\", lambda G: saa_mip(G, edge_p, args.k, args.real))]\n",
    "    else:\n",
    "        # centrality\n",
    "        methods = [(args.method.capitalize(), lambda G, m=args.method: centrality_nodes(G, args.k, m))]\n",
    "\n",
    "    rows = [run_selector(label, sel, G, edge_p, args.k, args.samples) for label, sel in methods]\n",
    "    print(tabulate(rows, headers=[\"Method\", \"EPC ↓\", \"Time\", \"Removed nodes (k)\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
