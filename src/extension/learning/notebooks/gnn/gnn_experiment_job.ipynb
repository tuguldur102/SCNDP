{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee59a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import glob\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "# === Standard Library ===\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import heapq\n",
    "import itertools\n",
    "from collections import defaultdict, deque\n",
    "from itertools import combinations\n",
    "from typing import Any, Tuple, Dict, List, Set, Sequence, Union\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "\n",
    "# --- Scientific Computing ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse    import coo_matrix\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# --- Plotting ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parallel Processing ---\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Graph Processing ---\n",
    "import networkx as nx\n",
    "\n",
    "# --- JIT Compilation ---\n",
    "from numba import njit, prange\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "\n",
    "# --- Model definition ---\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f7f2b",
   "metadata": {},
   "source": [
    "# 1. Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "structural_params = {\n",
    "    'ER': {'p': [0.0443, 0.0667]},\n",
    "    'BA': {'m': [2, 3]},\n",
    "    'SW': {'beta': [4, 5]}\n",
    "}\n",
    "train_sizes   = [20, 50, 80]\n",
    "test_sizes    = [100, 200, 300, 500, 1000]\n",
    "reliability_p = [i/10 for i in range(1, 11)]  # 0.1, 0.2, ..., 1.0\n",
    "val_reliability_p = [0.15, 0.35, 0.55, 0.75, 0.95]  # for validation set\n",
    "\n",
    "n_train     = 3\n",
    "n_val       = 6\n",
    "n_test100   = 3\n",
    "n_test_large = 2\n",
    "\n",
    "base_dir     = 'data'\n",
    "graphs_dir   = os.path.join(base_dir, 'graphs')\n",
    "labels_dir   = os.path.join(base_dir, 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_dirs():\n",
    "    for split in ['train', 'val', 'test100', 'test_large', 'test_10000']:\n",
    "        os.makedirs(os.path.join(graphs_dir, split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(labels_dir, split), exist_ok=True)\n",
    "\n",
    "make_split_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca1b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Graph generation ---\n",
    "def gen_graph(topo, size, s_param, rel_p, seed):\n",
    "    random.seed(seed)\n",
    "    if topo == 'ER':\n",
    "        G = nx.erdos_renyi_graph(size, s_param, seed=seed)\n",
    "    elif topo == 'BA':\n",
    "        G = nx.barabasi_albert_graph(size, int(s_param), seed=seed)\n",
    "    elif topo == 'SW':\n",
    "        G = nx.watts_strogatz_graph(size, k=4, p=s_param, seed=seed)\n",
    "    nx.set_edge_attributes(G, rel_p, 'p')\n",
    "    # for u, v in G.edges():\n",
    "    #     G[u][v]['p'] = rel_p\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778fd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(G, meta, idx, split):\n",
    "    # save into corresponding split subfolder\n",
    "    fname = f\"{meta['topo']}_sz{meta['size']}_sp{meta['s_param']}_rp{meta['rel_p']}_{split}_{idx}.pkl\"\n",
    "    path = os.path.join(graphs_dir, split, fname)\n",
    "    # ensure the directory exists\n",
    "    dirpath = os.path.dirname(path)\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    # if a directory with the same name exists, this will fail; remove or rename it first\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump({'graph': G, 'meta': meta}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d76874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split(split, sizes, n_graphs, reliability_p):\n",
    "    for topo, params in structural_params.items():\n",
    "        key = list(params.keys())[0]\n",
    "\n",
    "        for s_param in params[key]:\n",
    "            for size in sizes:\n",
    "                for rel_p in reliability_p:\n",
    "                    for i in range(n_graphs):\n",
    "\n",
    "                        # seed = hash((topo, s_param, size, rel_p, split, i)) & 0xffffffff\n",
    "                        seed = 42\n",
    "                        G = gen_graph(topo, size, s_param, rel_p, seed)\n",
    "                        meta = {'topo': topo, 'size': size, 's_param': s_param, 'rel_p': rel_p}\n",
    "                        save_graph(G, meta, i, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bd1f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_split('train',      train_sizes,   n_train, reliability_p)\n",
    "# generate_split('val',        [100],         n_val, reliability_p=val_reliability_p)\n",
    "# generate_split('test100',    [100],         n_test100, reliability_p)\n",
    "generate_split('test_large', test_sizes[1:],n_test_large, reliability_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2672607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_to_csr(G: nx.Graph) -> Tuple[List[int], Dict[int, int], np.ndarray, np.ndarray, np.ndarray]:\n",
    "     \"\"\"Convert an undirected NetworkX graph (edge attr `'p'`) to CSR arrays.\"\"\"\n",
    "     nodes: List[int] = list(G.nodes())\n",
    "     idx_of: Dict[int, int] = {u: i for i, u in enumerate(nodes)}\n",
    "\n",
    "     indptr: List[int] = [0]\n",
    "     indices: List[int] = []\n",
    "     probs: List[float] = []\n",
    "\n",
    "     for u in nodes:\n",
    "         for v in G.neighbors(u):\n",
    "             indices.append(idx_of[v])\n",
    "             probs.append(G.edges[u, v]['p'])\n",
    "         indptr.append(len(indices))\n",
    "\n",
    "     return (\n",
    "         nodes,\n",
    "         idx_of,\n",
    "         np.asarray(indptr, dtype=np.int32),\n",
    "         np.asarray(indices, dtype=np.int32),\n",
    "         np.asarray(probs, dtype=np.float32),\n",
    "     )\n",
    "\n",
    "@njit(inline=\"always\")\n",
    "def _bfs_component_size(start: int,\n",
    "                    indptr: np.ndarray,\n",
    "                    indices: np.ndarray,\n",
    "                    probs: np.ndarray,\n",
    "                    deleted: np.ndarray) -> int:\n",
    "    \"\"\"Return |C_u|−1 for **one** random realisation (stack BFS).\"\"\"\n",
    "    n = deleted.size\n",
    "    stack = np.empty(n, dtype=np.int32)\n",
    "    visited = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "    size = 1\n",
    "    top = 0\n",
    "    stack[top] = start\n",
    "    top += 1\n",
    "    visited[start] = 1\n",
    "\n",
    "    while top:\n",
    "        top -= 1\n",
    "        v = stack[top]\n",
    "        for eid in range(indptr[v], indptr[v + 1]):\n",
    "            w = indices[eid]\n",
    "            if deleted[w]:\n",
    "                continue\n",
    "            if np.random.random() >= probs[eid]:\n",
    "                continue\n",
    "            if visited[w]:\n",
    "                continue\n",
    "            visited[w] = 1\n",
    "            stack[top] = w\n",
    "            top += 1\n",
    "            size += 1\n",
    "    return size - 1\n",
    "\n",
    "@njit(parallel=True)\n",
    "def epc_mc(indptr: np.ndarray,\n",
    "            indices: np.ndarray,\n",
    "            probs: np.ndarray,\n",
    "            deleted: np.ndarray,\n",
    "            num_samples: int) -> float:\n",
    "    \"\"\"Monte‑Carlo estimator of **expected pairwise connectivity** (EPC).\"\"\"\n",
    "    surv = np.where(~deleted)[0]\n",
    "    m = surv.size\n",
    "    if m < 2:\n",
    "        return 0.0\n",
    "\n",
    "    acc = 0.0\n",
    "    for _ in prange(num_samples):\n",
    "        u = surv[np.random.randint(m)]\n",
    "        acc += _bfs_component_size(u, indptr, indices, probs, deleted)\n",
    "\n",
    "    return (m * acc) / (2.0 * num_samples)\n",
    "\n",
    "def epc_mc_deleted(\n",
    "  G: nx.Graph,\n",
    "  S: set,\n",
    "  num_samples: int = 100_000,\n",
    ") -> float:\n",
    "  # build csr once\n",
    "  nodes, idx_of, indptr, indices, probs = nx_to_csr(G)\n",
    "  n = len(nodes)\n",
    "\n",
    "  # turn python set S into a mask (node-IDs to delete)\n",
    "  deleted = np.zeros(n, dtype=np.bool_)\n",
    "  for u in S:\n",
    "    deleted[idx_of[u]] = True\n",
    "\n",
    "  epc = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "\n",
    "  return epc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b18a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Label generation  ---\n",
    "def compute_labels(file_path, mc_samples=1_000):\n",
    "\n",
    "    data = pickle.load(open(file_path, 'rb'))\n",
    "    G_orig = data['graph']\n",
    "\n",
    "    base = epc_mc_deleted(G_orig, set(), num_samples=mc_samples)\n",
    "    \n",
    "    n = G_orig.number_of_nodes()\n",
    "    labels = torch.zeros(n)\n",
    "\n",
    "    for v in G_orig.nodes():\n",
    "        # print(v)\n",
    "        # print(type(set(v)))\n",
    "        drop = epc_mc_deleted(G_orig, {v}, num_samples=mc_samples)\n",
    "        labels[v] = base - drop\n",
    "        \n",
    "    # stabilise scale\n",
    "    labels = torch.log1p(labels.clamp(min=0))\n",
    "\n",
    "    # save labels\n",
    "    fname = os.path.basename(file_path).replace('.pkl', '_labels.pt')\n",
    "    split = os.path.basename(os.path.dirname(file_path))\n",
    "    save_path = os.path.join(labels_dir, split, fname)\n",
    "    torch.save(labels, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d94bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_cndp_epc_celf(\n",
    "    G: nx.Graph,\n",
    "    K: int,\n",
    "    *,\n",
    "    num_samples: int = 20_000,\n",
    "    reuse_csr: Tuple = None,\n",
    "    return_trace: bool = False,\n",
    ") -> Union[Set[int], Tuple[Set[int], List[float]]]:\n",
    "    \"\"\"Select **K** nodes that minimise EPC using CELF & Numba.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    return_trace : bool, default *False*\n",
    "        If *True*, also return a list `[σ(S₁), σ(S₂), …]` where `S_i` is the\n",
    "        prefix after deleting *i* nodes.  Useful for plots.\n",
    "    \"\"\"\n",
    "\n",
    "    # CSR cache --------------------------------------------------------\n",
    "    if reuse_csr is None:\n",
    "        nodes, idx_of, indptr, indices, probs = nx_to_csr(G)\n",
    "    else:\n",
    "        nodes, idx_of, indptr, indices, probs = reuse_csr\n",
    "    n = len(nodes)\n",
    "\n",
    "    deleted = np.zeros(n, dtype=np.bool_)\n",
    "    current_sigma = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "\n",
    "    pq: List[Tuple[float, int, int]] = []  # (-gain, v, last_round)\n",
    "    gains = np.empty(n, dtype=np.float32)\n",
    "\n",
    "    for v in range(n):\n",
    "        deleted[v] = True\n",
    "        gains[v] = current_sigma - epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "        deleted[v] = False\n",
    "        heapq.heappush(pq, (-gains[v], v, 0))\n",
    "\n",
    "    S: Set[int] = set()\n",
    "    trace: List[float] = []\n",
    "    round_ = 0\n",
    "\n",
    "    trace.append(current_sigma)\n",
    "\n",
    "    while len(S) < K and pq:\n",
    "        neg_gain, v, last = heapq.heappop(pq)\n",
    "        if last == round_:\n",
    "            # gain up‑to‑date → accept\n",
    "            S.add(nodes[v])\n",
    "            deleted[v] = True\n",
    "            current_sigma += neg_gain  # add neg (= subtract gain)\n",
    "            round_ += 1\n",
    "            if return_trace:\n",
    "                trace.append(current_sigma)\n",
    "        else:\n",
    "            # recompute gain lazily\n",
    "            deleted[v] = True\n",
    "            new_gain = current_sigma - epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "            deleted[v] = False\n",
    "            heapq.heappush(pq, (-new_gain, v, round_))\n",
    "\n",
    "    return (S, trace) if return_trace else S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d75e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lp_reaga_sparse(G: nx.Graph, pre_fixed: set, k: int):\n",
    "    V = list(G.nodes())\n",
    "    n = len(V)\n",
    "\n",
    "    # variables: s_i  (i = 0…n-1)      x_ij (j = 0…m2-1)\n",
    "    Pairs = [tuple(sorted(e)) for e in combinations(V, 2)]\n",
    "    m2    = len(Pairs)\n",
    "    Nvar  = n + m2\n",
    "    s_idx = {v: i         for i, v in enumerate(V)}\n",
    "    x_idx = {e: n + j     for j, e in enumerate(Pairs)}\n",
    "\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    rhs              = []\n",
    "\n",
    "    def add_coef(r, c, val):\n",
    "        rows.append(r); cols.append(c); data.append(val)\n",
    "\n",
    "    r = 0 \n",
    "\n",
    "    # budget \n",
    "    for i in range(n):\n",
    "        add_coef(r, i, 1.0)\n",
    "    rhs.append(k); r += 1\n",
    "\n",
    "    # edge upper bounds  x_uv − s_u − s_v ≤ 1 − p_uv\n",
    "    for (u, v) in G.edges():\n",
    "        u, v   = sorted((u, v))\n",
    "        puv    = G.edges[u, v]['p']\n",
    "        add_coef(r, x_idx[(u, v)],  1.0)\n",
    "        add_coef(r, s_idx[u],      -1.0)\n",
    "        add_coef(r, s_idx[v],      -1.0)\n",
    "        rhs.append(1 - puv); r += 1\n",
    "\n",
    "    # triangle cuts for each real edge (i,j) and every\n",
    "    for (i, j) in G.edges():\n",
    "        i, j = sorted((i, j))\n",
    "        for k_ in V:\n",
    "            if k_ == i or k_ == j:\n",
    "                continue\n",
    "            add_coef(r, x_idx[tuple(sorted((i, k_)))],  1.0)  \n",
    "            add_coef(r, x_idx[(i, j)]               , -1.0)  \n",
    "            add_coef(r, x_idx[tuple(sorted((j, k_)))], -1.0)   \n",
    "            rhs.append(0.0); r += 1\n",
    "\n",
    "    n_rows = r\n",
    "    A_ub   = coo_matrix((data, (rows, cols)), shape=(n_rows, Nvar)).tocsr()\n",
    "    b_ub   = np.asarray(rhs)\n",
    "\n",
    "    # bounds \n",
    "    bounds = [(0.0, 1.0)] * Nvar\n",
    "    for v in pre_fixed:\n",
    "        bounds[s_idx[v]] = (1.0, 1.0)\n",
    "\n",
    "    #  objective \n",
    "    c = np.zeros(Nvar)\n",
    "    for e in Pairs:\n",
    "        c[x_idx[e]] = -1.0\n",
    "\n",
    "    # \n",
    "    res = linprog(c, A_ub=A_ub, b_ub=b_ub,\n",
    "                  bounds=bounds, method=\"highs\")\n",
    "    if not res.success:\n",
    "        raise RuntimeError(\"LP infeasible: \" + res.message)\n",
    "\n",
    "    #\n",
    "    s_vals = {v: res.x[s_idx[v]] for v in V}\n",
    "    x_sum  = res.x[n:].sum()\n",
    "    obj    = len(Pairs) - x_sum\n",
    "    return s_vals, obj\n",
    "\n",
    "def local_search_(\n",
    "  G: nx.Graph,\n",
    "  S_init: set,\n",
    "  num_samples: int = 10_000\n",
    "):\n",
    "  \"\"\"1-swap local search\"\"\"\n",
    "\n",
    "  S = S_init.copy()\n",
    "  nodes_not_in_set = set(G.nodes()) - S\n",
    "\n",
    "  current_epc = epc_mc_deleted(G, S, num_samples)\n",
    "\n",
    "  improved = True\n",
    "  while improved:\n",
    "    improved = False\n",
    "    best_swap = None\n",
    "\n",
    "    for u in list(S):\n",
    "      for v in nodes_not_in_set:        \n",
    "        \n",
    "        D_new = (S - {u}) | {v}\n",
    "\n",
    "        temp_epc = epc_mc_deleted(G, D_new, num_samples)\n",
    "\n",
    "        if temp_epc < current_epc:\n",
    "            current_epc = temp_epc\n",
    "            best_swap = (u, v)\n",
    "            improved = True\n",
    "\n",
    "    if improved and best_swap:\n",
    "      u, v = best_swap\n",
    "\n",
    "      S.remove(u)\n",
    "      S.add(v)\n",
    "      nodes_not_in_set.remove(v)\n",
    "      nodes_not_in_set.add(u)\n",
    "  \n",
    "  return S\n",
    "\n",
    "def rega(G: nx.Graph,\n",
    "        k: int,\n",
    "        num_samples: int = 100_000,\n",
    "        max_iter: int = 1,\n",
    "        # epsilon: float = None,\n",
    "        # delta: float = None,\n",
    "        use_tqdm: bool = False):\n",
    "    \"\"\"\n",
    "    Full REGA pipeline: LP‐rounding + CSP‐refined local swaps.\n",
    "    \"\"\"\n",
    "\n",
    "    csr = nx_to_csr(G)\n",
    "\n",
    "    # iterative rounding\n",
    "    D = set()\n",
    "    for _ in range(k):\n",
    "      # s_vals, _ = solve_lp_(G, pre_fixed=D, k=k)\n",
    "      s_vals, _ = solve_lp_reaga_sparse(G, pre_fixed=D, k=k)\n",
    "\n",
    "      # pick the fractional s_i largest among V\\D\n",
    "      u = max((v for v in G.nodes() if v not in D),\n",
    "              key=lambda v: s_vals[v])\n",
    "      D.add(u)\n",
    "\n",
    "    # local‐swap refinement\n",
    "\n",
    "    S_opt = local_search_(G, D, num_samples)\n",
    "    \n",
    "    # S_opt = local_search_(G, greedy_es_S, num_samples)\n",
    "\n",
    "    # S_opt = local_search_swap(\n",
    "    #   D, csr=csr, num_samples=num_samples, max_iter=max_iter)\n",
    "    \n",
    "    # improved = True\n",
    "    \n",
    "    # while improved:\n",
    "\n",
    "    #     improved = False\n",
    "    #     best_epc = current_epc\n",
    "    #     best_swap = None\n",
    "\n",
    "    #     for u in list(D):\n",
    "    #         for v in G.nodes():\n",
    "\n",
    "    #             if v in D: \n",
    "    #                 continue\n",
    "\n",
    "    #             D_new = (D - {u}) | {v}\n",
    "\n",
    "    #             epc_val = epc_func(G, D_new,\n",
    "    #                                num_samples=num_samples,\n",
    "    #                             #    epsilon=epsilon,\n",
    "    #                             #    delta=delta,\n",
    "    #                             #    use_tqdm=use_tqdm\n",
    "    #                                )\n",
    "                \n",
    "    #             if epc_val < best_epc:\n",
    "    #                 best_epc = epc_val\n",
    "    #                 best_swap = (u, v)\n",
    "\n",
    "    #     if best_swap is not None:\n",
    "\n",
    "    #         u, v = best_swap\n",
    "    #         D.remove(u)\n",
    "    #         D.add(v)\n",
    "    #         current_epc = best_epc\n",
    "    #         improved = True\n",
    "\n",
    "    return S_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143e92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS_ROOT  = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs\"     # expecting graphs/<split>/<type>/*.pkl\n",
    "LABELS_ROOT  = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/rega_labels\"     # will mirror the same structure\n",
    "\n",
    "# ----- budget percentage -----\n",
    "ALPHA        = 0.10         # 10 % of nodes\n",
    "\n",
    "# ----- MC parameters -----\n",
    "MC_SAMPLES   = 10_000       # inside greedy\n",
    "MC_EPC_SAVE  = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a30d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_label(pkl_path: str, alpha: float = ALPHA):\n",
    "    # --- load --------------------------------------------------------\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        G = pickle.load(f)[\"graph\"]\n",
    "\n",
    "    N = G.number_of_nodes()\n",
    "    K = max(1, math.ceil(alpha * N))\n",
    "\n",
    "    # print(f\"K: {K}\")\n",
    "    \n",
    "    # --- greedy delete set ------------------------------------------\n",
    "    delete_set = rega(G, K, num_samples=MC_SAMPLES)\n",
    "\n",
    "    # --- binary mask -------------------------------------------------\n",
    "    mask = torch.zeros(N, dtype=torch.float32)\n",
    "    mask[list(delete_set)] = 1.0\n",
    "\n",
    "    # --- save --------------------------------------------------------\n",
    "    #  graphs/<split>/<type>/foo.pkl  ->  labels/<split>/<type>/foo_labels.pt\n",
    "    rel_dir   = os.path.relpath(os.path.dirname(pkl_path), GRAPHS_ROOT)\n",
    "    save_dir  = os.path.join(LABELS_ROOT, rel_dir)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    fname_out = os.path.basename(pkl_path).replace(\".pkl\", \"_labels.pt\")\n",
    "    torch.save(mask, os.path.join(save_dir, fname_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6191b44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building greedy labels: 100%|██████████| 900/900 [3:19:13<00:00, 13.28s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Finished.  All binary-mask labels written to /home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/rega_labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# label_path = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs\"\n",
    "\n",
    "all_graphs = glob.glob(os.path.join(GRAPHS_ROOT, '*', '*.pkl'))\n",
    "\n",
    "for fp in tqdm(all_graphs, desc=\"building greedy labels\"):\n",
    "    build_and_save_label(fp, alpha=ALPHA)\n",
    "\n",
    "print(\"✓ Finished.  All binary-mask labels written to\", LABELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e1945e",
   "metadata": {},
   "source": [
    "## Previous label generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2026b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing labels:   0%|          | 0/1080 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m all_graphs \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(graphs_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m tqdm(all_graphs, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_graphs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing labels\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mcompute_labels\u001b[49m(fp, \u001b[38;5;241m10_000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "all_graphs = glob.glob(os.path.join(graphs_dir, '*', '*.pkl'))\n",
    "\n",
    "for fp in tqdm(all_graphs, total=len(all_graphs), desc=\"Computing labels\"):\n",
    "    compute_labels(fp, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"path/to/your_file.pt\", map_location=\"cpu\")\n",
    "\n",
    "# 2. See what you got:\n",
    "print(type(data))\n",
    "# e.g. <class 'dict'> (often a state_dict) or a ScriptModule\n",
    "\n",
    "# 3. If it’s a dict of tensors (state_dict):\n",
    "if isinstance(data, dict):\n",
    "    for k, v in data.items():\n",
    "        print(f\"{k:40s} → {tuple(v.shape) if hasattr(v, 'shape') else type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c71db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "base_folder = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels\"         # Folder with .pt files\n",
    "output_folder = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels_txt\"   # Destination folder for .txt files\n",
    "\n",
    "for root, _, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pt\"):\n",
    "            pt_path = os.path.join(root, file)\n",
    "            \n",
    "            # Load tensor or model (depending on format)\n",
    "            try:\n",
    "                content = torch.load(pt_path, map_location='cpu')\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {pt_path} due to load error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Generate corresponding txt path\n",
    "            rel_path = os.path.relpath(pt_path, base_folder)\n",
    "            txt_path = os.path.join(output_folder, os.path.splitext(rel_path)[0] + \".txt\")\n",
    "            os.makedirs(os.path.dirname(txt_path), exist_ok=True)\n",
    "\n",
    "            # Write to text file\n",
    "            try:\n",
    "                with open(txt_path, \"w\") as f:\n",
    "                    f.write(str(content))\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing {txt_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a58222",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bab057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neigbors(g, node, depth):\n",
    "    output = {}\n",
    "    layers = dict(nx.bfs_successors(g, source=node, depth_limit=depth))\n",
    "    nodes = [node]\n",
    "    for i in range(1, depth + 1):\n",
    "        output[i] = []\n",
    "        for x in nodes:\n",
    "            output[i].extend(layers.get(x, []))\n",
    "        nodes = output[i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87959b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgl_g_input(G):\n",
    "    input = torch.ones(len(G), 11)\n",
    "    for i in G.nodes():\n",
    "        input[i, 0] = G.degree()[i]\n",
    "        input[i, 1] = sum([G.degree()[j] for j in list(G.neighbors(i))]) / max(len(list(G.neighbors(i))), 1)\n",
    "        input[i, 2] = sum([nx.clustering(G, j) for j in list(G.neighbors(i))]) / max(len(list(G.neighbors(i))), 1)\n",
    "        egonet = G.subgraph(list(G.neighbors(i)) + [i])\n",
    "        input[i, 3] = len(egonet.edges())\n",
    "        input[i, 4] = sum([G.degree()[j] for j in egonet.nodes()]) - 2 * input[i, 3]\n",
    "\n",
    "    for l in [1, 2, 3]:\n",
    "        for i in G.nodes():\n",
    "            ball = get_neigbors(G, i, l)\n",
    "            input[i, 5 + l - 1] = (G.degree()[i] - 1) * sum([G.degree()[j] - 1 for j in ball[l]])\n",
    "\n",
    "    v = nx.voterank(G)\n",
    "    votescore = dict()\n",
    "    \n",
    "    for i in list(G.nodes()): votescore[i] = 0\n",
    "    for i in range(len(v)):\n",
    "        votescore[v[i]] = len(G) - i\n",
    "    e = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    k = nx.core_number(G)\n",
    "    for i in G.nodes():\n",
    "        input[i, 8] = votescore[i]\n",
    "        input[i, 9] = e[i]\n",
    "        input[i, 10] = k[i]\n",
    "    for i in range(len(input[0])):\n",
    "        if max(input[:, i]) != 0:\n",
    "            input[:, i] = input[:, i] / max(input[:, i])\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345313a3",
   "metadata": {},
   "source": [
    "# 3. Traininig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11336b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node_features(G):\n",
    "    \"\"\"\n",
    "    Compute per-node structural features for DGL input.\n",
    "    Returns: torch.FloatTensor of shape [num_nodes, 11]\n",
    "    Features:\n",
    "      0: degree\n",
    "      1: avg neighbor degree\n",
    "      2: avg neighbor clustering coeff\n",
    "      3: egonet edge count\n",
    "      4: egonet sum-degree minus internal edges (volume)\n",
    "      5-7: l-hop neighbor sum-degree offsets for l=1,2,3\n",
    "      8: voterank score\n",
    "      9: eigenvector centrality\n",
    "     10: k-core number\n",
    "    Normalized per feature by dividing by feature-wise max.\n",
    "    \"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    feats = torch.ones(n, 11)\n",
    "\n",
    "    # precompute degrees and clustering\n",
    "    deg = dict(G.degree())\n",
    "    clust = nx.clustering(G)\n",
    "\n",
    "    # voterank ordering and scoring\n",
    "    order = nx.voterank(G)\n",
    "    vote_score = {u: n - i for i, u in enumerate(order)}\n",
    "\n",
    "    # eigenvector centrality\n",
    "    eig = nx.eigenvector_centrality(G, max_iter=500)\n",
    "    core = nx.core_number(G)\n",
    "\n",
    "    # compute for each node\n",
    "    for u in G.nodes():\n",
    "        nbrs = list(G.neighbors(u))\n",
    "        feats[u, 0] = deg[u]\n",
    "        feats[u, 1] = sum(deg[v] for v in nbrs) / max(len(nbrs), 1)\n",
    "        feats[u, 2] = sum(clust[v] for v in nbrs) / max(len(nbrs), 1)\n",
    "        egonet = G.subgraph(nbrs + [u])\n",
    "        feats[u, 3] = egonet.number_of_edges()\n",
    "        feats[u, 4] = sum(deg[v] for v in egonet.nodes()) - 2 * feats[u, 3]\n",
    "        # l-hop neighbor sums\n",
    "        for l in (1,2,3):\n",
    "            # BFS up to l hops\n",
    "            visited = {u}\n",
    "            queue = deque([(u, 0)])\n",
    "            hop_nodes = set()\n",
    "            while queue:\n",
    "                v, d = queue.popleft()\n",
    "                if d == l: continue\n",
    "                for w in G.neighbors(v):\n",
    "                    if w not in visited:\n",
    "                        visited.add(w)\n",
    "                        queue.append((w, d+1))\n",
    "                        if d+1 == l:\n",
    "                            hop_nodes.add(w)\n",
    "            feats[u, 4 + l] = sum(deg[v] - 1 for v in hop_nodes)\n",
    "        feats[u, 8] = vote_score.get(u, 0)\n",
    "        feats[u, 9] = eig.get(u, 0)\n",
    "        feats[u, 10] = core.get(u, 0)\n",
    "\n",
    "    # normalize each feature dimension\n",
    "    for i in range(feats.size(1)):\n",
    "        col = feats[:, i]\n",
    "        maxval = col.max()\n",
    "        if maxval > 0:\n",
    "            feats[:, i] = col / maxval\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82417ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAGE2AttnModel_fanout(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden_dim=128, num_heads=2):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         self.norms  = nn.ModuleList()\n",
    "\n",
    "#         fanouts = [15, 10, 5]\n",
    "\n",
    "#         for _ in fanouts:\n",
    "#             self.layers.append(SAGEConv(in_dim, hidden_dim, 'lstm'))\n",
    "#             self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "#             in_dim = hidden_dim\n",
    "            \n",
    "#         self.attn = nn.MultiheadAttention(hidden_dim, num_heads)\n",
    "#         self.out  = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, g, x):\n",
    "#         h = x\n",
    "#         for sage, norm in zip(self.layers, self.norms):\n",
    "#             h = F.relu(norm(sage(g, h)))\n",
    "#         # attention works with seq_len x batch_size x hidden\n",
    "#         # here nodes as sequence, batch=1\n",
    "#         h2, _ = self.attn(h.unsqueeze(1), h.unsqueeze(1), h.unsqueeze(1))\n",
    "#         h2 = h2.squeeze(1)\n",
    "#         return self.out(h2).squeeze(-1)\n",
    "\n",
    "class SAGE2AttnModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE + attention model using full-graph (no neighbor sampling).\n",
    "    Three SAGEConv layers followed by multi-head self-attention and output head.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim=128, num_heads=2, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms  = nn.ModuleList()\n",
    "        # Build fixed number of layers without explicit fan-out\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(SAGEConv(in_dim, hidden_dim, 'lstm'))\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        # self-attention across all nodes\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads)\n",
    "        self.out  = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        h = x\n",
    "        # message-passing over full graph\n",
    "        for sage, norm in zip(self.layers, self.norms):\n",
    "            h = F.relu(norm(sage(g, h)))\n",
    "        # apply self-attention: treat nodes as sequence length\n",
    "        h2, _ = self.attn(h.unsqueeze(1), h.unsqueeze(1), h.unsqueeze(1))\n",
    "        h2 = h2.squeeze(1)\n",
    "        return self.out(h2).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14507a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class GraphEPCDataset(Dataset):\n",
    "    def __init__(self, graphs_dir, labels_dir, split):\n",
    "        self.graph_paths = glob.glob(os.path.join(graphs_dir, split, '*.pkl'))\n",
    "        self.labels_dir  = os.path.join(labels_dir, split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graph_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --------  load graph ----------\n",
    "        path = self.graph_paths[idx]\n",
    "        G_nx  = pickle.load(open(path, 'rb'))['graph']\n",
    "\n",
    "        # node-level features\n",
    "        x = extract_node_features(G_nx)          # [N, 11]\n",
    "\n",
    "        # edge index & probabilities\n",
    "        edges      = list(G_nx.edges())\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)  # undirected\n",
    "        p_list     = [G_nx[u][v]['p'] for u, v in edges]\n",
    "        edge_prob  = torch.tensor(p_list + p_list, dtype=torch.float)\n",
    "\n",
    "        # labels  (make sure they are float for MSELoss)\n",
    "        lbl_name = os.path.basename(path).replace('.pkl', '_labels.pt')\n",
    "        y        = torch.load(os.path.join(self.labels_dir, lbl_name)).float()\n",
    "\n",
    "        # --------  wrap in Data ----------\n",
    "        data = Data(x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_prob=edge_prob,\n",
    "                    y=y)   \n",
    "        \n",
    "        data.file_name = os.path.basename(path)\n",
    "        data.idx = torch.tensor(idx, dtype=torch.long)  # add index for reference\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e630be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "class EdgeProbGATConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 heads: int = 2,\n",
    "                 negative_slope: float = 0.2,\n",
    "                 dropout: float = 0.2,\n",
    "                 concat: bool = True,\n",
    "                 bias: bool = True):\n",
    "        super().__init__(aggr='add', node_dim=0)  # standard GAT aggregation\n",
    "        \n",
    "        self.in_channels   = in_channels\n",
    "        self.out_channels  = out_channels\n",
    "        self.heads         = heads\n",
    "        self.negative_slope= negative_slope\n",
    "        self.dropout       = dropout\n",
    "        self.concat        = concat\n",
    "\n",
    "        # Linear projection for query/key/value\n",
    "        self.lin = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        # Attention weights aᵀ [Wh_i || Wh_j]\n",
    "        self.att = nn.Parameter(torch.Tensor(1, heads, 2*out_channels + 1))\n",
    "        # self.att = nn.Parameter(torch.Tensor(1, heads, 2*out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "        nn.init.xavier_uniform_(self.att)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                edge_index: torch.LongTensor,\n",
    "                edge_prob: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [N, in_channels]\n",
    "        edge_index: [2, E]\n",
    "        edge_prob: [E]   (the p_ij for each edge in edge_index order)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        # 1. Linearly project node features to multi-head space\n",
    "        x = self.lin(x)                              # [N, heads*out]\n",
    "        x = x.view(N, self.heads, self.out_channels) # [N, heads, out]\n",
    "\n",
    "        # 2. Start propagation\n",
    "        out = self.propagate(edge_index, x=x, edge_prob=edge_prob, size=(N, N))\n",
    "        # out: [N, heads, out]\n",
    "\n",
    "        # 3. Concat or average heads\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # [N, out]\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    # def message(self,\n",
    "    #             x_j: torch.Tensor,\n",
    "    #             x_i: torch.Tensor,\n",
    "    #             edge_prob: torch.Tensor,\n",
    "    #             index: torch.LongTensor,\n",
    "    #             ptr,\n",
    "    #             size_i):\n",
    "    #     \"\"\"\n",
    "    #     x_j, x_i: [E, heads, out_channels] (sender and receiver node reps)\n",
    "    #     edge_prob: [E]           (scalar reliability)\n",
    "    #     index:   [E]             (destination node indices)\n",
    "    #     \"\"\"\n",
    "    #     # 1. compute standard attention logits: aᵀ [Wh_i || Wh_j]\n",
    "    #     cat = torch.cat([x_i, x_j], dim=-1)           # [E, heads, 2*out]\n",
    "    #     alpha = (cat * self.att).sum(dim=-1)          # [E, heads]\n",
    "\n",
    "    #     # 2. add log(edge_prob)\n",
    "    #     log_p = edge_prob.log().unsqueeze(-1)        # [E, 1]\n",
    "    #     alpha = alpha + log_p                        # broadcasting to [E, heads]\n",
    "\n",
    "    #     # 3. leaky‐relu + softmax over all incoming edges\n",
    "    #     alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "    #     alpha = softmax(alpha, index, ptr, size_i)    # [E, heads]\n",
    "\n",
    "    #     # 4. dropout on attention weights\n",
    "    #     alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "    #     # 5. scale messages\n",
    "    #     return x_j * alpha.unsqueeze(-1)             # [E, heads, out]\n",
    "\n",
    "    def message(self, x_j, x_i, edge_prob, index, ptr, size_i):\n",
    "        # concat node reps and edge scalar\n",
    "        edge_prob = edge_prob.view(-1, 1, 1)               # [E,1,1]\n",
    "        cat = torch.cat([x_i, x_j, edge_prob.expand(-1, self.heads, 1)], dim=-1)\n",
    "        # shape: [E, heads, 2*out+1]\n",
    "\n",
    "        alpha = (cat * self.att).sum(dim=-1)               # [E, heads]\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha.unsqueeze(-1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out: [N, heads, out] if concat else [N, out]\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4494fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEEdgeProbModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256, heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = SAGEConv(in_dim,  hidden_dim, normalize=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim, normalize=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim, normalize=True)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # now our custom GAT that adds log(p_ij)\n",
    "        self.gat_edge = EdgeProbGATConv(hidden_dim, hidden_dim, \n",
    "                                        heads=heads, dropout=0.3)\n",
    "        \n",
    "        self.out       = nn.Linear(heads * hidden_dim, 1)  # if concat=True\n",
    "\n",
    "    def forward(self, x, edge_index, edge_prob):\n",
    "        # x: [N, in_dim], edge_prob: [E]\n",
    "        h = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = F.relu(self.bn2(self.conv2(h, edge_index))) + h\n",
    "        h = F.relu(self.bn3(self.conv3(h, edge_index))) + h\n",
    "\n",
    "        # incorporate per-edge probabilities\n",
    "        h = self.gat_edge(h, edge_index, edge_prob)  # [N, heads*out]\n",
    "\n",
    "        return self.out(h).squeeze(-1)               # [N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a4b8b",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "def train_model():\n",
    "    SEED = 42\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    in_dim = 11                   # keep your original setting\n",
    "    model  = SAGEEdgeProbModel(in_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn  = nn.MSELoss()\n",
    "\n",
    "    train_ds = GraphEPCDataset(graphs_dir, labels_dir, 'train')\n",
    "    val_ds   = GraphEPCDataset(graphs_dir, labels_dir, 'val')\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=32)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    for epoch in range(1, 31):\n",
    "        # -------------------- training --------------------\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for data in train_loader:                   # data is a Batch\n",
    "            data = data.to(device)                  \n",
    "            preds = model(data.x, data.edge_index, data.edge_prob)\n",
    "            loss  = loss_fn(preds, data.y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # -------------------- validation ------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                val_loss += loss_fn(\n",
    "                    model(data.x, data.edge_index, data.edge_prob),\n",
    "                    data.y\n",
    "                ).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: Train={avg_loss:.4f} | Val={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(base_dir, 'best_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e3758",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f18f9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS_DIR = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs\"\n",
    "LABELS_DIR = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs_labels\"\n",
    "LABELS_OLD_DIR = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef39ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 256\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2c7029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma: float = 2.0, alpha: float | None = None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        # optional α-balancing (same role as pos_weight)\n",
    "        self.alpha = alpha            # scalar ∈ (0,1) or None\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: [N], targets: 0/1 floats\n",
    "        prob = torch.sigmoid(logits)\n",
    "        pt   = prob * targets + (1 - prob) * (1 - targets)   # p_t\n",
    "        focal = (1 - pt) ** self.gamma\n",
    "        logp  = F.binary_cross_entropy_with_logits(\n",
    "                    logits, targets, reduction='none')\n",
    "        if self.alpha is not None:\n",
    "            α_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            logp = α_t * logp\n",
    "        return (focal * logp).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01ec889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "import torch.nn as nn, torch\n",
    "\n",
    "base_dir     = 'data'\n",
    "\n",
    "def train_model():\n",
    "    SEED = 42\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ---------------- model ----------------\n",
    "    model = SAGEEdgeProbModel(in_dim=11).to(device)\n",
    "    optimizer  = torch.optim.AdamW(model.parameters(), \n",
    "                                   lr=1e-3, weight_decay=1e-4)\n",
    "    # scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #                optimizer, mode='min', factor=0.5, patience=4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=4, min_lr=1e-5)\n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    #           optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    pos_weight = torch.tensor(9.0, device=device)\n",
    "    # loss_fn = nn.MSELoss()  # for regression task\n",
    "    loss_fn    = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    # loss_fn = FocalLoss(gamma=2.0, alpha=0.10).to(device)\n",
    "\n",
    "    # ---------------- data -----------------\n",
    "    train_ds = GraphEPCDataset(GRAPHS_DIR, LABELS_DIR, 'train')\n",
    "    val_ds   = GraphEPCDataset(GRAPHS_DIR, LABELS_DIR, 'val')\n",
    "\n",
    "    train_loader = PyGDataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
    "    val_loader   = PyGDataLoader(val_ds,   batch_size=BATCH)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # ---------- training ----------\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_prob)\n",
    "            loss   = loss_fn(logits, batch.y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # scheduler.step()\n",
    "        # ---------- validation ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        auroc = BinaryAUROC().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                logits = model(batch.x, batch.edge_index, batch.edge_prob)\n",
    "                auroc.update(logits, batch.y.int())\n",
    "                val_loss += loss_fn(logits, batch.y).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train {avg_loss:.4f} \"\n",
    "              f\"| val {val_loss:.4f} | AUROC {auroc.compute():.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(base_dir, 'best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4c82ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 1.3924 | val 1.2341 | AUROC 0.8254\n",
      "Epoch 02 | train 1.3658 | val 1.2441 | AUROC 0.8197\n",
      "Epoch 03 | train 1.5095 | val 1.2164 | AUROC 0.8079\n",
      "Epoch 04 | train 1.0844 | val 1.2181 | AUROC 0.8481\n",
      "Epoch 05 | train 0.9476 | val 1.2262 | AUROC 0.8237\n",
      "Epoch 06 | train 0.8391 | val 1.2064 | AUROC 0.8447\n",
      "Epoch 07 | train 0.8221 | val 1.2100 | AUROC 0.8608\n",
      "Epoch 08 | train 0.7744 | val 1.2022 | AUROC 0.8467\n",
      "Epoch 09 | train 0.7763 | val 1.2063 | AUROC 0.8724\n",
      "Epoch 10 | train 0.7343 | val 1.2113 | AUROC 0.8669\n",
      "Epoch 11 | train 0.7883 | val 1.2202 | AUROC 0.8662\n",
      "Epoch 12 | train 0.7593 | val 1.1398 | AUROC 0.8875\n",
      "Epoch 13 | train 0.7104 | val 1.2218 | AUROC 0.8654\n",
      "Epoch 14 | train 0.6668 | val 1.1726 | AUROC 0.8761\n",
      "Epoch 15 | train 0.6537 | val 1.1232 | AUROC 0.8803\n",
      "Epoch 16 | train 0.6384 | val 1.0037 | AUROC 0.8878\n",
      "Epoch 17 | train 0.6967 | val 1.0711 | AUROC 0.8941\n",
      "Epoch 18 | train 0.6778 | val 0.9484 | AUROC 0.8964\n",
      "Epoch 19 | train 0.6364 | val 0.8916 | AUROC 0.9034\n",
      "Epoch 20 | train 0.6313 | val 0.9571 | AUROC 0.8997\n",
      "Epoch 21 | train 0.6588 | val 0.7273 | AUROC 0.9077\n",
      "Epoch 22 | train 0.6575 | val 0.9305 | AUROC 0.8956\n",
      "Epoch 23 | train 0.6597 | val 0.7533 | AUROC 0.9080\n",
      "Epoch 24 | train 0.6274 | val 0.8576 | AUROC 0.9028\n",
      "Epoch 25 | train 0.6048 | val 0.6831 | AUROC 0.9103\n",
      "Epoch 26 | train 0.6228 | val 0.8118 | AUROC 0.9095\n",
      "Epoch 27 | train 0.6085 | val 0.6896 | AUROC 0.9112\n",
      "Epoch 28 | train 0.6365 | val 0.7293 | AUROC 0.9063\n",
      "Epoch 29 | train 0.5969 | val 0.7148 | AUROC 0.9159\n",
      "Epoch 30 | train 0.5806 | val 0.7284 | AUROC 0.9122\n",
      "Epoch 31 | train 0.6226 | val 0.6895 | AUROC 0.9119\n",
      "Epoch 32 | train 0.5810 | val 0.7066 | AUROC 0.9128\n",
      "Epoch 33 | train 0.5936 | val 0.7269 | AUROC 0.9100\n",
      "Epoch 34 | train 0.6048 | val 0.6882 | AUROC 0.9129\n",
      "Epoch 35 | train 0.5918 | val 0.6797 | AUROC 0.9141\n",
      "Epoch 36 | train 0.5914 | val 0.7163 | AUROC 0.9111\n",
      "Epoch 37 | train 0.5607 | val 0.6823 | AUROC 0.9132\n",
      "Epoch 38 | train 0.5538 | val 0.7104 | AUROC 0.9130\n",
      "Epoch 39 | train 0.5634 | val 0.6909 | AUROC 0.9130\n",
      "Epoch 40 | train 0.5110 | val 0.7090 | AUROC 0.9120\n",
      "Epoch 41 | train 0.5531 | val 0.6932 | AUROC 0.9141\n",
      "Epoch 42 | train 0.5208 | val 0.6870 | AUROC 0.9138\n",
      "Epoch 43 | train 0.5149 | val 0.6920 | AUROC 0.9129\n",
      "Epoch 44 | train 0.5393 | val 0.7118 | AUROC 0.9124\n",
      "Epoch 45 | train 0.5187 | val 0.6954 | AUROC 0.9136\n",
      "Epoch 46 | train 0.5282 | val 0.6895 | AUROC 0.9141\n",
      "Epoch 47 | train 0.4942 | val 0.6869 | AUROC 0.9143\n",
      "Epoch 48 | train 0.4944 | val 0.6956 | AUROC 0.9136\n",
      "Epoch 49 | train 0.5321 | val 0.6975 | AUROC 0.9131\n",
      "Epoch 50 | train 0.5191 | val 0.6971 | AUROC 0.9125\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff267894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  10%|█         | 6/60 [00:00<00:00, 55.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  ['ER_sz100_sp0.0443_rp1.0_test100_2.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  32%|███▏      | 19/60 [00:00<00:01, 35.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp1.0_test100_2.pkl  EPC₀=4751.1  after=3652.9  Δ=-1098.2\n",
      "Graph 009 | EPC(after delete) = 3652.9092 | top-K = [87, 27, 25, 1, 49, 60, 92, 94, 99, 57]\n",
      "top-scores id,logit:\n",
      "[(87, 5.062656402587891), (27, 2.9162180423736572), (25, 2.57100510597229), (1, 2.172837495803833), (49, 2.0008339881896973)]\n",
      "file:  ['ER_sz100_sp0.0443_rp0.9_test100_1.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  50%|█████     | 30/60 [00:01<00:01, 28.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_1.pkl  EPC₀=4815.7  after=3651.8  Δ=-1163.8\n",
      "Graph 020 | EPC(after delete) = 3651.8450 | top-K = [38, 23, 89, 98, 76, 70, 34, 29, 30, 55]\n",
      "top-scores id,logit:\n",
      "[(38, 2.3899905681610107), (23, 2.260484457015991), (89, 2.113187313079834), (98, 1.8053447008132935), (76, 1.6229392290115356)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  57%|█████▋    | 34/60 [00:01<00:00, 27.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  ['ER_sz100_sp0.0443_rp0.9_test100_0.pkl']\n",
      "ER_sz100_sp0.0443_rp0.9_test100_0.pkl  EPC₀=4889.8  after=3558.0  Δ=-1331.8\n",
      "Graph 033 | EPC(after delete) = 3558.0082 | top-K = [75, 17, 25, 53, 74, 50, 57, 66, 71, 44]\n",
      "top-scores id,logit:\n",
      "[(75, 3.572188138961792), (17, 3.3117687702178955), (25, 3.134037494659424), (53, 2.73591685295105), (74, 2.701092481613159)]\n",
      "file:  ['ER_sz100_sp0.0443_rp0.9_test100_2.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  65%|██████▌   | 39/60 [00:01<00:00, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_2.pkl  EPC₀=4801.1  after=3487.3  Δ=-1313.8\n",
      "Graph 038 | EPC(after delete) = 3487.3290 | top-K = [82, 90, 4, 59, 95, 93, 28, 46, 44, 17]\n",
      "top-scores id,logit:\n",
      "[(82, 2.9729554653167725), (90, 2.7378523349761963), (4, 2.167814016342163), (59, 2.1636431217193604), (95, 2.11735463142395)]\n",
      "file:  ['ER_sz100_sp0.0443_rp1.0_test100_1.pkl']\n",
      "ER_sz100_sp0.0443_rp1.0_test100_1.pkl  EPC₀=4950.0  after=3827.2  Δ=-1122.8\n",
      "Graph 043 | EPC(after delete) = 3827.1865 | top-K = [17, 29, 8, 26, 13, 81, 47, 2, 74, 7]\n",
      "top-scores id,logit:\n",
      "[(17, 4.122374534606934), (29, 3.8109734058380127), (8, 2.849382162094116), (26, 2.6491904258728027), (13, 2.555166482925415)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  78%|███████▊  | 47/60 [00:01<00:00, 24.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  ['ER_sz100_sp0.0443_rp1.0_test100_0.pkl']\n",
      "ER_sz100_sp0.0443_rp1.0_test100_0.pkl  EPC₀=4750.2  after=3652.9  Δ=-1097.3\n",
      "Graph 045 | EPC(after delete) = 3652.8750 | top-K = [18, 63, 57, 5, 54, 23, 84, 22, 33, 30]\n",
      "top-scores id,logit:\n",
      "[(18, 2.717495918273926), (63, 2.287719488143921), (57, 2.092172861099243), (5, 1.7826387882232666), (54, 1.7607983350753784)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 60/60 [00:02<00:00, 29.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average EPC over 6 test graphs: 3638.3588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "ROOT = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data\"\n",
    "\n",
    "graphs_dir   = f\"{ROOT}/graphs/test_100_separate\"          # same dirs you used for train/val\n",
    "labels_dir   = f\"{ROOT}/graphs_labels/test_100_separate\"          # not needed for inference but Dataset expects it\n",
    "ckpt_path    = f\"{ROOT}/best_model_bse_50.pt\" # saved in train_model()\n",
    "K            = 10                     # number of nodes to delete\n",
    "mc_samples   = 100_000                 # per-graph Monte-Carlo samples for EPC\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  DATASET & DATALOADER  (batch_size = 1 for clarity)\n",
    "# ------------------------------------------------------------\n",
    "test_ds      = GraphEPCDataset(graphs_dir, labels_dir, split=\"ER\")\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  LOAD MODEL\n",
    "# ------------------------------------------------------------\n",
    "in_dim       = 11                     # you decided to keep only the 11 node features\n",
    "model        = SAGEEdgeProbModel(in_dim).to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  INFERENCE + EPC\n",
    "# ------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_epc = []\n",
    "i = 0\n",
    "\n",
    "for data in tqdm(test_loader, desc=\"Inference\"):\n",
    "\n",
    "    # if i == 30:\n",
    "    #     break\n",
    "    # move everything to GPU/CPU\n",
    "    data = data.to(device)\n",
    "\n",
    "    fname = data.file_name[0]\n",
    "    \n",
    "    if \"sp0.0443\" in fname and (\"rp0.9\" in fname or \"rp1.0\" in fname):\n",
    "        # ---- 3.1 node scores ----\n",
    "        print(\"file: \", data.file_name)\n",
    "        with torch.no_grad():\n",
    "            scores = model(data.x, data.edge_index, data.edge_prob)   # [N]\n",
    "        \n",
    "        # print(data.edge_prob)\n",
    "        # ---- 3.2 pick top-K nodes ----\n",
    "        # scores is already on the same device; .cpu() only if epc_mc_deleted needs CPU tensors\n",
    "        topk = scores.topk(K, largest=True).indices.tolist()         # list[int]\n",
    "\n",
    "        # ---- 3.3 compute EPC after deleting top-K ----\n",
    "        #   We need the *NetworkX graph*; fetch it via the original .pkl\n",
    "        #   The path is stored in test_ds.graph_paths[index] where `index`\n",
    "        #   is the position in the dataset.  The DataLoader gives us that\n",
    "        #   index in data.__dict__['idx']  (PyG attaches it automatically).\n",
    "        idx  = data.idx.item()         # scalar tensor → int\n",
    "        G_nx = pickle.load(open(test_ds.graph_paths[idx], 'rb'))['graph']\n",
    "        \n",
    "        epc_0   = epc_mc_deleted(G_nx.copy(), set(), num_samples=mc_samples)  \n",
    "        epc_del = epc_mc_deleted(G_nx.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "        all_epc.append(epc_del)\n",
    "\n",
    "        delta   = epc_del - epc_0     # negative  ⇒ improvement\n",
    "        print(f\"{fname}  EPC₀={epc_0:.1f}  after={epc_del:.1f}  Δ={delta:+.1f}\")\n",
    "\n",
    "        print(f\"Graph {idx:03d} | EPC(after delete) = {epc_del:.4f} | top-K = {topk}\")\n",
    "        \n",
    "        print(\"top-scores id,logit:\")\n",
    "        print(sorted(zip(topk, scores[topk].tolist()), key=lambda x: -x[1])[:5])\n",
    "\n",
    "        # confirm they exist in the NetworkX graph\n",
    "        assert all(v in G_nx for v in topk)\n",
    "        i += 1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  SUMMARY\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "print(f\"\\nAverage EPC over {len(all_epc)} test graphs: {np.mean(all_epc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73e91235",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/best_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m in_dim       \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m                     \u001b[38;5;66;03m# you decided to keep only the 11 node features\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model        \u001b[38;5;241m=\u001b[39m SAGEEdgeProbModel(in_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 3.  INFERENCE + EPC\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/Research/SCNDP/lib/python3.10/site-packages/torch/serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Development/Research/SCNDP/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Development/Research/SCNDP/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/best_model.pt'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "graphs_dir   = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs/test_large_separate\"          # same dirs you used for train/val\n",
    "labels_dir   = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels/test_large_separate\"          # not needed for inference but Dataset expects it\n",
    "ckpt_path    = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/best_model.pt\" # saved in train_model()\n",
    "K            = 10                     # number of nodes to delete\n",
    "mc_samples   = 10_000                 # per-graph Monte-Carlo samples for EPC\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  DATASET & DATALOADER  (batch_size = 1 for clarity)\n",
    "# ------------------------------------------------------------\n",
    "test_ds      = GraphEPCDataset(graphs_dir, labels_dir, split=\"ER\")\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  LOAD MODEL\n",
    "# ------------------------------------------------------------\n",
    "in_dim       = 11                     # you decided to keep only the 11 node features\n",
    "model        = SAGEEdgeProbModel(in_dim).to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  INFERENCE + EPC\n",
    "# ------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_epc = []\n",
    "i = 0\n",
    "\n",
    "for data in tqdm(test_loader, desc=\"Inference\"):\n",
    "\n",
    "    # if i == 30:\n",
    "    #     break\n",
    "    # move everything to GPU/CPU\n",
    "    data = data.to(device)\n",
    "\n",
    "    fname = data.file_name[0]\n",
    "    \n",
    "    if \"sp0.0443\" in fname and (\"rp0.9\" in fname or \"rp1.0\" in fname):\n",
    "        # ---- 3.1 node scores ----\n",
    "        print(\"file: \", data.file_name)\n",
    "        with torch.no_grad():\n",
    "            scores = model(data.x, data.edge_index, data.edge_prob)   # [N]\n",
    "        \n",
    "        # print(data.edge_prob)\n",
    "        # ---- 3.2 pick top-K nodes ----\n",
    "        # scores is already on the same device; .cpu() only if epc_mc_deleted needs CPU tensors\n",
    "        topk = scores.topk(K, largest=True).indices.tolist()         # list[int]\n",
    "\n",
    "        # ---- 3.3 compute EPC after deleting top-K ----\n",
    "        #   We need the *NetworkX graph*; fetch it via the original .pkl\n",
    "        #   The path is stored in test_ds.graph_paths[index] where `index`\n",
    "        #   is the position in the dataset.  The DataLoader gives us that\n",
    "        #   index in data.__dict__['idx']  (PyG attaches it automatically).\n",
    "        idx  = data.idx.item()         # scalar tensor → int\n",
    "        G_nx = pickle.load(open(test_ds.graph_paths[idx], 'rb'))['graph']\n",
    "        \n",
    "        epc_0   = epc_mc_deleted(G_nx.copy(), set(), num_samples=mc_samples)  \n",
    "        epc_del = epc_mc_deleted(G_nx.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "        all_epc.append(epc_del)\n",
    "\n",
    "        delta   = epc_del - epc_0     # negative  ⇒ improvement\n",
    "        print(f\"{fname}  EPC₀={epc_0:.1f}  after={epc_del:.1f}  Δ={delta:+.1f}\")\n",
    "\n",
    "        print(f\"Graph {idx:03d} | EPC(after delete) = {epc_del:.4f} | top-K = {topk}\")\n",
    "        \n",
    "        print(\"top-scores id,logit:\")\n",
    "        print(sorted(zip(topk, scores[topk].tolist()), key=lambda x: -x[1])[:5])\n",
    "\n",
    "        # confirm they exist in the NetworkX graph\n",
    "        assert all(v in G_nx for v in topk)\n",
    "        i += 1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  SUMMARY\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "print(f\"\\nAverage EPC over {len(all_epc)} test graphs: {np.mean(all_epc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ae8c295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:   7%|▋         | 4/60 [00:00<00:01, 34.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.1_test100_0.pkl EPC0=39.5800  EPC-del=24.3855  Δ=-15.1945  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_1.pkl EPC0=33.5550  EPC-del=20.0340  Δ=-13.5210  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_2.pkl EPC0=36.4950  EPC-del=21.7890  Δ=-14.7060  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_0.pkl EPC0=206.7300  EPC-del=74.4525  Δ=-132.2775  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_1.pkl EPC0=225.4850  EPC-del=63.9765  Δ=-161.5085  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_2.pkl EPC0=223.3700  EPC-del=71.1315  Δ=-152.2385  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_0.pkl EPC0=1069.6250  EPC-del=201.0375  Δ=-868.5875  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  20%|██        | 12/60 [00:00<00:01, 31.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.3_test100_1.pkl EPC0=1015.0900  EPC-del=188.9190  Δ=-826.1710  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_2.pkl EPC0=868.6600  EPC-del=158.2245  Δ=-710.4355  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_0.pkl EPC0=2556.6050  EPC-del=895.8645  Δ=-1660.7405  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_1.pkl EPC0=2758.2750  EPC-del=723.4830  Δ=-2034.7920  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_2.pkl EPC0=3103.0700  EPC-del=1142.8155  Δ=-1960.2545  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_0.pkl EPC0=3225.4200  EPC-del=1195.1460  Δ=-2030.2740  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_1.pkl EPC0=2999.6550  EPC-del=1153.9350  Δ=-1845.7200  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  33%|███▎      | 20/60 [00:00<00:01, 30.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.5_test100_2.pkl EPC0=3702.0400  EPC-del=2174.4630  Δ=-1527.5770  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_0.pkl EPC0=4439.0500  EPC-del=3007.7190  Δ=-1431.3310  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_1.pkl EPC0=3891.4600  EPC-del=2129.1075  Δ=-1762.3525  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_2.pkl EPC0=3904.3700  EPC-del=2063.5515  Δ=-1840.8185  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_0.pkl EPC0=4354.1150  EPC-del=2876.7780  Δ=-1477.3370  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_1.pkl EPC0=4180.1350  EPC-del=2393.1495  Δ=-1786.9855  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  40%|████      | 24/60 [00:00<00:01, 30.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.7_test100_2.pkl EPC0=4177.4600  EPC-del=2403.7515  Δ=-1773.7085  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_0.pkl EPC0=4775.0400  EPC-del=3512.2545  Δ=-1262.7855  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_1.pkl EPC0=4638.9150  EPC-del=3093.4260  Δ=-1545.4890  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_2.pkl EPC0=4552.2600  EPC-del=2915.7570  Δ=-1636.5030  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_0.pkl EPC0=4895.4800  EPC-del=3300.3405  Δ=-1595.1395  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_1.pkl EPC0=4818.5200  EPC-del=3452.0445  Δ=-1366.4755  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  52%|█████▏    | 31/60 [00:01<00:01, 27.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_2.pkl EPC0=4801.9200  EPC-del=3301.5600  Δ=-1500.3600  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_0.pkl EPC0=4753.0000  EPC-del=3732.6150  Δ=-1020.3850  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3734.1630  Δ=-1215.8370  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_2.pkl EPC0=4762.6400  EPC-del=3469.4370  Δ=-1293.2030  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_0.pkl EPC0=88.1100  EPC-del=48.2355  Δ=-39.8745  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_1.pkl EPC0=90.0400  EPC-del=46.6920  Δ=-43.3480  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  65%|██████▌   | 39/60 [00:01<00:00, 30.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.1_test100_2.pkl EPC0=83.4400  EPC-del=44.3160  Δ=-39.1240  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_0.pkl EPC0=818.1300  EPC-del=235.2825  Δ=-582.8475  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_1.pkl EPC0=891.2250  EPC-del=266.9040  Δ=-624.3210  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_2.pkl EPC0=930.5250  EPC-del=252.3150  Δ=-678.2100  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_0.pkl EPC0=2964.3100  EPC-del=1183.6575  Δ=-1780.6525  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_1.pkl EPC0=2410.0000  EPC-del=663.5295  Δ=-1746.4705  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_2.pkl EPC0=3247.7200  EPC-del=1636.1055  Δ=-1611.6145  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  72%|███████▏  | 43/60 [00:01<00:00, 27.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.4_test100_0.pkl EPC0=4154.5450  EPC-del=2779.1865  Δ=-1375.3585  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_1.pkl EPC0=3980.9750  EPC-del=2569.8780  Δ=-1411.0970  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_2.pkl EPC0=4190.5700  EPC-del=2892.8925  Δ=-1297.6775  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_0.pkl EPC0=4563.9500  EPC-del=3225.7170  Δ=-1338.2330  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_1.pkl EPC0=4516.0250  EPC-del=3048.5700  Δ=-1467.4550  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  82%|████████▏ | 49/60 [00:01<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.5_test100_2.pkl EPC0=4620.4350  EPC-del=3324.2175  Δ=-1296.2175  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_0.pkl EPC0=4694.0150  EPC-del=3240.7605  Δ=-1453.2545  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_1.pkl EPC0=4712.3650  EPC-del=3333.5055  Δ=-1378.8595  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_2.pkl EPC0=4796.9400  EPC-del=3588.8085  Δ=-1208.1315  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_0.pkl EPC0=4847.3450  EPC-del=3584.6640  Δ=-1262.6810  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  87%|████████▋ | 52/60 [00:01<00:00, 24.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.7_test100_1.pkl EPC0=4886.2050  EPC-del=3756.9150  Δ=-1129.2900  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_2.pkl EPC0=4821.6300  EPC-del=3510.3735  Δ=-1311.2565  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_0.pkl EPC0=4897.6350  EPC-del=3646.3185  Δ=-1251.3165  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_1.pkl EPC0=4939.0500  EPC-del=3774.8520  Δ=-1164.1980  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_2.pkl EPC0=4917.1750  EPC-del=3633.2460  Δ=-1283.9290  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  97%|█████████▋| 58/60 [00:02<00:00, 23.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.9_test100_0.pkl EPC0=4927.1100  EPC-del=3651.8580  Δ=-1275.2520  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_1.pkl EPC0=4947.4750  EPC-del=3736.6740  Δ=-1210.8010  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_2.pkl EPC0=4927.9550  EPC-del=3400.4925  Δ=-1527.4625  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=4005.0000  Δ=-945.0000  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3912.4800  Δ=-1037.5200  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity: 100%|██████████| 60/60 [00:02<00:00, 27.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp1.0_test100_2.pkl EPC0=4851.9800  EPC-del=3733.3890  Δ=-1118.5910  K=10\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 0. CONFIGURATION\n",
    "# ----------------------------------------------------------------------\n",
    "graphs_dir  = \"data/graphs/test_100/ER\"\n",
    "labels_dir  = \"data/labels/test_100/ER\"          # same sub-folder structure\n",
    "K           = 10                            # how many nodes to delete\n",
    "mc_samples  = 10_000                        # EPC Monte-Carlo samples\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. HELPER: load graph + label file\n",
    "# ----------------------------------------------------------------------\n",
    "def load_graph_and_scores(pkl_path):\n",
    "    G   = pickle.load(open(pkl_path, \"rb\"))[\"graph\"]\n",
    "\n",
    "    # label file has the same stem plus '_labels.pt'\n",
    "    lbl_path = os.path.join(\n",
    "        labels_dir,\n",
    "        os.path.basename(pkl_path).replace(\".pkl\", \"_labels.pt\")\n",
    "    )\n",
    "    if not os.path.exists(lbl_path):\n",
    "        raise FileNotFoundError(f\"label file not found for {pkl_path}\")\n",
    "\n",
    "    # tensor shape [N], dtype=float\n",
    "    log1_scores = torch.load(lbl_path)\n",
    "    # undo stabilisation: score = exp(label) - 1\n",
    "    scores = log1_scores.exp() - 1.0\n",
    "\n",
    "    return G, scores\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. MAIN LOOP\n",
    "# ----------------------------------------------------------------------\n",
    "for pkl in tqdm(sorted(glob.glob(os.path.join(graphs_dir, \"*.pkl\"))),\n",
    "                desc=\"sanity\"):\n",
    "\n",
    "    G, scores = load_graph_and_scores(pkl)\n",
    "    N         = G.number_of_nodes()\n",
    "\n",
    "    # baseline EPC (no deletions)\n",
    "    epc_0 = epc_mc_deleted(G.copy(), set(), num_samples=mc_samples)\n",
    "\n",
    "    # top-K indices by descending score\n",
    "    topk   = scores.topk(K).indices.tolist()       # list[int]\n",
    "    epc_K  = epc_mc_deleted(G.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "    print(f\"{os.path.basename(pkl):<25}\"\n",
    "          f\" EPC0={epc_0:7.4f}  \"\n",
    "          f\"EPC-del={epc_K:7.4f}  \"\n",
    "          f\"Δ={epc_K-epc_0:+.4f}  \"\n",
    "          f\"K={K}\")\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2eef6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:   3%|▎         | 5/180 [00:01<00:37,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.1_test100_0.pkl EPC0=42.3950  EPC-del= 9.8190  Δ=-32.5760  K=10\n",
      "BA_sz100_sp2_rp0.1_test100_1.pkl EPC0=46.6100  EPC-del= 9.4995  Δ=-37.1105  K=10\n",
      "BA_sz100_sp2_rp0.1_test100_2.pkl EPC0=45.5850  EPC-del= 9.3330  Δ=-36.2520  K=10\n",
      "BA_sz100_sp2_rp0.2_test100_0.pkl EPC0=238.3150  EPC-del=24.9120  Δ=-213.4030  K=10\n",
      "BA_sz100_sp2_rp0.2_test100_1.pkl EPC0=224.1650  EPC-del=25.5510  Δ=-198.6140  K=10\n",
      "BA_sz100_sp2_rp0.2_test100_2.pkl EPC0=254.5900  EPC-del=21.3345  Δ=-233.2555  K=10\n",
      "BA_sz100_sp2_rp0.3_test100_0.pkl EPC0=901.0350  EPC-del=48.6540  Δ=-852.3810  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:   7%|▋         | 13/180 [00:01<00:13, 12.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.3_test100_1.pkl EPC0=882.4300  EPC-del=49.8555  Δ=-832.5745  K=10\n",
      "BA_sz100_sp2_rp0.3_test100_2.pkl EPC0=888.5700  EPC-del=41.4630  Δ=-847.1070  K=10\n",
      "BA_sz100_sp2_rp0.4_test100_0.pkl EPC0=1990.2350  EPC-del=79.0830  Δ=-1911.1520  K=10\n",
      "BA_sz100_sp2_rp0.4_test100_1.pkl EPC0=1999.4700  EPC-del=90.2115  Δ=-1909.2585  K=10\n",
      "BA_sz100_sp2_rp0.4_test100_2.pkl EPC0=1977.6900  EPC-del=95.9355  Δ=-1881.7545  K=10\n",
      "BA_sz100_sp2_rp0.5_test100_0.pkl EPC0=2984.7950  EPC-del=107.8785  Δ=-2876.9165  K=10\n",
      "BA_sz100_sp2_rp0.5_test100_1.pkl EPC0=3031.0350  EPC-del=119.0700  Δ=-2911.9650  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  11%|█         | 19/180 [00:01<00:08, 18.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.5_test100_2.pkl EPC0=3027.6050  EPC-del=115.8750  Δ=-2911.7300  K=10\n",
      "BA_sz100_sp2_rp0.6_test100_0.pkl EPC0=3825.3850  EPC-del=236.7900  Δ=-3588.5950  K=10\n",
      "BA_sz100_sp2_rp0.6_test100_1.pkl EPC0=3831.2000  EPC-del=248.8365  Δ=-3582.3635  K=10\n",
      "BA_sz100_sp2_rp0.6_test100_2.pkl EPC0=3793.5050  EPC-del=182.7855  Δ=-3610.7195  K=10\n",
      "BA_sz100_sp2_rp0.7_test100_0.pkl EPC0=4388.2750  EPC-del=411.8130  Δ=-3976.4620  K=10\n",
      "BA_sz100_sp2_rp0.7_test100_1.pkl EPC0=4362.2300  EPC-del=703.5615  Δ=-3658.6685  K=10\n",
      "BA_sz100_sp2_rp0.7_test100_2.pkl EPC0=4369.7700  EPC-del=300.5325  Δ=-4069.2375  K=10\n",
      "BA_sz100_sp2_rp0.8_test100_0.pkl EPC0=4718.8250  EPC-del=688.8510  Δ=-4029.9740  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  20%|██        | 36/180 [00:02<00:03, 43.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.8_test100_1.pkl EPC0=4712.4100  EPC-del=1039.2030  Δ=-3673.2070  K=10\n",
      "BA_sz100_sp2_rp0.8_test100_2.pkl EPC0=4712.4550  EPC-del=895.1130  Δ=-3817.3420  K=10\n",
      "BA_sz100_sp2_rp0.9_test100_0.pkl EPC0=4895.1000  EPC-del=1909.3410  Δ=-2985.7590  K=10\n",
      "BA_sz100_sp2_rp0.9_test100_1.pkl EPC0=4905.0800  EPC-del=2196.6345  Δ=-2708.4455  K=10\n",
      "BA_sz100_sp2_rp0.9_test100_2.pkl EPC0=4893.6150  EPC-del=631.6875  Δ=-4261.9275  K=10\n",
      "BA_sz100_sp2_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=1639.1925  Δ=-3310.8075  K=10\n",
      "BA_sz100_sp2_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=1477.0260  Δ=-3472.9740  K=10\n",
      "BA_sz100_sp2_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=1111.8465  Δ=-3838.1535  K=10\n",
      "BA_sz100_sp3_rp0.1_test100_0.pkl EPC0=93.5050  EPC-del=19.9800  Δ=-73.5250  K=10\n",
      "BA_sz100_sp3_rp0.1_test100_1.pkl EPC0=87.6600  EPC-del=22.5000  Δ=-65.1600  K=10\n",
      "BA_sz100_sp3_rp0.1_test100_2.pkl EPC0=93.7550  EPC-del=20.9970  Δ=-72.7580  K=10\n",
      "BA_sz100_sp3_rp0.2_test100_0.pkl EPC0=912.9750  EPC-del=55.1430  Δ=-857.8320  K=10\n",
      "BA_sz100_sp3_rp0.2_test100_1.pkl EPC0=884.2550  EPC-del=61.0740  Δ=-823.1810  K=10\n",
      "BA_sz100_sp3_rp0.2_test100_2.pkl EPC0=875.0850  EPC-del=63.7830  Δ=-811.3020  K=10\n",
      "BA_sz100_sp3_rp0.3_test100_0.pkl EPC0=2416.8200  EPC-del=192.4470  Δ=-2224.3730  K=10\n",
      "BA_sz100_sp3_rp0.3_test100_1.pkl EPC0=2405.8250  EPC-del=228.3255  Δ=-2177.4995  K=10\n",
      "BA_sz100_sp3_rp0.3_test100_2.pkl EPC0=2397.3850  EPC-del=176.9985  Δ=-2220.3865  K=10\n",
      "BA_sz100_sp3_rp0.4_test100_0.pkl EPC0=3473.3950  EPC-del=456.0435  Δ=-3017.3515  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  24%|██▍       | 43/180 [00:02<00:03, 45.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.4_test100_1.pkl EPC0=3436.3600  EPC-del=276.7500  Δ=-3159.6100  K=10\n",
      "BA_sz100_sp3_rp0.4_test100_2.pkl EPC0=3483.1100  EPC-del=437.1435  Δ=-3045.9665  K=10\n",
      "BA_sz100_sp3_rp0.5_test100_0.pkl EPC0=4244.7550  EPC-del=1809.3825  Δ=-2435.3725  K=10\n",
      "BA_sz100_sp3_rp0.5_test100_1.pkl EPC0=4164.4050  EPC-del=1017.9765  Δ=-3146.4285  K=10\n",
      "BA_sz100_sp3_rp0.5_test100_2.pkl EPC0=4182.6600  EPC-del=1495.3140  Δ=-2687.3460  K=10\n",
      "BA_sz100_sp3_rp0.6_test100_0.pkl EPC0=4599.6100  EPC-del=2138.8680  Δ=-2460.7420  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  27%|██▋       | 49/180 [00:02<00:03, 36.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.6_test100_1.pkl EPC0=4601.1000  EPC-del=2069.8335  Δ=-2531.2665  K=10\n",
      "BA_sz100_sp3_rp0.6_test100_2.pkl EPC0=4605.6550  EPC-del=1839.3345  Δ=-2766.3205  K=10\n",
      "BA_sz100_sp3_rp0.7_test100_0.pkl EPC0=4801.4650  EPC-del=2065.4055  Δ=-2736.0595  K=10\n",
      "BA_sz100_sp3_rp0.7_test100_1.pkl EPC0=4831.4850  EPC-del=2518.7265  Δ=-2312.7585  K=10\n",
      "BA_sz100_sp3_rp0.7_test100_2.pkl EPC0=4815.5900  EPC-del=2601.3690  Δ=-2214.2210  K=10\n",
      "BA_sz100_sp3_rp0.8_test100_0.pkl EPC0=4918.0650  EPC-del=3186.8775  Δ=-1731.1875  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  32%|███▏      | 58/180 [00:02<00:03, 31.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.8_test100_1.pkl EPC0=4917.5550  EPC-del=3285.3195  Δ=-1632.2355  K=10\n",
      "BA_sz100_sp3_rp0.8_test100_2.pkl EPC0=4909.4900  EPC-del=2905.2495  Δ=-2004.2405  K=10\n",
      "BA_sz100_sp3_rp0.9_test100_0.pkl EPC0=4946.2300  EPC-del=3529.1835  Δ=-1417.0465  K=10\n",
      "BA_sz100_sp3_rp0.9_test100_1.pkl EPC0=4944.4750  EPC-del=3412.9125  Δ=-1531.5625  K=10\n",
      "BA_sz100_sp3_rp0.9_test100_2.pkl EPC0=4944.4350  EPC-del=2972.6505  Δ=-1971.7845  K=10\n",
      "BA_sz100_sp3_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3472.0560  Δ=-1477.9440  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  46%|████▌     | 82/180 [00:03<00:01, 62.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3672.0000  Δ=-1278.0000  K=10\n",
      "BA_sz100_sp3_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=3398.7285  Δ=-1551.2715  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_0.pkl EPC0=38.8200  EPC-del=21.5505  Δ=-17.2695  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_1.pkl EPC0=33.1500  EPC-del=18.8145  Δ=-14.3355  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_2.pkl EPC0=38.7950  EPC-del=22.3830  Δ=-16.4120  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_0.pkl EPC0=210.3650  EPC-del=72.7425  Δ=-137.6225  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_1.pkl EPC0=226.6450  EPC-del=62.1495  Δ=-164.4955  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_2.pkl EPC0=219.5850  EPC-del=61.5645  Δ=-158.0205  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_0.pkl EPC0=1085.2800  EPC-del=183.4020  Δ=-901.8780  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_1.pkl EPC0=990.1500  EPC-del=174.0105  Δ=-816.1395  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_2.pkl EPC0=891.7450  EPC-del=123.5925  Δ=-768.1525  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_0.pkl EPC0=2530.6650  EPC-del=696.6945  Δ=-1833.9705  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_1.pkl EPC0=2736.5000  EPC-del=615.1185  Δ=-2121.3815  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_2.pkl EPC0=3102.3100  EPC-del=964.3635  Δ=-2137.9465  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_0.pkl EPC0=3215.1550  EPC-del=927.3960  Δ=-2287.7590  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_1.pkl EPC0=3021.6800  EPC-del=929.2140  Δ=-2092.4660  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_2.pkl EPC0=3713.3200  EPC-del=1724.7105  Δ=-1988.6095  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_0.pkl EPC0=4454.7850  EPC-del=2788.4340  Δ=-1666.3510  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_1.pkl EPC0=3923.8200  EPC-del=1972.7910  Δ=-1951.0290  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_2.pkl EPC0=3927.3550  EPC-del=1985.4990  Δ=-1941.8560  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_0.pkl EPC0=4356.6100  EPC-del=2721.9420  Δ=-1634.6680  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_1.pkl EPC0=4209.2200  EPC-del=2153.2725  Δ=-2055.9475  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_2.pkl EPC0=4158.2300  EPC-del=1980.4050  Δ=-2177.8250  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_0.pkl EPC0=4762.3400  EPC-del=3057.6870  Δ=-1704.6530  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_1.pkl EPC0=4637.2300  EPC-del=2534.8005  Δ=-2102.4295  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_2.pkl EPC0=4549.6950  EPC-del=2666.9880  Δ=-1882.7070  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  57%|█████▋    | 102/180 [00:03<00:00, 78.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_0.pkl EPC0=4888.6500  EPC-del=3233.8440  Δ=-1654.8060  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_1.pkl EPC0=4816.9450  EPC-del=3147.4035  Δ=-1669.5415  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_2.pkl EPC0=4801.4900  EPC-del=3121.3350  Δ=-1680.1550  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_0.pkl EPC0=4753.4850  EPC-del=3347.9595  Δ=-1405.5255  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3238.4430  Δ=-1711.5570  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_2.pkl EPC0=4753.5200  EPC-del=3095.2170  Δ=-1658.3030  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_0.pkl EPC0=91.6850  EPC-del=41.9940  Δ=-49.6910  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_1.pkl EPC0=91.3300  EPC-del=42.9255  Δ=-48.4045  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_2.pkl EPC0=86.2950  EPC-del=39.5865  Δ=-46.7085  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_0.pkl EPC0=823.7150  EPC-del=214.1730  Δ=-609.5420  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_1.pkl EPC0=901.7350  EPC-del=219.5370  Δ=-682.1980  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_2.pkl EPC0=934.0200  EPC-del=221.6205  Δ=-712.3995  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_0.pkl EPC0=3001.8800  EPC-del=1098.0315  Δ=-1903.8485  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_1.pkl EPC0=2374.6700  EPC-del=647.7210  Δ=-1726.9490  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_2.pkl EPC0=3271.7300  EPC-del=1405.6335  Δ=-1866.0965  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_0.pkl EPC0=4147.0100  EPC-del=2590.8255  Δ=-1556.1845  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_1.pkl EPC0=4001.5700  EPC-del=2315.9205  Δ=-1685.6495  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_2.pkl EPC0=4188.5200  EPC-del=2799.9270  Δ=-1388.5930  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_0.pkl EPC0=4555.1800  EPC-del=3103.2585  Δ=-1451.9215  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  62%|██████▏   | 111/180 [00:03<00:00, 70.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.5_test100_1.pkl EPC0=4495.5550  EPC-del=3040.2045  Δ=-1455.3505  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_2.pkl EPC0=4599.9450  EPC-del=3224.0430  Δ=-1375.9020  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_0.pkl EPC0=4704.2700  EPC-del=3072.9240  Δ=-1631.3460  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_1.pkl EPC0=4697.0800  EPC-del=3279.8070  Δ=-1417.2730  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_2.pkl EPC0=4802.6650  EPC-del=3462.2910  Δ=-1340.3740  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_0.pkl EPC0=4857.8950  EPC-del=3495.1680  Δ=-1362.7270  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_1.pkl EPC0=4881.9050  EPC-del=3565.9710  Δ=-1315.9340  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_2.pkl EPC0=4823.9100  EPC-del=3470.0535  Δ=-1353.8565  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_0.pkl EPC0=4897.3250  EPC-del=3467.9250  Δ=-1429.4000  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_1.pkl EPC0=4939.4750  EPC-del=3702.4920  Δ=-1236.9830  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_2.pkl EPC0=4911.9950  EPC-del=3560.8410  Δ=-1351.1540  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_0.pkl EPC0=4923.0150  EPC-del=3336.2775  Δ=-1586.7375  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_1.pkl EPC0=4947.0950  EPC-del=3565.4715  Δ=-1381.6235  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  76%|███████▌  | 137/180 [00:03<00:00, 96.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.9_test100_2.pkl EPC0=4928.7200  EPC-del=3259.8000  Δ=-1668.9200  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3921.1920  Δ=-1028.8080  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3917.2320  Δ=-1032.7680  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_2.pkl EPC0=4846.1000  EPC-del=3483.2610  Δ=-1362.8390  K=10\n",
      "SW_sz100_sp4_rp0.1_test100_0.pkl EPC0=31.0550  EPC-del=19.6875  Δ=-11.3675  K=10\n",
      "SW_sz100_sp4_rp0.1_test100_1.pkl EPC0=30.5100  EPC-del=20.6730  Δ=-9.8370  K=10\n",
      "SW_sz100_sp4_rp0.1_test100_2.pkl EPC0=30.1450  EPC-del=19.0170  Δ=-11.1280  K=10\n",
      "SW_sz100_sp4_rp0.2_test100_0.pkl EPC0=117.1400  EPC-del=54.9540  Δ=-62.1860  K=10\n",
      "SW_sz100_sp4_rp0.2_test100_1.pkl EPC0=122.6250  EPC-del=49.4370  Δ=-73.1880  K=10\n",
      "SW_sz100_sp4_rp0.2_test100_2.pkl EPC0=112.1600  EPC-del=49.0905  Δ=-63.0695  K=10\n",
      "SW_sz100_sp4_rp0.3_test100_0.pkl EPC0=448.2400  EPC-del=135.6525  Δ=-312.5875  K=10\n",
      "SW_sz100_sp4_rp0.3_test100_1.pkl EPC0=495.7750  EPC-del=138.8700  Δ=-356.9050  K=10\n",
      "SW_sz100_sp4_rp0.3_test100_2.pkl EPC0=475.6400  EPC-del=131.8635  Δ=-343.7765  K=10\n",
      "SW_sz100_sp4_rp0.4_test100_0.pkl EPC0=1913.7100  EPC-del=363.6720  Δ=-1550.0380  K=10\n",
      "SW_sz100_sp4_rp0.4_test100_1.pkl EPC0=1838.4900  EPC-del=335.3265  Δ=-1503.1635  K=10\n",
      "SW_sz100_sp4_rp0.4_test100_2.pkl EPC0=1889.8200  EPC-del=361.3815  Δ=-1528.4385  K=10\n",
      "SW_sz100_sp4_rp0.5_test100_0.pkl EPC0=3413.5350  EPC-del=964.6605  Δ=-2448.8745  K=10\n",
      "SW_sz100_sp4_rp0.5_test100_1.pkl EPC0=3398.6950  EPC-del=1146.3660  Δ=-2252.3290  K=10\n",
      "SW_sz100_sp4_rp0.5_test100_2.pkl EPC0=3421.3700  EPC-del=1103.1705  Δ=-2318.1995  K=10\n",
      "SW_sz100_sp4_rp0.6_test100_0.pkl EPC0=4288.5750  EPC-del=2049.3225  Δ=-2239.2525  K=10\n",
      "SW_sz100_sp4_rp0.6_test100_1.pkl EPC0=4261.8200  EPC-del=2128.9680  Δ=-2132.8520  K=10\n",
      "SW_sz100_sp4_rp0.6_test100_2.pkl EPC0=4243.4350  EPC-del=2126.4075  Δ=-2117.0275  K=10\n",
      "SW_sz100_sp4_rp0.7_test100_0.pkl EPC0=4722.1700  EPC-del=2967.8985  Δ=-1754.2715  K=10\n",
      "SW_sz100_sp4_rp0.7_test100_1.pkl EPC0=4664.1700  EPC-del=2818.8090  Δ=-1845.3610  K=10\n",
      "SW_sz100_sp4_rp0.7_test100_2.pkl EPC0=4714.6500  EPC-del=2860.5690  Δ=-1854.0810  K=10\n",
      "SW_sz100_sp4_rp0.8_test100_0.pkl EPC0=4868.7100  EPC-del=3058.2990  Δ=-1810.4110  K=10\n",
      "SW_sz100_sp4_rp0.8_test100_1.pkl EPC0=4855.6300  EPC-del=3102.2280  Δ=-1753.4020  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  89%|████████▉ | 160/180 [00:03<00:00, 101.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp4_rp0.8_test100_2.pkl EPC0=4852.0850  EPC-del=3000.2625  Δ=-1851.8225  K=10\n",
      "SW_sz100_sp4_rp0.9_test100_0.pkl EPC0=4933.0500  EPC-del=3373.9110  Δ=-1559.1390  K=10\n",
      "SW_sz100_sp4_rp0.9_test100_1.pkl EPC0=4928.8150  EPC-del=3047.5800  Δ=-1881.2350  K=10\n",
      "SW_sz100_sp4_rp0.9_test100_2.pkl EPC0=4932.9750  EPC-del=3457.1430  Δ=-1475.8320  K=10\n",
      "SW_sz100_sp4_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3503.0340  Δ=-1446.9660  K=10\n",
      "SW_sz100_sp4_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3263.3865  Δ=-1686.6135  K=10\n",
      "SW_sz100_sp4_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=3485.7945  Δ=-1464.2055  K=10\n",
      "SW_sz100_sp5_rp0.1_test100_0.pkl EPC0=31.7050  EPC-del=22.9725  Δ=-8.7325  K=10\n",
      "SW_sz100_sp5_rp0.1_test100_1.pkl EPC0=31.2050  EPC-del=19.0260  Δ=-12.1790  K=10\n",
      "SW_sz100_sp5_rp0.1_test100_2.pkl EPC0=31.1350  EPC-del=18.5760  Δ=-12.5590  K=10\n",
      "SW_sz100_sp5_rp0.2_test100_0.pkl EPC0=117.8400  EPC-del=53.2035  Δ=-64.6365  K=10\n",
      "SW_sz100_sp5_rp0.2_test100_1.pkl EPC0=117.6750  EPC-del=55.9710  Δ=-61.7040  K=10\n",
      "SW_sz100_sp5_rp0.2_test100_2.pkl EPC0=116.6650  EPC-del=52.6095  Δ=-64.0555  K=10\n",
      "SW_sz100_sp5_rp0.3_test100_0.pkl EPC0=476.4100  EPC-del=143.2260  Δ=-333.1840  K=10\n",
      "SW_sz100_sp5_rp0.3_test100_1.pkl EPC0=496.8700  EPC-del=131.2695  Δ=-365.6005  K=10\n",
      "SW_sz100_sp5_rp0.3_test100_2.pkl EPC0=490.0450  EPC-del=129.9240  Δ=-360.1210  K=10\n",
      "SW_sz100_sp5_rp0.4_test100_0.pkl EPC0=1873.1850  EPC-del=320.6205  Δ=-1552.5645  K=10\n",
      "SW_sz100_sp5_rp0.4_test100_1.pkl EPC0=1815.8250  EPC-del=424.3005  Δ=-1391.5245  K=10\n",
      "SW_sz100_sp5_rp0.4_test100_2.pkl EPC0=1812.8500  EPC-del=361.5165  Δ=-1451.3335  K=10\n",
      "SW_sz100_sp5_rp0.5_test100_0.pkl EPC0=3402.0900  EPC-del=885.5190  Δ=-2516.5710  K=10\n",
      "SW_sz100_sp5_rp0.5_test100_1.pkl EPC0=3377.5250  EPC-del=856.8585  Δ=-2520.6665  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity: 100%|██████████| 180/180 [00:04<00:00, 44.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp5_rp0.5_test100_2.pkl EPC0=3489.9650  EPC-del=1133.3070  Δ=-2356.6580  K=10\n",
      "SW_sz100_sp5_rp0.6_test100_0.pkl EPC0=4269.7450  EPC-del=2173.5540  Δ=-2096.1910  K=10\n",
      "SW_sz100_sp5_rp0.6_test100_1.pkl EPC0=4176.3950  EPC-del=1998.9630  Δ=-2177.4320  K=10\n",
      "SW_sz100_sp5_rp0.6_test100_2.pkl EPC0=4245.5700  EPC-del=2099.9610  Δ=-2145.6090  K=10\n",
      "SW_sz100_sp5_rp0.7_test100_0.pkl EPC0=4663.4000  EPC-del=2514.7215  Δ=-2148.6785  K=10\n",
      "SW_sz100_sp5_rp0.7_test100_1.pkl EPC0=4679.1550  EPC-del=2844.5805  Δ=-1834.5745  K=10\n",
      "SW_sz100_sp5_rp0.7_test100_2.pkl EPC0=4637.8950  EPC-del=2650.3740  Δ=-1987.5210  K=10\n",
      "SW_sz100_sp5_rp0.8_test100_0.pkl EPC0=4851.0300  EPC-del=3178.2105  Δ=-1672.8195  K=10\n",
      "SW_sz100_sp5_rp0.8_test100_1.pkl EPC0=4854.3400  EPC-del=3019.7655  Δ=-1834.5745  K=10\n",
      "SW_sz100_sp5_rp0.8_test100_2.pkl EPC0=4845.9150  EPC-del=3057.5160  Δ=-1788.3990  K=10\n",
      "SW_sz100_sp5_rp0.9_test100_0.pkl EPC0=4933.8800  EPC-del=3436.1370  Δ=-1497.7430  K=10\n",
      "SW_sz100_sp5_rp0.9_test100_1.pkl EPC0=4938.4450  EPC-del=3431.1690  Δ=-1507.2760  K=10\n",
      "SW_sz100_sp5_rp0.9_test100_2.pkl EPC0=4933.4800  EPC-del=3269.2005  Δ=-1664.2795  K=10\n",
      "SW_sz100_sp5_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3130.2225  Δ=-1819.7775  K=10\n",
      "SW_sz100_sp5_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3489.5115  Δ=-1460.4885  K=10\n",
      "SW_sz100_sp5_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=3492.9720  Δ=-1457.0280  K=10\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 0. CONFIGURATION\n",
    "# ----------------------------------------------------------------------\n",
    "graphs_dir  = \"data/graphs/test_100\"\n",
    "labels_dir  = \"data/rega_labels/test_100\"          # same sub-folder structure\n",
    "K           = 10                            # how many nodes to delete\n",
    "mc_samples  = 10_000                        # EPC Monte-Carlo samples\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. HELPER: load graph + label file\n",
    "# ----------------------------------------------------------------------\n",
    "def load_graph_and_scores(pkl_path):\n",
    "    G   = pickle.load(open(pkl_path, \"rb\"))[\"graph\"]\n",
    "\n",
    "    # label file has the same stem plus '_labels.pt'\n",
    "    lbl_path = os.path.join(\n",
    "        labels_dir,\n",
    "        os.path.basename(pkl_path).replace(\".pkl\", \"_labels.pt\")\n",
    "    )\n",
    "    if not os.path.exists(lbl_path):\n",
    "        raise FileNotFoundError(f\"label file not found for {pkl_path}\")\n",
    "\n",
    "    # tensor shape [N], dtype=float\n",
    "    scores = torch.load(lbl_path)\n",
    "    # undo stabilisation: score = exp(label) - 1\n",
    "    # scores = log1_scores.exp() - 1.0\n",
    "\n",
    "    return G, scores\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. MAIN LOOP\n",
    "# ----------------------------------------------------------------------\n",
    "for pkl in tqdm(sorted(glob.glob(os.path.join(graphs_dir, \"*.pkl\"))),\n",
    "                desc=\"sanity\"):\n",
    "\n",
    "    G, scores = load_graph_and_scores(pkl)\n",
    "    N         = G.number_of_nodes()\n",
    "\n",
    "    # baseline EPC (no deletions)\n",
    "    epc_0 = epc_mc_deleted(G.copy(), set(), num_samples=mc_samples)\n",
    "\n",
    "    # top-K indices by descending score\n",
    "    # print(scores)\n",
    "    topk   = scores.topk(K).indices.tolist()       # list[int]\n",
    "    epc_K  = epc_mc_deleted(G.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "    print(f\"{os.path.basename(pkl):<25}\"\n",
    "          f\" EPC0={epc_0:7.4f}  \"\n",
    "          f\"EPC-del={epc_K:7.4f}  \"\n",
    "          f\"Δ={epc_K-epc_0:+.4f}  \"\n",
    "          f\"K={K}\")\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd992e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCNDP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
