{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee59a00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_geometric\\typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: Could not find module 'C:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\libpyg.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "c:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not find module 'C:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_scatter\\_scatter_cuda.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "c:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_geometric\\typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not find module 'C:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_cluster\\_grid_cuda.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "c:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not find module 'C:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_spline_conv\\_basis_cuda.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warnings.warn(\n",
      "c:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not find module 'C:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch_sparse\\_convert_cuda.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import glob\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "# === Standard Library ===\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import heapq\n",
    "import itertools\n",
    "from collections import defaultdict, deque\n",
    "from itertools import combinations\n",
    "from typing import Any, Tuple, Dict, List, Set, Sequence, Union\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "\n",
    "# --- Scientific Computing ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse    import coo_matrix\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# --- Plotting ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parallel Processing ---\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Graph Processing ---\n",
    "import networkx as nx\n",
    "\n",
    "# --- JIT Compilation ---\n",
    "from numba import njit, prange\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "\n",
    "# --- Model definition ---\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e612d27",
   "metadata": {},
   "source": [
    "# 0. EPC mc deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d829646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_to_csr(G: nx.Graph) -> Tuple[List[int], Dict[int, int], np.ndarray, np.ndarray, np.ndarray]:\n",
    "     \"\"\"Convert an undirected NetworkX graph (edge attr `'p'`) to CSR arrays.\"\"\"\n",
    "     nodes: List[int] = list(G.nodes())\n",
    "     idx_of: Dict[int, int] = {u: i for i, u in enumerate(nodes)}\n",
    "\n",
    "     indptr: List[int] = [0]\n",
    "     indices: List[int] = []\n",
    "     probs: List[float] = []\n",
    "\n",
    "     for u in nodes:\n",
    "         for v in G.neighbors(u):\n",
    "             indices.append(idx_of[v])\n",
    "             probs.append(G.edges[u, v]['p'])\n",
    "         indptr.append(len(indices))\n",
    "\n",
    "     return (\n",
    "         nodes,\n",
    "         idx_of,\n",
    "         np.asarray(indptr, dtype=np.int32),\n",
    "         np.asarray(indices, dtype=np.int32),\n",
    "         np.asarray(probs, dtype=np.float32),\n",
    "     )\n",
    "\n",
    "@njit(inline=\"always\")\n",
    "def _bfs_component_size(start: int,\n",
    "                    indptr: np.ndarray,\n",
    "                    indices: np.ndarray,\n",
    "                    probs: np.ndarray,\n",
    "                    deleted: np.ndarray) -> int:\n",
    "    \"\"\"Return |C_u|−1 for **one** random realisation (stack BFS).\"\"\"\n",
    "    n = deleted.size\n",
    "    stack = np.empty(n, dtype=np.int32)\n",
    "    visited = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "    size = 1\n",
    "    top = 0\n",
    "    stack[top] = start\n",
    "    top += 1\n",
    "    visited[start] = 1\n",
    "\n",
    "    while top:\n",
    "        top -= 1\n",
    "        v = stack[top]\n",
    "        for eid in range(indptr[v], indptr[v + 1]):\n",
    "            w = indices[eid]\n",
    "            if deleted[w]:\n",
    "                continue\n",
    "            if np.random.random() >= probs[eid]:\n",
    "                continue\n",
    "            if visited[w]:\n",
    "                continue\n",
    "            visited[w] = 1\n",
    "            stack[top] = w\n",
    "            top += 1\n",
    "            size += 1\n",
    "    return size - 1\n",
    "\n",
    "@njit(parallel=True)\n",
    "def epc_mc(indptr: np.ndarray,\n",
    "            indices: np.ndarray,\n",
    "            probs: np.ndarray,\n",
    "            deleted: np.ndarray,\n",
    "            num_samples: int) -> float:\n",
    "    \"\"\"Monte‑Carlo estimator of **expected pairwise connectivity** (EPC).\"\"\"\n",
    "    surv = np.where(~deleted)[0]\n",
    "    m = surv.size\n",
    "    if m < 2:\n",
    "        return 0.0\n",
    "\n",
    "    acc = 0.0\n",
    "    for _ in prange(num_samples):\n",
    "        u = surv[np.random.randint(m)]\n",
    "        acc += _bfs_component_size(u, indptr, indices, probs, deleted)\n",
    "\n",
    "    return (m * acc) / (2.0 * num_samples)\n",
    "\n",
    "def epc_mc_deleted(\n",
    "  G: nx.Graph,\n",
    "  S: set,\n",
    "  num_samples: int = 100_000,\n",
    ") -> float:\n",
    "  # build csr once\n",
    "  nodes, idx_of, indptr, indices, probs = nx_to_csr(G)\n",
    "  n = len(nodes)\n",
    "\n",
    "  # turn python set S into a mask (node-IDs to delete)\n",
    "  deleted = np.zeros(n, dtype=np.bool_)\n",
    "  for u in S:\n",
    "    deleted[idx_of[u]] = True\n",
    "\n",
    "  epc = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "\n",
    "  return epc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb87ba0",
   "metadata": {},
   "source": [
    "## REGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68d0192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lp_reaga_sparse(G: nx.Graph, pre_fixed: set, k: int):\n",
    "    V = list(G.nodes())\n",
    "    n = len(V)\n",
    "\n",
    "    # variables: s_i  (i = 0…n-1)      x_ij (j = 0…m2-1)\n",
    "    Pairs = [tuple(sorted(e)) for e in combinations(V, 2)]\n",
    "    m2    = len(Pairs)\n",
    "    Nvar  = n + m2\n",
    "    s_idx = {v: i         for i, v in enumerate(V)}\n",
    "    x_idx = {e: n + j     for j, e in enumerate(Pairs)}\n",
    "\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    rhs              = []\n",
    "\n",
    "    def add_coef(r, c, val):\n",
    "        rows.append(r); cols.append(c); data.append(val)\n",
    "\n",
    "    r = 0 \n",
    "\n",
    "    # budget \n",
    "    for i in range(n):\n",
    "        add_coef(r, i, 1.0)\n",
    "    rhs.append(k); r += 1\n",
    "\n",
    "    # edge upper bounds  x_uv − s_u − s_v ≤ 1 − p_uv\n",
    "    for (u, v) in G.edges():\n",
    "        u, v   = sorted((u, v))\n",
    "        puv    = G.edges[u, v]['p']\n",
    "        add_coef(r, x_idx[(u, v)],  1.0)\n",
    "        add_coef(r, s_idx[u],      -1.0)\n",
    "        add_coef(r, s_idx[v],      -1.0)\n",
    "        rhs.append(1 - puv); r += 1\n",
    "\n",
    "    # triangle cuts for each real edge (i,j) and every\n",
    "    for (i, j) in G.edges():\n",
    "        i, j = sorted((i, j))\n",
    "        for k_ in V:\n",
    "            if k_ == i or k_ == j:\n",
    "                continue\n",
    "            add_coef(r, x_idx[tuple(sorted((i, k_)))],  1.0)  \n",
    "            add_coef(r, x_idx[(i, j)]               , -1.0)  \n",
    "            add_coef(r, x_idx[tuple(sorted((j, k_)))], -1.0)   \n",
    "            rhs.append(0.0); r += 1\n",
    "\n",
    "    n_rows = r\n",
    "    A_ub   = coo_matrix((data, (rows, cols)), shape=(n_rows, Nvar)).tocsr()\n",
    "    b_ub   = np.asarray(rhs)\n",
    "\n",
    "    # bounds \n",
    "    bounds = [(0.0, 1.0)] * Nvar\n",
    "    for v in pre_fixed:\n",
    "        bounds[s_idx[v]] = (1.0, 1.0)\n",
    "\n",
    "    #  objective \n",
    "    c = np.zeros(Nvar)\n",
    "    for e in Pairs:\n",
    "        c[x_idx[e]] = -1.0\n",
    "\n",
    "    # \n",
    "    res = linprog(c, A_ub=A_ub, b_ub=b_ub,\n",
    "                  bounds=bounds, method=\"highs\")\n",
    "    if not res.success:\n",
    "        raise RuntimeError(\"LP infeasible: \" + res.message)\n",
    "\n",
    "    #\n",
    "    s_vals = {v: res.x[s_idx[v]] for v in V}\n",
    "    x_sum  = res.x[n:].sum()\n",
    "    obj    = len(Pairs) - x_sum\n",
    "    return s_vals, obj\n",
    "\n",
    "def local_search_(\n",
    "  G: nx.Graph,\n",
    "  S_init: set,\n",
    "  num_samples: int = 10_000\n",
    "):\n",
    "  \"\"\"1-swap local search\"\"\"\n",
    "\n",
    "  S = S_init.copy()\n",
    "  nodes_not_in_set = set(G.nodes()) - S\n",
    "\n",
    "  current_epc = epc_mc_deleted(G, S, num_samples)\n",
    "\n",
    "  improved = True\n",
    "  while improved:\n",
    "    improved = False\n",
    "    best_swap = None\n",
    "\n",
    "    for u in list(S):\n",
    "      for v in nodes_not_in_set:        \n",
    "        \n",
    "        D_new = (S - {u}) | {v}\n",
    "\n",
    "        temp_epc = epc_mc_deleted(G, D_new, num_samples)\n",
    "\n",
    "        if temp_epc < current_epc:\n",
    "            current_epc = temp_epc\n",
    "            best_swap = (u, v)\n",
    "            improved = True\n",
    "\n",
    "    if improved and best_swap:\n",
    "      u, v = best_swap\n",
    "\n",
    "      S.remove(u)\n",
    "      S.add(v)\n",
    "      nodes_not_in_set.remove(v)\n",
    "      nodes_not_in_set.add(u)\n",
    "  \n",
    "  return S\n",
    "\n",
    "def rega(G: nx.Graph,\n",
    "        k: int,\n",
    "        num_samples: int = 100_000,\n",
    "        max_iter: int = 1,\n",
    "        # epsilon: float = None,\n",
    "        # delta: float = None,\n",
    "        use_tqdm: bool = False):\n",
    "    \"\"\"\n",
    "    Full REGA pipeline: LP‐rounding + CSP‐refined local swaps.\n",
    "    \"\"\"\n",
    "\n",
    "    csr = nx_to_csr(G)\n",
    "\n",
    "    # iterative rounding\n",
    "    D = set()\n",
    "    for _ in range(k):\n",
    "      # s_vals, _ = solve_lp_(G, pre_fixed=D, k=k)\n",
    "      s_vals, _ = solve_lp_reaga_sparse(G, pre_fixed=D, k=k)\n",
    "\n",
    "      # pick the fractional s_i largest among V\\D\n",
    "      u = max((v for v in G.nodes() if v not in D),\n",
    "              key=lambda v: s_vals[v])\n",
    "      D.add(u)\n",
    "\n",
    "    # local‐swap refinement\n",
    "\n",
    "    S_opt = local_search_(G, D, num_samples)\n",
    "    \n",
    "    # S_opt = local_search_(G, greedy_es_S, num_samples)\n",
    "\n",
    "    # S_opt = local_search_swap(\n",
    "    #   D, csr=csr, num_samples=num_samples, max_iter=max_iter)\n",
    "    \n",
    "    # improved = True\n",
    "    \n",
    "    # while improved:\n",
    "\n",
    "    #     improved = False\n",
    "    #     best_epc = current_epc\n",
    "    #     best_swap = None\n",
    "\n",
    "    #     for u in list(D):\n",
    "    #         for v in G.nodes():\n",
    "\n",
    "    #             if v in D: \n",
    "    #                 continue\n",
    "\n",
    "    #             D_new = (D - {u}) | {v}\n",
    "\n",
    "    #             epc_val = epc_func(G, D_new,\n",
    "    #                                num_samples=num_samples,\n",
    "    #                             #    epsilon=epsilon,\n",
    "    #                             #    delta=delta,\n",
    "    #                             #    use_tqdm=use_tqdm\n",
    "    #                                )\n",
    "                \n",
    "    #             if epc_val < best_epc:\n",
    "    #                 best_epc = epc_val\n",
    "    #                 best_swap = (u, v)\n",
    "\n",
    "    #     if best_swap is not None:\n",
    "\n",
    "    #         u, v = best_swap\n",
    "    #         D.remove(u)\n",
    "    #         D.add(v)\n",
    "    #         current_epc = best_epc\n",
    "    #         improved = True\n",
    "\n",
    "    return S_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f7f2b",
   "metadata": {},
   "source": [
    "# 1. Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182f32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "structural_params = {\n",
    "    'ER': {'p': [0.0443, 0.0667]},\n",
    "    'BA': {'m': [2, 3]},\n",
    "    'SW': {'beta': [4, 5]}\n",
    "}\n",
    "train_sizes   = [20, 50, 80]\n",
    "test_sizes    = [100, 200, 300, 500]\n",
    "reliability_p = [i/10 for i in range(1, 11)]  # 0.1, 0.2, ..., 1.0\n",
    "val_reliability_p = [0.15, 0.35, 0.55, 0.75, 0.95]  # for validation set\n",
    "\n",
    "n_train     = 3\n",
    "n_val       = 6\n",
    "n_test100   = 3\n",
    "n_test_large = 2\n",
    "\n",
    "base_dir     = 'data'\n",
    "graphs_dir   = os.path.join(base_dir, 'graphs')\n",
    "labels_dir   = os.path.join(base_dir, 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fcb37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_dirs():\n",
    "    for split in ['train', 'val', 'test100', 'test_large']:\n",
    "        os.makedirs(os.path.join(graphs_dir, split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(labels_dir, split), exist_ok=True)\n",
    "\n",
    "make_split_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ca1b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Graph generation ---\n",
    "def gen_graph(topo, size, s_param, rel_p, seed):\n",
    "    random.seed(seed)\n",
    "    if topo == 'ER':\n",
    "        G = nx.erdos_renyi_graph(size, s_param, seed=seed)\n",
    "    elif topo == 'BA':\n",
    "        G = nx.barabasi_albert_graph(size, int(s_param), seed=seed)\n",
    "    elif topo == 'SW':\n",
    "        G = nx.watts_strogatz_graph(size, k=4, p=s_param, seed=seed)\n",
    "    nx.set_edge_attributes(G, rel_p, 'p')\n",
    "    # for u, v in G.edges():\n",
    "    #     G[u][v]['p'] = rel_p\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "778fd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(G, meta, idx, split):\n",
    "    # save into corresponding split subfolder\n",
    "    fname = f\"{meta['topo']}_sz{meta['size']}_sp{meta['s_param']}_rp{meta['rel_p']}_{split}_{idx}.pkl\"\n",
    "    path = os.path.join(graphs_dir, split, fname)\n",
    "    # ensure the directory exists\n",
    "    dirpath = os.path.dirname(path)\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    # if a directory with the same name exists, this will fail; remove or rename it first\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump({'graph': G, 'meta': meta}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56d76874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split(split, sizes, n_graphs, reliability_p):\n",
    "    for topo, params in structural_params.items():\n",
    "        key = list(params.keys())[0]\n",
    "\n",
    "        for s_param in params[key]:\n",
    "            for size in sizes:\n",
    "                for rel_p in reliability_p:\n",
    "                    for i in range(n_graphs):\n",
    "\n",
    "                        seed = hash((topo, s_param, size, rel_p, split, i)) & 0xffffffff\n",
    "                        G = gen_graph(topo, size, s_param, rel_p, seed)\n",
    "                        meta = {'topo': topo, 'size': size, 's_param': s_param, 'rel_p': rel_p}\n",
    "                        save_graph(G, meta, i, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bd1f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_split('train',      train_sizes,   n_train, reliability_p)\n",
    "# generate_split('val',        [100],         n_val, reliability_p=val_reliability_p)\n",
    "# generate_split('test100',    [100],         n_test100, reliability_p)\n",
    "generate_split('test_large', test_sizes[1:],n_test_large, reliability_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2672607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b18a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Label generation  ---\n",
    "def compute_labels(file_path, mc_samples=1_000):\n",
    "\n",
    "    data = pickle.load(open(file_path, 'rb'))\n",
    "    G_orig = data['graph']\n",
    "\n",
    "    base = epc_mc_deleted(G_orig, set(), num_samples=mc_samples)\n",
    "    \n",
    "    n = G_orig.number_of_nodes()\n",
    "    labels = torch.zeros(n)\n",
    "\n",
    "    for v in G_orig.nodes():\n",
    "        # print(v)\n",
    "        # print(type(set(v)))\n",
    "        drop = epc_mc_deleted(G_orig, {v}, num_samples=mc_samples)\n",
    "        labels[v] = base - drop\n",
    "        \n",
    "    # stabilise scale\n",
    "    labels = torch.log1p(labels.clamp(min=0))\n",
    "\n",
    "    # save labels\n",
    "    fname = os.path.basename(file_path).replace('.pkl', '_labels.pt')\n",
    "    split = os.path.basename(os.path.dirname(file_path))\n",
    "    save_path = os.path.join(labels_dir, split, fname)\n",
    "    torch.save(labels, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d94bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_cndp_epc_celf(\n",
    "    G: nx.Graph,\n",
    "    K: int,\n",
    "    *,\n",
    "    num_samples: int = 20_000,\n",
    "    reuse_csr: Tuple = None,\n",
    "    return_trace: bool = False,\n",
    ") -> Union[Set[int], Tuple[Set[int], List[float]]]:\n",
    "    \"\"\"Select **K** nodes that minimise EPC using CELF & Numba.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    return_trace : bool, default *False*\n",
    "        If *True*, also return a list `[σ(S₁), σ(S₂), …]` where `S_i` is the\n",
    "        prefix after deleting *i* nodes.  Useful for plots.\n",
    "    \"\"\"\n",
    "\n",
    "    # CSR cache --------------------------------------------------------\n",
    "    if reuse_csr is None:\n",
    "        nodes, idx_of, indptr, indices, probs = nx_to_csr(G)\n",
    "    else:\n",
    "        nodes, idx_of, indptr, indices, probs = reuse_csr\n",
    "    n = len(nodes)\n",
    "\n",
    "    deleted = np.zeros(n, dtype=np.bool_)\n",
    "    current_sigma = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "\n",
    "    pq: List[Tuple[float, int, int]] = []  # (-gain, v, last_round)\n",
    "    gains = np.empty(n, dtype=np.float32)\n",
    "\n",
    "    for v in range(n):\n",
    "        deleted[v] = True\n",
    "        gains[v] = current_sigma - epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "        deleted[v] = False\n",
    "        heapq.heappush(pq, (-gains[v], v, 0))\n",
    "\n",
    "    S: Set[int] = set()\n",
    "    trace: List[float] = []\n",
    "    round_ = 0\n",
    "\n",
    "    trace.append(current_sigma)\n",
    "\n",
    "    while len(S) < K and pq:\n",
    "        neg_gain, v, last = heapq.heappop(pq)\n",
    "        if last == round_:\n",
    "            # gain up‑to‑date → accept\n",
    "            S.add(nodes[v])\n",
    "            deleted[v] = True\n",
    "            current_sigma += neg_gain  # add neg (= subtract gain)\n",
    "            round_ += 1\n",
    "            if return_trace:\n",
    "                trace.append(current_sigma)\n",
    "        else:\n",
    "            # recompute gain lazily\n",
    "            deleted[v] = True\n",
    "            new_gain = current_sigma - epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "            deleted[v] = False\n",
    "            heapq.heappush(pq, (-new_gain, v, round_))\n",
    "\n",
    "    return (S, trace) if return_trace else S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75e883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "143e92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS_ROOT  = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs\"     # expecting graphs/<split>/<type>/*.pkl\n",
    "LABELS_ROOT  = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/rega_labels\"     # will mirror the same structure\n",
    "\n",
    "# ----- budget percentage -----\n",
    "ALPHA        = 0.10         # 10 % of nodes\n",
    "\n",
    "# ----- MC parameters -----\n",
    "MC_SAMPLES   = 10_000       # inside greedy\n",
    "MC_EPC_SAVE  = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_label(pkl_path: str, alpha: float = ALPHA):\n",
    "    # --- load --------------------------------------------------------\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        G = pickle.load(f)[\"graph\"]\n",
    "\n",
    "    N = G.number_of_nodes()\n",
    "    K = max(1, math.ceil(alpha * N))\n",
    "\n",
    "    # print(f\"K: {K}\")\n",
    "    \n",
    "    # --- greedy delete set ------------------------------------------\n",
    "    delete_set = rega(G, K, num_samples=MC_SAMPLES)\n",
    "\n",
    "    # --- binary mask -------------------------------------------------\n",
    "    mask = torch.zeros(N, dtype=torch.float32)\n",
    "    mask[list(delete_set)] = 1.0\n",
    "\n",
    "    # --- save --------------------------------------------------------\n",
    "    #  graphs/<split>/<type>/foo.pkl  ->  labels/<split>/<type>/foo_labels.pt\n",
    "    rel_dir   = os.path.relpath(os.path.dirname(pkl_path), GRAPHS_ROOT)\n",
    "    save_dir  = os.path.join(LABELS_ROOT, rel_dir)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    fname_out = os.path.basename(pkl_path).replace(\".pkl\", \"_labels.pt\")\n",
    "    torch.save(mask, os.path.join(save_dir, fname_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6191b44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building greedy labels:   0%|          | 0/720 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rega() missing 1 required positional argument: 'epc_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m all_graphs \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(GRAPHS_ROOT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m tqdm(all_graphs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding greedy labels\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mbuild_and_save_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALPHA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Finished.  All binary-mask labels written to\u001b[39m\u001b[38;5;124m\"\u001b[39m, LABELS_ROOT)\n",
      "Cell \u001b[0;32mIn[65], line 12\u001b[0m, in \u001b[0;36mbuild_and_save_label\u001b[0;34m(pkl_path, alpha)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- greedy delete set ------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m delete_set \u001b[38;5;241m=\u001b[39m \u001b[43mrega\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMC_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# --- binary mask -------------------------------------------------\u001b[39;00m\n\u001b[1;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(N, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mTypeError\u001b[0m: rega() missing 1 required positional argument: 'epc_func'"
     ]
    }
   ],
   "source": [
    "# label_path = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs\"\n",
    "\n",
    "all_graphs = glob.glob(os.path.join(GRAPHS_ROOT, '*', '*.pkl'))\n",
    "\n",
    "for fp in tqdm(all_graphs, desc=\"building greedy labels\"):\n",
    "    build_and_save_label(fp, alpha=ALPHA)\n",
    "\n",
    "print(\"✓ Finished.  All binary-mask labels written to\", LABELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e1945e",
   "metadata": {},
   "source": [
    "## Previous label generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2026b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing labels:   0%|          | 0/1080 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m all_graphs \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(graphs_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m tqdm(all_graphs, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_graphs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing labels\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mcompute_labels\u001b[49m(fp, \u001b[38;5;241m10_000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "all_graphs = glob.glob(os.path.join(graphs_dir, '*', '*.pkl'))\n",
    "\n",
    "for fp in tqdm(all_graphs, total=len(all_graphs), desc=\"Computing labels\"):\n",
    "    compute_labels(fp, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"path/to/your_file.pt\", map_location=\"cpu\")\n",
    "\n",
    "# 2. See what you got:\n",
    "print(type(data))\n",
    "# e.g. <class 'dict'> (often a state_dict) or a ScriptModule\n",
    "\n",
    "# 3. If it’s a dict of tensors (state_dict):\n",
    "if isinstance(data, dict):\n",
    "    for k, v in data.items():\n",
    "        print(f\"{k:40s} → {tuple(v.shape) if hasattr(v, 'shape') else type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c71db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "base_folder = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels\"         # Folder with .pt files\n",
    "output_folder = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels_txt\"   # Destination folder for .txt files\n",
    "\n",
    "for root, _, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pt\"):\n",
    "            pt_path = os.path.join(root, file)\n",
    "            \n",
    "            # Load tensor or model (depending on format)\n",
    "            try:\n",
    "                content = torch.load(pt_path, map_location='cpu')\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {pt_path} due to load error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Generate corresponding txt path\n",
    "            rel_path = os.path.relpath(pt_path, base_folder)\n",
    "            txt_path = os.path.join(output_folder, os.path.splitext(rel_path)[0] + \".txt\")\n",
    "            os.makedirs(os.path.dirname(txt_path), exist_ok=True)\n",
    "\n",
    "            # Write to text file\n",
    "            try:\n",
    "                with open(txt_path, \"w\") as f:\n",
    "                    f.write(str(content))\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing {txt_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a58222",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bab057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neigbors(g, node, depth):\n",
    "    output = {}\n",
    "    layers = dict(nx.bfs_successors(g, source=node, depth_limit=depth))\n",
    "    nodes = [node]\n",
    "    for i in range(1, depth + 1):\n",
    "        output[i] = []\n",
    "        for x in nodes:\n",
    "            output[i].extend(layers.get(x, []))\n",
    "        nodes = output[i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87959b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgl_g_input(G):\n",
    "    input = torch.ones(len(G), 11)\n",
    "    for i in G.nodes():\n",
    "        input[i, 0] = G.degree()[i]\n",
    "        input[i, 1] = sum([G.degree()[j] for j in list(G.neighbors(i))]) / max(len(list(G.neighbors(i))), 1)\n",
    "        input[i, 2] = sum([nx.clustering(G, j) for j in list(G.neighbors(i))]) / max(len(list(G.neighbors(i))), 1)\n",
    "        egonet = G.subgraph(list(G.neighbors(i)) + [i])\n",
    "        input[i, 3] = len(egonet.edges())\n",
    "        input[i, 4] = sum([G.degree()[j] for j in egonet.nodes()]) - 2 * input[i, 3]\n",
    "\n",
    "    for l in [1, 2, 3]:\n",
    "        for i in G.nodes():\n",
    "            ball = get_neigbors(G, i, l)\n",
    "            input[i, 5 + l - 1] = (G.degree()[i] - 1) * sum([G.degree()[j] - 1 for j in ball[l]])\n",
    "\n",
    "    v = nx.voterank(G)\n",
    "    votescore = dict()\n",
    "    \n",
    "    for i in list(G.nodes()): votescore[i] = 0\n",
    "    for i in range(len(v)):\n",
    "        votescore[v[i]] = len(G) - i\n",
    "    e = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    k = nx.core_number(G)\n",
    "    for i in G.nodes():\n",
    "        input[i, 8] = votescore[i]\n",
    "        input[i, 9] = e[i]\n",
    "        input[i, 10] = k[i]\n",
    "    for i in range(len(input[0])):\n",
    "        if max(input[:, i]) != 0:\n",
    "            input[:, i] = input[:, i] / max(input[:, i])\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345313a3",
   "metadata": {},
   "source": [
    "# 3. Traininig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11336b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node_features(G):\n",
    "    \"\"\"\n",
    "    Compute per-node structural features for DGL input.\n",
    "    Returns: torch.FloatTensor of shape [num_nodes, 11]\n",
    "    Features:\n",
    "      0: degree\n",
    "      1: avg neighbor degree\n",
    "      2: avg neighbor clustering coeff\n",
    "      3: egonet edge count\n",
    "      4: egonet sum-degree minus internal edges (volume)\n",
    "      5-7: l-hop neighbor sum-degree offsets for l=1,2,3\n",
    "      8: voterank score\n",
    "      9: eigenvector centrality\n",
    "     10: k-core number\n",
    "    Normalized per feature by dividing by feature-wise max.\n",
    "    \"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    feats = torch.ones(n, 11)\n",
    "\n",
    "    # precompute degrees and clustering\n",
    "    deg = dict(G.degree())\n",
    "    clust = nx.clustering(G)\n",
    "\n",
    "    # voterank ordering and scoring\n",
    "    order = nx.voterank(G)\n",
    "    vote_score = {u: n - i for i, u in enumerate(order)}\n",
    "\n",
    "    # eigenvector centrality\n",
    "    eig = nx.eigenvector_centrality(G, max_iter=500)\n",
    "    core = nx.core_number(G)\n",
    "\n",
    "    # compute for each node\n",
    "    for u in G.nodes():\n",
    "        nbrs = list(G.neighbors(u))\n",
    "        feats[u, 0] = deg[u]\n",
    "        feats[u, 1] = sum(deg[v] for v in nbrs) / max(len(nbrs), 1)\n",
    "        feats[u, 2] = sum(clust[v] for v in nbrs) / max(len(nbrs), 1)\n",
    "        egonet = G.subgraph(nbrs + [u])\n",
    "        feats[u, 3] = egonet.number_of_edges()\n",
    "        feats[u, 4] = sum(deg[v] for v in egonet.nodes()) - 2 * feats[u, 3]\n",
    "        # l-hop neighbor sums\n",
    "        for l in (1,2,3):\n",
    "            # BFS up to l hops\n",
    "            visited = {u}\n",
    "            queue = deque([(u, 0)])\n",
    "            hop_nodes = set()\n",
    "            while queue:\n",
    "                v, d = queue.popleft()\n",
    "                if d == l: continue\n",
    "                for w in G.neighbors(v):\n",
    "                    if w not in visited:\n",
    "                        visited.add(w)\n",
    "                        queue.append((w, d+1))\n",
    "                        if d+1 == l:\n",
    "                            hop_nodes.add(w)\n",
    "            feats[u, 4 + l] = sum(deg[v] - 1 for v in hop_nodes)\n",
    "        feats[u, 8] = vote_score.get(u, 0)\n",
    "        feats[u, 9] = eig.get(u, 0)\n",
    "        feats[u, 10] = core.get(u, 0)\n",
    "\n",
    "    # normalize each feature dimension\n",
    "    for i in range(feats.size(1)):\n",
    "        col = feats[:, i]\n",
    "        maxval = col.max()\n",
    "        if maxval > 0:\n",
    "            feats[:, i] = col / maxval\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82417ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAGE2AttnModel_fanout(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden_dim=128, num_heads=2):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         self.norms  = nn.ModuleList()\n",
    "\n",
    "#         fanouts = [15, 10, 5]\n",
    "\n",
    "#         for _ in fanouts:\n",
    "#             self.layers.append(SAGEConv(in_dim, hidden_dim, 'lstm'))\n",
    "#             self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "#             in_dim = hidden_dim\n",
    "            \n",
    "#         self.attn = nn.MultiheadAttention(hidden_dim, num_heads)\n",
    "#         self.out  = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, g, x):\n",
    "#         h = x\n",
    "#         for sage, norm in zip(self.layers, self.norms):\n",
    "#             h = F.relu(norm(sage(g, h)))\n",
    "#         # attention works with seq_len x batch_size x hidden\n",
    "#         # here nodes as sequence, batch=1\n",
    "#         h2, _ = self.attn(h.unsqueeze(1), h.unsqueeze(1), h.unsqueeze(1))\n",
    "#         h2 = h2.squeeze(1)\n",
    "#         return self.out(h2).squeeze(-1)\n",
    "\n",
    "class SAGE2AttnModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE + attention model using full-graph (no neighbor sampling).\n",
    "    Three SAGEConv layers followed by multi-head self-attention and output head.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim=128, num_heads=2, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms  = nn.ModuleList()\n",
    "        # Build fixed number of layers without explicit fan-out\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(SAGEConv(in_dim, hidden_dim, 'lstm'))\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        # self-attention across all nodes\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads)\n",
    "        self.out  = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        h = x\n",
    "        # message-passing over full graph\n",
    "        for sage, norm in zip(self.layers, self.norms):\n",
    "            h = F.relu(norm(sage(g, h)))\n",
    "        # apply self-attention: treat nodes as sequence length\n",
    "        h2, _ = self.attn(h.unsqueeze(1), h.unsqueeze(1), h.unsqueeze(1))\n",
    "        h2 = h2.squeeze(1)\n",
    "        return self.out(h2).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "14507a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class GraphEPCDataset(Dataset):\n",
    "    def __init__(self, graphs_dir, labels_dir, split):\n",
    "        self.graph_paths = glob.glob(os.path.join(graphs_dir, split, '*.pkl'))\n",
    "        self.labels_dir  = os.path.join(labels_dir, split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graph_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --------  load graph ----------\n",
    "        path = self.graph_paths[idx]\n",
    "        G_nx  = pickle.load(open(path, 'rb'))['graph']\n",
    "\n",
    "        # node-level features\n",
    "        x = extract_node_features(G_nx)          # [N, 11]\n",
    "\n",
    "        # edge index & probabilities\n",
    "        edges      = list(G_nx.edges())\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)  # undirected\n",
    "        p_list     = [G_nx[u][v]['p'] for u, v in edges]\n",
    "        edge_prob  = torch.tensor(p_list + p_list, dtype=torch.float)\n",
    "\n",
    "        # labels  (make sure they are float for MSELoss)\n",
    "        lbl_name = os.path.basename(path).replace('.pkl', '_labels.pt')\n",
    "        y        = torch.load(os.path.join(self.labels_dir, lbl_name)).float()\n",
    "\n",
    "        # --------  wrap in Data ----------\n",
    "        data = Data(x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_prob=edge_prob,\n",
    "                    y=y)   \n",
    "        \n",
    "        data.file_name = os.path.basename(path)\n",
    "        data.idx = torch.tensor(idx, dtype=torch.long)  # add index for reference\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e630be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "class EdgeProbGATConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 heads: int = 2,\n",
    "                 negative_slope: float = 0.2,\n",
    "                 dropout: float = 0.2,\n",
    "                 concat: bool = True,\n",
    "                 bias: bool = True):\n",
    "        super().__init__(aggr='add', node_dim=0)  # standard GAT aggregation\n",
    "        \n",
    "        self.in_channels   = in_channels\n",
    "        self.out_channels  = out_channels\n",
    "        self.heads         = heads\n",
    "        self.negative_slope= negative_slope\n",
    "        self.dropout       = dropout\n",
    "        self.concat        = concat\n",
    "\n",
    "        # Linear projection for query/key/value\n",
    "        self.lin = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        # Attention weights aᵀ [Wh_i || Wh_j]\n",
    "        # self.att = nn.Parameter(torch.Tensor(1, heads, 2*out_channels))\n",
    "        self.att = nn.Parameter(torch.Tensor(1, heads, 2*out_channels + 1))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "        nn.init.xavier_uniform_(self.att)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                edge_index: torch.LongTensor,\n",
    "                edge_prob: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [N, in_channels]\n",
    "        edge_index: [2, E]\n",
    "        edge_prob: [E]   (the p_ij for each edge in edge_index order)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        # 1. Linearly project node features to multi-head space\n",
    "        x = self.lin(x)                              # [N, heads*out]\n",
    "        x = x.view(N, self.heads, self.out_channels) # [N, heads, out]\n",
    "\n",
    "        # 2. Start propagation\n",
    "        out = self.propagate(edge_index, x=x, edge_prob=edge_prob, size=(N, N))\n",
    "        # out: [N, heads, out]\n",
    "\n",
    "        # 3. Concat or average heads\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # [N, out]\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    # def message(self,\n",
    "    #             x_j: torch.Tensor,\n",
    "    #             x_i: torch.Tensor,\n",
    "    #             edge_prob: torch.Tensor,\n",
    "    #             index: torch.LongTensor,\n",
    "    #             ptr,\n",
    "    #             size_i):\n",
    "    #     \"\"\"\n",
    "    #     x_j, x_i: [E, heads, out_channels] (sender and receiver node reps)\n",
    "    #     edge_prob: [E]           (scalar reliability)\n",
    "    #     index:   [E]             (destination node indices)\n",
    "    #     \"\"\"\n",
    "    #     # 1. compute standard attention logits: aᵀ [Wh_i || Wh_j]\n",
    "    #     cat = torch.cat([x_i, x_j], dim=-1)           # [E, heads, 2*out]\n",
    "    #     alpha = (cat * self.att).sum(dim=-1)          # [E, heads]\n",
    "\n",
    "    #     # 2. add log(edge_prob)\n",
    "    #     log_p = edge_prob.log().unsqueeze(-1)        # [E, 1]\n",
    "    #     alpha = alpha + log_p                        # broadcasting to [E, heads]\n",
    "\n",
    "    #     # 3. leaky‐relu + softmax over all incoming edges\n",
    "    #     alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "    #     alpha = softmax(alpha, index, ptr, size_i)    # [E, heads]\n",
    "\n",
    "    #     # 4. dropout on attention weights\n",
    "    #     alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "    #     # 5. scale messages\n",
    "    #     return x_j * alpha.unsqueeze(-1)             # [E, heads, out]\n",
    "\n",
    "    def message(self, x_j, x_i, edge_prob, index, ptr, size_i):\n",
    "        # concat node reps and edge scalar\n",
    "        edge_prob = edge_prob.view(-1, 1, 1)               # [E,1,1]\n",
    "        cat = torch.cat([x_i, x_j, edge_prob.expand(-1, self.heads, 1)], dim=-1)\n",
    "        # shape: [E, heads, 2*out+1]\n",
    "\n",
    "        alpha = (cat * self.att).sum(dim=-1)               # [E, heads]\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha.unsqueeze(-1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out: [N, heads, out] if concat else [N, out]\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e4494fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEEdgeProbModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = SAGEConv(in_dim,  hidden_dim, normalize=True)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim , normalize=True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim )\n",
    "        self.conv3 = SAGEConv(hidden_dim, hidden_dim, normalize=True)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # now our custom GAT that adds log(p_ij)\n",
    "        self.gat_edge = EdgeProbGATConv(hidden_dim, hidden_dim, \n",
    "                                        heads=heads, dropout=dropout)\n",
    "        \n",
    "        self.out       = nn.Linear(heads * hidden_dim, 1)  # if concat=True\n",
    "\n",
    "    def forward(self, x, edge_index, edge_prob):\n",
    "        # x: [N, in_dim], edge_prob: [E]\n",
    "        h = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h = F.relu(self.bn2(self.conv2(h, edge_index)))\n",
    "        h = F.relu(self.bn3(self.conv3(h, edge_index)))\n",
    "\n",
    "        # incorporate per-edge probabilities\n",
    "        h = self.gat_edge(h, edge_index, edge_prob)  # [N, heads*out]\n",
    "\n",
    "        return self.out(h).squeeze(-1)               # [N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be556652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SAGEEdgeProbModel(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden_dim=256, heads=4):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # ── SAGE blocks ───────────────────────────────────────\n",
    "#         self.conv1 = SAGEConv(in_dim, hidden_dim, normalize=True)\n",
    "#         self.bn1   = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "#         self.conv2 = SAGEConv(hidden_dim, hidden_dim * 2, normalize=True)\n",
    "#         self.bn2   = nn.BatchNorm1d(hidden_dim * 2)\n",
    "\n",
    "#         self.conv3 = SAGEConv(hidden_dim * 2, hidden_dim, normalize=True)\n",
    "#         self.bn3   = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "#         # 1×1 “shortcut” projections so shapes match for +\n",
    "#         self.sc12 = nn.Linear(hidden_dim,       hidden_dim * 2, bias=False)\n",
    "#         self.sc23 = nn.Linear(hidden_dim * 2,   hidden_dim,     bias=False)\n",
    "\n",
    "#         # ── edge-aware GAT head ───────────────────────────────\n",
    "#         self.gat_edge = EdgeProbGATConv(\n",
    "#             hidden_dim, hidden_dim, heads=heads, dropout=0.3\n",
    "#         )\n",
    "\n",
    "#         self.out = nn.Linear(heads * hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_prob):\n",
    "#         # Block 1\n",
    "#         h1 = F.relu(self.bn1(self.conv1(x, edge_index)))        # [N, H]\n",
    "\n",
    "#         # Block 2  (add projected shortcut from h1)\n",
    "#         h2 = F.relu(self.bn2(self.conv2(h1, edge_index))\n",
    "#                     + self.sc12(h1))                            # [N, 2H]\n",
    "\n",
    "#         # Block 3  (add projected shortcut from h2)\n",
    "#         h3 = F.relu(self.bn3(self.conv3(h2, edge_index))\n",
    "#                     + self.sc23(h2))                            # [N, H]\n",
    "\n",
    "#         # Edge-probability attention\n",
    "#         h = self.gat_edge(h3, edge_index, edge_prob)            # [N, heads·H]\n",
    "\n",
    "#         return self.out(h).squeeze(-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a4b8b",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "def train_model():\n",
    "    SEED = 42\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    in_dim = 11                   # keep your original setting\n",
    "    model  = SAGEEdgeProbModel(in_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn  = nn.MSELoss()\n",
    "\n",
    "    train_ds = GraphEPCDataset(graphs_dir, labels_dir, 'train')\n",
    "    val_ds   = GraphEPCDataset(graphs_dir, labels_dir, 'val')\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=32)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    for epoch in range(1, 31):\n",
    "        # -------------------- training --------------------\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for data in train_loader:                   # data is a Batch\n",
    "            data = data.to(device)                  \n",
    "            preds = model(data.x, data.edge_index, data.edge_prob)\n",
    "            loss  = loss_fn(preds, data.y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # -------------------- validation ------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                val_loss += loss_fn(\n",
    "                    model(data.x, data.edge_index, data.edge_prob),\n",
    "                    data.y\n",
    "                ).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: Train={avg_loss:.4f} | Val={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(base_dir, 'best_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e3758",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f18f9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS_DIR = r\"C:\\Users\\btugu\\Documents\\develop\\research\\SCNDP\\src\\extension\\learning\\notebooks\\gnn\\data\\graphs\"\n",
    "LABELS_DIR = r\"C:\\Users\\btugu\\Documents\\develop\\research\\SCNDP\\src\\extension\\learning\\notebooks\\gnn\\data\\graphs_labels\"\n",
    "LABELS_OLD_DIR = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 256\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2c7029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma: float = 2.0, alpha: float | None = None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        # optional α-balancing (same role as pos_weight)\n",
    "        self.alpha = alpha            # scalar ∈ (0,1) or None\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: [N], targets: 0/1 floats\n",
    "        prob = torch.sigmoid(logits)\n",
    "        pt   = prob * targets + (1 - prob) * (1 - targets)   # p_t\n",
    "        focal = (1 - pt) ** self.gamma\n",
    "        logp  = F.binary_cross_entropy_with_logits(\n",
    "                    logits, targets, reduction='none')\n",
    "        if self.alpha is not None:\n",
    "            α_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            logp = α_t * logp\n",
    "        return (focal * logp).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "01ec889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "import torch.nn as nn, torch\n",
    "\n",
    "base_dir     = 'data'\n",
    "\n",
    "def train_model():\n",
    "    SEED = 42\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ---------------- model ----------------\n",
    "    model = SAGEEdgeProbModel(in_dim=11, hidden_dim=256).to(device)\n",
    "    optimizer  = torch.optim.AdamW(model.parameters(), \n",
    "                                   lr=1.5e-3, weight_decay=1e-4)\n",
    "    # scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #                optimizer, mode='min', factor=0.5, patience=4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=4, min_lr=1e-5)\n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    #           optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    pos_weight = torch.tensor(9.0, device=device)\n",
    "    # loss_fn = nn.MSELoss()  # for regression task\n",
    "    loss_fn    = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    # loss_fn = FocalLoss(gamma=2.0, alpha=0.10).to(device)\n",
    "\n",
    "    # ---------------- data -----------------\n",
    "    train_ds = GraphEPCDataset(GRAPHS_DIR, LABELS_DIR, 'train')\n",
    "    val_ds   = GraphEPCDataset(GRAPHS_DIR, LABELS_DIR, 'val')\n",
    "\n",
    "    train_loader = PyGDataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
    "    val_loader   = PyGDataLoader(val_ds,   batch_size=BATCH)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # ---------- training ----------\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index, batch.edge_prob)\n",
    "            loss   = loss_fn(logits, batch.y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # scheduler.step()\n",
    "        # ---------- validation ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        auroc = BinaryAUROC().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                logits = model(batch.x, batch.edge_index, batch.edge_prob)\n",
    "                auroc.update(logits, batch.y.int())\n",
    "                val_loss += loss_fn(logits, batch.y).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train {avg_loss:.4f} \"\n",
    "              f\"| val {val_loss:.4f} | AUROC {auroc.compute():.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(base_dir, 'best_model_bce_50_(1)_residual.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f1b812c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        *,\n",
    "        hidden_dim      = 256,\n",
    "        heads           = 4,\n",
    "        dropout         = 0.30,\n",
    "        lr              = 1e-3,\n",
    "        weight_decay    = 1e-4,\n",
    "        loss_name       = \"bce\",          # \"bce\" or \"focal\"\n",
    "        epochs          = 10,\n",
    "        batch_size      = 32,\n",
    "        seed            = 42,\n",
    "        silent          = True):          # suppress epoch prints\n",
    "    \"\"\"\n",
    "    Train for `epochs`, return best val_AUROC.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed); \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ---- model ----\n",
    "    model = SAGEEdgeProbModel(11, hidden_dim, heads, dropout).to(device)\n",
    "    opt   = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=lr, weight_decay=weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "              opt, mode='min', factor=0.5, patience=3, min_lr=1e-5)\n",
    "\n",
    "    if loss_name == \"mse\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    else:\n",
    "        pos_weight = torch.tensor(9.0, device=device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    train_ds = GraphEPCDataset(GRAPHS_DIR, LABELS_DIR, 'train')\n",
    "    val_ds   = GraphEPCDataset(GRAPHS_DIR, LABELS_DIR, 'val')\n",
    "    train_loader = PyGDataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = PyGDataLoader(val_ds,   batch_size=batch_size)\n",
    "\n",
    "    best_auroc = 0.0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            loss  = loss_fn(model(batch.x, batch.edge_index, batch.edge_prob),\n",
    "                            batch.y)\n",
    "            opt.zero_grad(); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        # --- validate ---\n",
    "        model.eval(); val_loss, auroc = 0.0, BinaryAUROC().to(device)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                logits = model(batch.x, batch.edge_index, batch.edge_prob)\n",
    "                val_loss += loss_fn(logits, batch.y).item()\n",
    "                auroc.update(logits, batch.y.int())\n",
    "        val_loss /= len(val_loader); au = auroc.compute().item()\n",
    "        sched.step(val_loss)\n",
    "        best_auroc = max(best_auroc, au)\n",
    "\n",
    "        if not silent:\n",
    "            print(f\"ep{epoch:02d} val_loss={val_loss:.4f} AUROC={au:.4f}\")\n",
    "\n",
    "    return best_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "651d1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-15 03:35:39,196] A new study created in memory with name: no-name-38c1eda4-a0cd-4ee5-be4a-75da962301be\n",
      "C:\\Users\\btugu\\AppData\\Local\\Temp\\ipykernel_6448\\1237262016.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr          = trial.suggest_loguniform(\"lr\", 1e-4, 3e-3),\n",
      "[I 2025-07-15 03:41:53,951] Trial 0 finished with value: 0.9117720127105713 and parameters: {'hidden_dim': 128, 'heads': 2, 'dropout': 0.5, 'lr': 0.0017795372456809143, 'loss_name': 'bce'}. Best is trial 0 with value: 0.9117720127105713.\n",
      "[I 2025-07-15 03:51:06,656] Trial 1 finished with value: 0.914645254611969 and parameters: {'hidden_dim': 512, 'heads': 8, 'dropout': 0.2, 'lr': 0.0009953769602514386, 'loss_name': 'bce'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 03:58:40,868] Trial 2 finished with value: 0.9085859060287476 and parameters: {'hidden_dim': 512, 'heads': 4, 'dropout': 0.25, 'lr': 0.0002874864607757464, 'loss_name': 'mse'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 04:07:49,786] Trial 3 finished with value: 0.9063542485237122 and parameters: {'hidden_dim': 512, 'heads': 8, 'dropout': 0.25, 'lr': 0.0009427363840057644, 'loss_name': 'mse'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 04:14:33,159] Trial 4 finished with value: 0.9132238030433655 and parameters: {'hidden_dim': 512, 'heads': 2, 'dropout': 0.25, 'lr': 0.00013566553488851842, 'loss_name': 'bce'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 04:20:54,010] Trial 5 finished with value: 0.8873653411865234 and parameters: {'hidden_dim': 128, 'heads': 8, 'dropout': 0.45, 'lr': 0.00027316387847157634, 'loss_name': 'mse'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 04:27:58,167] Trial 6 finished with value: 0.9094210863113403 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.30000000000000004, 'lr': 0.00014702281456552708, 'loss_name': 'bce'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 04:34:02,077] Trial 7 finished with value: 0.913539707660675 and parameters: {'hidden_dim': 256, 'heads': 2, 'dropout': 0.35000000000000003, 'lr': 0.00025912626719656916, 'loss_name': 'bce'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 04:40:42,429] Trial 8 finished with value: 0.9137137532234192 and parameters: {'hidden_dim': 512, 'heads': 2, 'dropout': 0.5, 'lr': 0.00047493377403399594, 'loss_name': 'bce'}. Best is trial 1 with value: 0.914645254611969.\n",
      "[I 2025-07-15 04:47:47,699] Trial 9 finished with value: 0.9148653745651245 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.45, 'lr': 0.001960906811177661, 'loss_name': 'bce'}. Best is trial 9 with value: 0.9148653745651245.\n",
      "[I 2025-07-15 04:54:17,880] Trial 10 finished with value: 0.9111837148666382 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.4, 'lr': 0.0029954783109274533, 'loss_name': 'mse'}. Best is trial 9 with value: 0.9148653745651245.\n",
      "[I 2025-07-15 05:01:22,245] Trial 11 finished with value: 0.9152041673660278 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.2, 'lr': 0.0010631701831727437, 'loss_name': 'bce'}. Best is trial 11 with value: 0.9152041673660278.\n",
      "[I 2025-07-15 05:08:29,102] Trial 12 finished with value: 0.9163463711738586 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.4, 'lr': 0.0014136763029707212, 'loss_name': 'bce'}. Best is trial 12 with value: 0.9163463711738586.\n",
      "[I 2025-07-15 05:15:33,164] Trial 13 finished with value: 0.9153498411178589 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.35000000000000003, 'lr': 0.0009620211754141918, 'loss_name': 'bce'}. Best is trial 12 with value: 0.9163463711738586.\n",
      "[I 2025-07-15 05:22:40,294] Trial 14 finished with value: 0.9136741161346436 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.35000000000000003, 'lr': 0.0006161327823287347, 'loss_name': 'bce'}. Best is trial 12 with value: 0.9163463711738586.\n",
      "[I 2025-07-15 05:29:49,852] Trial 15 finished with value: 0.9177669882774353 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.4, 'lr': 0.0015178563257490345, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 05:36:17,906] Trial 16 finished with value: 0.9166387319564819 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.4, 'lr': 0.0018813145998240759, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 05:42:19,786] Trial 17 finished with value: 0.9146334528923035 and parameters: {'hidden_dim': 128, 'heads': 4, 'dropout': 0.4, 'lr': 0.0025734067320685064, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 05:48:43,436] Trial 18 finished with value: 0.9090995788574219 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.0006602711078350775, 'loss_name': 'mse'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 05:55:11,833] Trial 19 finished with value: 0.9165747165679932 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.4, 'lr': 0.0017490649765041998, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:01:14,465] Trial 20 finished with value: 0.915534257888794 and parameters: {'hidden_dim': 128, 'heads': 4, 'dropout': 0.30000000000000004, 'lr': 0.0013187533817784777, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:07:39,756] Trial 21 finished with value: 0.9154056310653687 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.4, 'lr': 0.0022983507012308154, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:14:04,820] Trial 22 finished with value: 0.9167265892028809 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.001636367932453817, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:20:31,186] Trial 23 finished with value: 0.9146117568016052 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.0014897755375047315, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:26:58,143] Trial 24 finished with value: 0.9142662286758423 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.5, 'lr': 0.0007749570398503056, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:33:22,672] Trial 25 finished with value: 0.915733277797699 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.0022842854585527613, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:39:50,905] Trial 26 finished with value: 0.9062693119049072 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.4, 'lr': 0.0004551592845658851, 'loss_name': 'mse'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:46:21,102] Trial 27 finished with value: 0.9145545959472656 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.30000000000000004, 'lr': 0.0013184207717213052, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:52:17,074] Trial 28 finished with value: 0.9129284024238586 and parameters: {'hidden_dim': 128, 'heads': 2, 'dropout': 0.35000000000000003, 'lr': 0.002963807706057704, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 06:58:43,546] Trial 29 finished with value: 0.9170495271682739 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.5, 'lr': 0.0017516523408153976, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:04:08,332] Trial 30 finished with value: 0.9121416807174683 and parameters: {'hidden_dim': 128, 'heads': 2, 'dropout': 0.5, 'lr': 0.0017056839439621452, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:10:33,724] Trial 31 finished with value: 0.917241632938385 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.5, 'lr': 0.0019801531842330963, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:16:57,410] Trial 32 finished with value: 0.9167373776435852 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.5, 'lr': 0.0012985994600585194, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:23:20,626] Trial 33 finished with value: 0.9121692180633545 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.5, 'lr': 0.0010937871184760308, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:30:49,267] Trial 34 finished with value: 0.9145336151123047 and parameters: {'hidden_dim': 512, 'heads': 4, 'dropout': 0.5, 'lr': 0.0021225812689036365, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:37:12,782] Trial 35 finished with value: 0.9097270965576172 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.5, 'lr': 0.0008290208040897984, 'loss_name': 'mse'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:46:17,548] Trial 36 finished with value: 0.9165458679199219 and parameters: {'hidden_dim': 512, 'heads': 8, 'dropout': 0.5, 'lr': 0.00129714834117345, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:53:22,977] Trial 37 finished with value: 0.9087624549865723 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.45, 'lr': 0.00010410414937534626, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 07:59:46,592] Trial 38 finished with value: 0.9070833921432495 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.0010990209874404066, 'loss_name': 'mse'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:06:24,506] Trial 39 finished with value: 0.9132193326950073 and parameters: {'hidden_dim': 512, 'heads': 2, 'dropout': 0.5, 'lr': 0.0024588005870623663, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:13:30,053] Trial 40 finished with value: 0.914501428604126 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.5, 'lr': 0.0007600730526058774, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:19:56,301] Trial 41 finished with value: 0.9168282151222229 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.0016154044195425292, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:26:23,469] Trial 42 finished with value: 0.9162514209747314 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.0015835227843062167, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:32:49,110] Trial 43 finished with value: 0.9168704152107239 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.001993364360819567, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:39:14,566] Trial 44 finished with value: 0.9155520796775818 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.002056970498604403, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:45:39,121] Trial 45 finished with value: 0.9156282544136047 and parameters: {'hidden_dim': 256, 'heads': 4, 'dropout': 0.45, 'lr': 0.0027924417982803174, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 08:52:46,327] Trial 46 finished with value: 0.9119492769241333 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.45, 'lr': 0.0018172385636570198, 'loss_name': 'mse'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 09:00:14,335] Trial 47 finished with value: 0.9126566648483276 and parameters: {'hidden_dim': 512, 'heads': 4, 'dropout': 0.4, 'lr': 0.0011594505967046742, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 09:05:59,841] Trial 48 finished with value: 0.9106995463371277 and parameters: {'hidden_dim': 128, 'heads': 2, 'dropout': 0.5, 'lr': 0.0003340840978118456, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n",
      "[I 2025-07-15 09:13:03,466] Trial 49 finished with value: 0.9098690152168274 and parameters: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.4, 'lr': 0.00020966267694709197, 'loss_name': 'bce'}. Best is trial 15 with value: 0.9177669882774353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUROC: 0.9177669882774353\n",
      "Best params: {'hidden_dim': 256, 'heads': 8, 'dropout': 0.4, 'lr': 0.0015178563257490345, 'loss_name': 'bce'}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = dict(\n",
    "        hidden_dim  = trial.suggest_categorical(\"hidden_dim\", [128,256,512]),\n",
    "        heads       = trial.suggest_categorical(\"heads\", [2,4,8]),\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0.2, 0.5, step=0.05),\n",
    "        lr          = trial.suggest_loguniform(\"lr\", 1e-4, 3e-3),\n",
    "        loss_name   = trial.suggest_categorical(\"loss_name\", [\"bce\", \"mse\"]),\n",
    "    )\n",
    "    return train_model(**params, epochs=15, silent=True)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "print(\"Best AUROC:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e4c82ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 1.2741 | val 1.2377 | AUROC 0.8249\n",
      "Epoch 02 | train 0.9059 | val 1.2404 | AUROC 0.8032\n",
      "Epoch 03 | train 0.8322 | val 1.2367 | AUROC 0.7373\n",
      "Epoch 04 | train 0.8071 | val 1.2362 | AUROC 0.7228\n",
      "Epoch 05 | train 0.7622 | val 1.2272 | AUROC 0.7370\n",
      "Epoch 06 | train 0.7558 | val 1.2301 | AUROC 0.7782\n",
      "Epoch 07 | train 0.7374 | val 1.2279 | AUROC 0.7928\n",
      "Epoch 08 | train 0.6514 | val 1.2312 | AUROC 0.8120\n",
      "Epoch 09 | train 0.6763 | val 1.2060 | AUROC 0.8183\n",
      "Epoch 10 | train 0.7003 | val 1.1931 | AUROC 0.8555\n",
      "Epoch 11 | train 0.6629 | val 1.1802 | AUROC 0.8716\n",
      "Epoch 12 | train 0.6271 | val 1.1288 | AUROC 0.8860\n",
      "Epoch 13 | train 0.6559 | val 1.0651 | AUROC 0.8919\n",
      "Epoch 14 | train 0.6358 | val 0.9896 | AUROC 0.8928\n",
      "Epoch 15 | train 0.6149 | val 1.0048 | AUROC 0.8945\n",
      "Epoch 16 | train 0.5912 | val 0.9636 | AUROC 0.9031\n",
      "Epoch 17 | train 0.6448 | val 0.9057 | AUROC 0.8950\n",
      "Epoch 18 | train 0.5912 | val 0.9794 | AUROC 0.8896\n",
      "Epoch 19 | train 0.6273 | val 0.7631 | AUROC 0.9088\n",
      "Epoch 20 | train 0.6013 | val 0.7422 | AUROC 0.9116\n",
      "Epoch 21 | train 0.5716 | val 0.8298 | AUROC 0.9084\n",
      "Epoch 22 | train 0.5976 | val 0.6944 | AUROC 0.9128\n",
      "Epoch 23 | train 0.5799 | val 0.7325 | AUROC 0.9113\n",
      "Epoch 24 | train 0.5856 | val 0.7142 | AUROC 0.9111\n",
      "Epoch 25 | train 0.5611 | val 0.7053 | AUROC 0.9121\n",
      "Epoch 26 | train 0.5135 | val 0.7121 | AUROC 0.9164\n",
      "Epoch 27 | train 0.5462 | val 0.6955 | AUROC 0.9116\n",
      "Epoch 28 | train 0.5433 | val 0.6993 | AUROC 0.9120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[97], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     43\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 44\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_prob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\btugu\\Documents\\develop\\research\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mGraphEPCDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m G_nx  \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# node-level features\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mextract_node_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_nx\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# [N, 11]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# edge index & probabilities\u001b[39;00m\n\u001b[0;32m     20\u001b[0m edges      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G_nx\u001b[38;5;241m.\u001b[39medges())\n",
      "Cell \u001b[1;32mIn[4], line 40\u001b[0m, in \u001b[0;36mextract_node_features\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m     38\u001b[0m egonet \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39msubgraph(nbrs \u001b[38;5;241m+\u001b[39m [u])\n\u001b[0;32m     39\u001b[0m feats[u, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m egonet\u001b[38;5;241m.\u001b[39mnumber_of_edges()\n\u001b[1;32m---> 40\u001b[0m feats[u, \u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(deg[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43megonet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m feats[u, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# l-hop neighbor sums\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# BFS up to l hops\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\functools.py:979\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname:\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    975\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign the same cached_property to two different names \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    976\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    977\u001b[0m         )\n\u001b[1;32m--> 979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance, owner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c90b30",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff267894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  38%|███▊      | 23/60 [00:01<00:02, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  ['ER_sz100_sp0.0443_rp0.9_test100_0.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  42%|████▏     | 25/60 [00:02<00:04,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_0.pkl  EPC₀=4889.0  after=3632.3  Δ=-1256.6\n",
      "Graph 024 | EPC(after delete) = 3632.3316 | top-K = [75, 53, 44, 17, 25, 57, 50, 74, 12, 0]\n",
      "top-scores id,logit:\n",
      "[(75, 2.639892101287842), (53, 2.5764102935791016), (44, 2.5606229305267334), (17, 2.4412384033203125), (25, 2.128079652786255)]\n",
      "file:  ['ER_sz100_sp0.0443_rp0.9_test100_1.pkl']\n",
      "ER_sz100_sp0.0443_rp0.9_test100_1.pkl  EPC₀=4818.4  after=3695.0  Δ=-1123.3\n",
      "Graph 025 | EPC(after delete) = 3695.0449 | top-K = [89, 38, 23, 70, 76, 98, 22, 34, 46, 17]\n",
      "top-scores id,logit:\n",
      "[(89, 1.991632342338562), (38, 1.8881057500839233), (23, 1.5986566543579102), (70, 1.433693766593933), (76, 1.1931877136230469)]\n",
      "file:  ['ER_sz100_sp0.0443_rp0.9_test100_2.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  45%|████▌     | 27/60 [00:02<00:07,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_2.pkl  EPC₀=4802.8  after=3515.2  Δ=-1287.5\n",
      "Graph 026 | EPC(after delete) = 3515.2380 | top-K = [82, 18, 90, 28, 4, 93, 44, 59, 46, 95]\n",
      "top-scores id,logit:\n",
      "[(82, 2.2519538402557373), (18, 2.038780450820923), (90, 1.9877452850341797), (28, 1.6016144752502441), (4, 1.5051566362380981)]\n",
      "file:  ['ER_sz100_sp0.0443_rp1.0_test100_0.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  47%|████▋     | 28/60 [00:03<00:08,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp1.0_test100_0.pkl  EPC₀=4753.3  after=3654.1  Δ=-1099.3\n",
      "Graph 027 | EPC(after delete) = 3654.0608 | top-K = [63, 57, 54, 23, 84, 18, 5, 33, 22, 28]\n",
      "top-scores id,logit:\n",
      "[(63, 1.9420028924942017), (57, 1.8142141103744507), (54, 1.7594722509384155), (23, 1.6962289810180664), (84, 1.455165982246399)]\n",
      "file:  ['ER_sz100_sp0.0443_rp1.0_test100_1.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  48%|████▊     | 29/60 [00:03<00:08,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp1.0_test100_1.pkl  EPC₀=4950.0  after=3658.4  Δ=-1291.6\n",
      "Graph 028 | EPC(after delete) = 3658.3830 | top-K = [17, 29, 8, 47, 2, 26, 81, 55, 74, 88]\n",
      "top-scores id,logit:\n",
      "[(17, 3.069204807281494), (29, 2.2433431148529053), (8, 1.6436070203781128), (47, 1.5533246994018555), (2, 1.522660493850708)]\n",
      "file:  ['ER_sz100_sp0.0443_rp1.0_test100_2.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  53%|█████▎    | 32/60 [00:04<00:05,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp1.0_test100_2.pkl  EPC₀=4753.5  after=3656.8  Δ=-1096.7\n",
      "Graph 029 | EPC(after delete) = 3656.8476 | top-K = [87, 25, 27, 1, 60, 94, 92, 49, 57, 34]\n",
      "top-scores id,logit:\n",
      "[(87, 3.8588340282440186), (25, 1.9634592533111572), (27, 1.880918264389038), (1, 1.6437443494796753), (60, 1.5613504648208618)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 60/60 [00:06<00:00,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average EPC over 6 test graphs: 3635.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "ROOT = r\"C:\\Users\\btugu\\Documents\\develop\\research\\SCNDP\\src\\extension\\learning\\notebooks\\gnn\\data\"\n",
    "\n",
    "graphs_dir   = f\"{ROOT}/graphs/test_100_separate\"          # same dirs you used for train/val\n",
    "labels_dir   = f\"{ROOT}/graphs_labels/test_100_separate\"          # not needed for inference but Dataset expects it\n",
    "ckpt_path    = f\"{ROOT}/best_model_bce_50_(1)_residual.pt\" # saved in train_model()\n",
    "K            = 10                     # number of nodes to delete\n",
    "mc_samples   = 100_000                 # per-graph Monte-Carlo samples for EPC\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  DATASET & DATALOADER  (batch_size = 1 for clarity)\n",
    "# ------------------------------------------------------------\n",
    "test_ds      = GraphEPCDataset(graphs_dir, labels_dir, split=\"ER\")\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  LOAD MODEL\n",
    "# ------------------------------------------------------------\n",
    "in_dim       = 11                     # you decided to keep only the 11 node features\n",
    "model        = SAGEEdgeProbModel(in_dim).to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  INFERENCE + EPC\n",
    "# ------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_epc = []\n",
    "i = 0\n",
    "\n",
    "for data in tqdm(test_loader, desc=\"Inference\"):\n",
    "\n",
    "    # if i == 30:\n",
    "    #     break\n",
    "    # move everything to GPU/CPU\n",
    "    data = data.to(device)\n",
    "\n",
    "    fname = data.file_name[0]\n",
    "    \n",
    "    if \"sp0.0443\" in fname and (\"rp0.9\" in fname or \"rp1.0\" in fname):\n",
    "        # ---- 3.1 node scores ----\n",
    "        print(\"file: \", data.file_name)\n",
    "        with torch.no_grad():\n",
    "            scores = model(data.x, data.edge_index, data.edge_prob)   # [N]\n",
    "        \n",
    "        # print(data.edge_prob)\n",
    "        # ---- 3.2 pick top-K nodes ----\n",
    "        # scores is already on the same device; .cpu() only if epc_mc_deleted needs CPU tensors\n",
    "        topk = scores.topk(K, largest=True).indices.tolist()         # list[int]\n",
    "\n",
    "        # ---- 3.3 compute EPC after deleting top-K ----\n",
    "        #   We need the *NetworkX graph*; fetch it via the original .pkl\n",
    "        #   The path is stored in test_ds.graph_paths[index] where `index`\n",
    "        #   is the position in the dataset.  The DataLoader gives us that\n",
    "        #   index in data.__dict__['idx']  (PyG attaches it automatically).\n",
    "        idx  = data.idx.item()         # scalar tensor → int\n",
    "        G_nx = pickle.load(open(test_ds.graph_paths[idx], 'rb'))['graph']\n",
    "        \n",
    "        epc_0   = epc_mc_deleted(G_nx.copy(), set(), num_samples=mc_samples)  \n",
    "        epc_del = epc_mc_deleted(G_nx.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "        all_epc.append(epc_del)\n",
    "\n",
    "        delta   = epc_del - epc_0     # negative  ⇒ improvement\n",
    "        print(f\"{fname}  EPC₀={epc_0:.1f}  after={epc_del:.1f}  Δ={delta:+.1f}\")\n",
    "\n",
    "        print(f\"Graph {idx:03d} | EPC(after delete) = {epc_del:.4f} | top-K = {topk}\")\n",
    "        \n",
    "        print(\"top-scores id,logit:\")\n",
    "        print(sorted(zip(topk, scores[topk].tolist()), key=lambda x: -x[1])[:5])\n",
    "\n",
    "        # confirm they exist in the NetworkX graph\n",
    "        assert all(v in G_nx for v in topk)\n",
    "        i += 1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  SUMMARY\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "print(f\"\\nAverage EPC over {len(all_epc)} test graphs: {np.mean(all_epc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0d4c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = r\"C:\\Users\\btugu\\Documents\\develop\\research\\SCNDP\\src\\extension\\learning\\notebooks\\gnn\\data\"\n",
    "\n",
    "graphs_dir   = rf\"{ROOT}/graphs/test_100_separate\"          # same dirs you used for train/val\n",
    "labels_dir   = rf\"{ROOT}/graphs_labels/test_100_separate\"          # not needed for inference but Dataset expects it\n",
    "ckpt_path    = rf\"{ROOT}/best_model_bce_50_(1)_residual.pt\" # saved in train_model()\n",
    "K            = 10                     # number of nodes to delete\n",
    "mc_samples   = 100_000                 # per-graph Monte-Carlo samples for EPC\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Dataset & loader  (batch_size = 1 for clarity)\n",
    "# ------------------------------------------------------------------\n",
    "test_ds     = GraphEPCDataset(graphs_dir, labels_dir, split=\"ER\")\n",
    "test_loader = PyGDataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Load trained model\n",
    "# ------------------------------------------------------------------\n",
    "model = SAGEEdgeProbModel(in_dim=11, hidden_dim=256, heads=4).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Helper: greedy iterative delete\n",
    "# ------------------------------------------------------------------\n",
    "def greedy_iterative_delete(model, G_orig, K, device):\n",
    "    \"\"\"\n",
    "    Iteratively pick K nodes, re-running the model after each deletion.\n",
    "    Returns a list of ORIGINAL node IDs.\n",
    "    \"\"\"\n",
    "    S = []                          # selected original IDs\n",
    "    G = G_orig.copy()               # work on a local copy\n",
    "\n",
    "    for _ in range(K):\n",
    "        # ---- relabel so nodes are 0…n-1 contiguous ---------------\n",
    "        G_rel = nx.convert_node_labels_to_integers(\n",
    "                    G, label_attribute='orig_id')\n",
    "\n",
    "        # build PyG tensors from the *relabeled* graph\n",
    "        x = extract_node_features(G_rel)                       # [n, 11]\n",
    "        # print(x.shape, \"node features shape\")\n",
    "        edges = list(G_rel.edges())\n",
    "        ei = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        ei = torch.cat([ei, ei.flip(0)], dim=1)                # undirected\n",
    "        p  = torch.tensor([G_rel[u][v]['p'] for u, v in edges] * 2,\n",
    "                          dtype=torch.float)\n",
    "\n",
    "        # ---- run model & pick best node --------------------------\n",
    "        with torch.no_grad():\n",
    "            logits = model(x.to(device), ei.to(device), p.to(device)).cpu()\n",
    "\n",
    "        v_rel  = logits.argmax().item()                        # 0…n-1\n",
    "        v_orig = G_rel.nodes[v_rel]['orig_id']                 # back-map\n",
    "        S.append(v_orig)\n",
    "\n",
    "        # ---- delete from the *original-ID* graph ---------------\n",
    "        G.remove_node(v_orig)\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d6ad5f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy-iter:  40%|████      | 24/60 [00:01<00:01, 19.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_0.pkl | EPC₀ 4888.3 → 3484.1  Δ -1404.2 | time: 0.93s\n",
      "top-K nodes: [75, 44, 53, 17, 25, 57, 64, 12, 50, 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy-iter:  43%|████▎     | 26/60 [00:03<00:10,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_1.pkl | EPC₀ 4817.4 → 3594.4  Δ -1223.1 | time: 0.77s\n",
      "top-K nodes: [89, 38, 30, 7, 42, 80, 70, 22, 98, 76]\n",
      "ER_sz100_sp0.0443_rp0.9_test100_2.pkl | EPC₀ 4800.9 → 3364.2  Δ -1436.6 | time: 0.91s\n",
      "top-K nodes: [82, 90, 44, 68, 28, 59, 93, 48, 19, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy-iter:  47%|████▋     | 28/60 [00:05<00:18,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp1.0_test100_0.pkl | EPC₀ 4750.1 → 3654.9  Δ -1095.3 | time: 1.10s\n",
      "top-K nodes: [63, 57, 23, 49, 54, 5, 33, 24, 14, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy-iter:  48%|████▊     | 29/60 [00:07<00:22,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp1.0_test100_1.pkl | EPC₀ 4950.0 → 3657.3  Δ -1292.7 | time: 1.13s\n",
      "top-K nodes: [17, 29, 26, 8, 55, 2, 13, 20, 92, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy-iter:  53%|█████▎    | 32/60 [00:08<00:15,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp1.0_test100_2.pkl | EPC₀ 4750.5 → 3402.2  Δ -1348.3 | time: 0.98s\n",
      "top-K nodes: [87, 25, 60, 50, 1, 31, 2, 92, 51, 65]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy-iter: 100%|██████████| 60/60 [00:10<00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graphs processed: 6\n",
      "Avg del EPC: +3526.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.  Inference + EPC\n",
    "# ------------------------------------------------------------------\n",
    "import time\n",
    "\n",
    "K           = 10\n",
    "mc_samples  = 100_000\n",
    "all_delta   = []\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"Greedy-iter\"):\n",
    "    batch = batch.to(device)\n",
    "    idx   = batch.idx.item()\n",
    "    fname = batch.file_name[0]\n",
    "\n",
    "    if \"sp0.0443\" in fname and (\"rp0.9\" in fname or \"rp1.0\" in fname):\n",
    "        t0 = time.perf_counter()\n",
    "        G_nx = pickle.load(open(test_ds.graph_paths[idx], 'rb'))['graph']\n",
    "\n",
    "        topk = greedy_iterative_delete(model, G_nx.copy(), K, device)\n",
    "\n",
    "        # opt_k = local_search_(G_nx.copy(), set(topk), mc_samples)\n",
    "\n",
    "        epc_del = epc_mc_deleted(G_nx.copy(), set(topk), mc_samples)\n",
    "        time_greedy_gnn = time.perf_counter() - t0\n",
    "\n",
    "        epc_0   = epc_mc_deleted(G_nx.copy(), set(),      mc_samples)\n",
    "        delta   = epc_del - epc_0      # negative = improvement\n",
    "        all_delta.append(epc_del)\n",
    "\n",
    "        print(f\"{fname} | EPC₀ {epc_0:.1f} → {epc_del:.1f}  Δ {delta:+.1f} | time: {time_greedy_gnn:.2f}s\")\n",
    "        print(\"top-K nodes:\", topk)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Summary\n",
    "# ------------------------------------------------------------------\n",
    "if all_delta:\n",
    "    print(f\"\\nGraphs processed: {len(all_delta)}\")\n",
    "    print(f\"Avg del EPC: {np.mean(all_delta):+.1f}\")\n",
    "else:\n",
    "    print(\"No graphs matched the filename filter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73e91235",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/best_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m in_dim       \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m                     \u001b[38;5;66;03m# you decided to keep only the 11 node features\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model        \u001b[38;5;241m=\u001b[39m SAGEEdgeProbModel(in_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 3.  INFERENCE + EPC\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/Research/SCNDP/lib/python3.10/site-packages/torch/serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Development/Research/SCNDP/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Development/Research/SCNDP/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/best_model.pt'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "graphs_dir   = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/graphs/test_large_separate\"          # same dirs you used for train/val\n",
    "labels_dir   = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/labels/test_large_separate\"          # not needed for inference but Dataset expects it\n",
    "ckpt_path    = \"/home/tuguldurb/Development/Research/SCNDP/src/SCNDP/src/extension/learning/notebooks/gnn/data/best_model.pt\" # saved in train_model()\n",
    "K            = 10                     # number of nodes to delete\n",
    "mc_samples   = 10_000                 # per-graph Monte-Carlo samples for EPC\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  DATASET & DATALOADER  (batch_size = 1 for clarity)\n",
    "# ------------------------------------------------------------\n",
    "test_ds      = GraphEPCDataset(graphs_dir, labels_dir, split=\"ER\")\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  LOAD MODEL\n",
    "# ------------------------------------------------------------\n",
    "in_dim       = 11                     # you decided to keep only the 11 node features\n",
    "model        = SAGEEdgeProbModel(in_dim).to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  INFERENCE + EPC\n",
    "# ------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_epc = []\n",
    "i = 0\n",
    "\n",
    "for data in tqdm(test_loader, desc=\"Inference\"):\n",
    "\n",
    "    # if i == 30:\n",
    "    #     break\n",
    "    # move everything to GPU/CPU\n",
    "    data = data.to(device)\n",
    "\n",
    "    fname = data.file_name[0]\n",
    "    \n",
    "    if \"sp0.0443\" in fname and (\"rp0.9\" in fname or \"rp1.0\" in fname):\n",
    "        # ---- 3.1 node scores ----\n",
    "        print(\"file: \", data.file_name)\n",
    "        with torch.no_grad():\n",
    "            scores = model(data.x, data.edge_index, data.edge_prob)   # [N]\n",
    "        \n",
    "        # print(data.edge_prob)\n",
    "        # ---- 3.2 pick top-K nodes ----\n",
    "        # scores is already on the same device; .cpu() only if epc_mc_deleted needs CPU tensors\n",
    "        topk = scores.topk(K, largest=True).indices.tolist()         # list[int]\n",
    "\n",
    "        # ---- 3.3 compute EPC after deleting top-K ----\n",
    "        #   We need the *NetworkX graph*; fetch it via the original .pkl\n",
    "        #   The path is stored in test_ds.graph_paths[index] where `index`\n",
    "        #   is the position in the dataset.  The DataLoader gives us that\n",
    "        #   index in data.__dict__['idx']  (PyG attaches it automatically).\n",
    "        idx  = data.idx.item()         # scalar tensor → int\n",
    "        G_nx = pickle.load(open(test_ds.graph_paths[idx], 'rb'))['graph']\n",
    "        \n",
    "        epc_0   = epc_mc_deleted(G_nx.copy(), set(), num_samples=mc_samples)  \n",
    "        epc_del = epc_mc_deleted(G_nx.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "        all_epc.append(epc_del)\n",
    "\n",
    "        delta   = epc_del - epc_0     # negative  ⇒ improvement\n",
    "        print(f\"{fname}  EPC₀={epc_0:.1f}  after={epc_del:.1f}  Δ={delta:+.1f}\")\n",
    "\n",
    "        print(f\"Graph {idx:03d} | EPC(after delete) = {epc_del:.4f} | top-K = {topk}\")\n",
    "        \n",
    "        print(\"top-scores id,logit:\")\n",
    "        print(sorted(zip(topk, scores[topk].tolist()), key=lambda x: -x[1])[:5])\n",
    "\n",
    "        # confirm they exist in the NetworkX graph\n",
    "        assert all(v in G_nx for v in topk)\n",
    "        i += 1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  SUMMARY\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "print(f\"\\nAverage EPC over {len(all_epc)} test graphs: {np.mean(all_epc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ae8c295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:   7%|▋         | 4/60 [00:00<00:01, 34.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.1_test100_0.pkl EPC0=39.5800  EPC-del=24.3855  Δ=-15.1945  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_1.pkl EPC0=33.5550  EPC-del=20.0340  Δ=-13.5210  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_2.pkl EPC0=36.4950  EPC-del=21.7890  Δ=-14.7060  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_0.pkl EPC0=206.7300  EPC-del=74.4525  Δ=-132.2775  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_1.pkl EPC0=225.4850  EPC-del=63.9765  Δ=-161.5085  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_2.pkl EPC0=223.3700  EPC-del=71.1315  Δ=-152.2385  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_0.pkl EPC0=1069.6250  EPC-del=201.0375  Δ=-868.5875  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  20%|██        | 12/60 [00:00<00:01, 31.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.3_test100_1.pkl EPC0=1015.0900  EPC-del=188.9190  Δ=-826.1710  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_2.pkl EPC0=868.6600  EPC-del=158.2245  Δ=-710.4355  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_0.pkl EPC0=2556.6050  EPC-del=895.8645  Δ=-1660.7405  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_1.pkl EPC0=2758.2750  EPC-del=723.4830  Δ=-2034.7920  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_2.pkl EPC0=3103.0700  EPC-del=1142.8155  Δ=-1960.2545  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_0.pkl EPC0=3225.4200  EPC-del=1195.1460  Δ=-2030.2740  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_1.pkl EPC0=2999.6550  EPC-del=1153.9350  Δ=-1845.7200  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  33%|███▎      | 20/60 [00:00<00:01, 30.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.5_test100_2.pkl EPC0=3702.0400  EPC-del=2174.4630  Δ=-1527.5770  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_0.pkl EPC0=4439.0500  EPC-del=3007.7190  Δ=-1431.3310  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_1.pkl EPC0=3891.4600  EPC-del=2129.1075  Δ=-1762.3525  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_2.pkl EPC0=3904.3700  EPC-del=2063.5515  Δ=-1840.8185  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_0.pkl EPC0=4354.1150  EPC-del=2876.7780  Δ=-1477.3370  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_1.pkl EPC0=4180.1350  EPC-del=2393.1495  Δ=-1786.9855  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  40%|████      | 24/60 [00:00<00:01, 30.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.7_test100_2.pkl EPC0=4177.4600  EPC-del=2403.7515  Δ=-1773.7085  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_0.pkl EPC0=4775.0400  EPC-del=3512.2545  Δ=-1262.7855  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_1.pkl EPC0=4638.9150  EPC-del=3093.4260  Δ=-1545.4890  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_2.pkl EPC0=4552.2600  EPC-del=2915.7570  Δ=-1636.5030  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_0.pkl EPC0=4895.4800  EPC-del=3300.3405  Δ=-1595.1395  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_1.pkl EPC0=4818.5200  EPC-del=3452.0445  Δ=-1366.4755  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  52%|█████▏    | 31/60 [00:01<00:01, 27.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_2.pkl EPC0=4801.9200  EPC-del=3301.5600  Δ=-1500.3600  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_0.pkl EPC0=4753.0000  EPC-del=3732.6150  Δ=-1020.3850  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3734.1630  Δ=-1215.8370  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_2.pkl EPC0=4762.6400  EPC-del=3469.4370  Δ=-1293.2030  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_0.pkl EPC0=88.1100  EPC-del=48.2355  Δ=-39.8745  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_1.pkl EPC0=90.0400  EPC-del=46.6920  Δ=-43.3480  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  65%|██████▌   | 39/60 [00:01<00:00, 30.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.1_test100_2.pkl EPC0=83.4400  EPC-del=44.3160  Δ=-39.1240  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_0.pkl EPC0=818.1300  EPC-del=235.2825  Δ=-582.8475  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_1.pkl EPC0=891.2250  EPC-del=266.9040  Δ=-624.3210  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_2.pkl EPC0=930.5250  EPC-del=252.3150  Δ=-678.2100  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_0.pkl EPC0=2964.3100  EPC-del=1183.6575  Δ=-1780.6525  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_1.pkl EPC0=2410.0000  EPC-del=663.5295  Δ=-1746.4705  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_2.pkl EPC0=3247.7200  EPC-del=1636.1055  Δ=-1611.6145  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  72%|███████▏  | 43/60 [00:01<00:00, 27.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.4_test100_0.pkl EPC0=4154.5450  EPC-del=2779.1865  Δ=-1375.3585  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_1.pkl EPC0=3980.9750  EPC-del=2569.8780  Δ=-1411.0970  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_2.pkl EPC0=4190.5700  EPC-del=2892.8925  Δ=-1297.6775  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_0.pkl EPC0=4563.9500  EPC-del=3225.7170  Δ=-1338.2330  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_1.pkl EPC0=4516.0250  EPC-del=3048.5700  Δ=-1467.4550  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  82%|████████▏ | 49/60 [00:01<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.5_test100_2.pkl EPC0=4620.4350  EPC-del=3324.2175  Δ=-1296.2175  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_0.pkl EPC0=4694.0150  EPC-del=3240.7605  Δ=-1453.2545  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_1.pkl EPC0=4712.3650  EPC-del=3333.5055  Δ=-1378.8595  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_2.pkl EPC0=4796.9400  EPC-del=3588.8085  Δ=-1208.1315  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_0.pkl EPC0=4847.3450  EPC-del=3584.6640  Δ=-1262.6810  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  87%|████████▋ | 52/60 [00:01<00:00, 24.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.7_test100_1.pkl EPC0=4886.2050  EPC-del=3756.9150  Δ=-1129.2900  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_2.pkl EPC0=4821.6300  EPC-del=3510.3735  Δ=-1311.2565  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_0.pkl EPC0=4897.6350  EPC-del=3646.3185  Δ=-1251.3165  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_1.pkl EPC0=4939.0500  EPC-del=3774.8520  Δ=-1164.1980  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_2.pkl EPC0=4917.1750  EPC-del=3633.2460  Δ=-1283.9290  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  97%|█████████▋| 58/60 [00:02<00:00, 23.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.9_test100_0.pkl EPC0=4927.1100  EPC-del=3651.8580  Δ=-1275.2520  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_1.pkl EPC0=4947.4750  EPC-del=3736.6740  Δ=-1210.8010  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_2.pkl EPC0=4927.9550  EPC-del=3400.4925  Δ=-1527.4625  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=4005.0000  Δ=-945.0000  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3912.4800  Δ=-1037.5200  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity: 100%|██████████| 60/60 [00:02<00:00, 27.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp1.0_test100_2.pkl EPC0=4851.9800  EPC-del=3733.3890  Δ=-1118.5910  K=10\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 0. CONFIGURATION\n",
    "# ----------------------------------------------------------------------\n",
    "graphs_dir  = \"data/graphs/test_100/ER\"\n",
    "labels_dir  = \"data/labels/test_100/ER\"          # same sub-folder structure\n",
    "K           = 10                            # how many nodes to delete\n",
    "mc_samples  = 10_000                        # EPC Monte-Carlo samples\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. HELPER: load graph + label file\n",
    "# ----------------------------------------------------------------------\n",
    "def load_graph_and_scores(pkl_path):\n",
    "    G   = pickle.load(open(pkl_path, \"rb\"))[\"graph\"]\n",
    "\n",
    "    # label file has the same stem plus '_labels.pt'\n",
    "    lbl_path = os.path.join(\n",
    "        labels_dir,\n",
    "        os.path.basename(pkl_path).replace(\".pkl\", \"_labels.pt\")\n",
    "    )\n",
    "    if not os.path.exists(lbl_path):\n",
    "        raise FileNotFoundError(f\"label file not found for {pkl_path}\")\n",
    "\n",
    "    # tensor shape [N], dtype=float\n",
    "    log1_scores = torch.load(lbl_path)\n",
    "    # undo stabilisation: score = exp(label) - 1\n",
    "    scores = log1_scores.exp() - 1.0\n",
    "\n",
    "    return G, scores\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. MAIN LOOP\n",
    "# ----------------------------------------------------------------------\n",
    "for pkl in tqdm(sorted(glob.glob(os.path.join(graphs_dir, \"*.pkl\"))),\n",
    "                desc=\"sanity\"):\n",
    "\n",
    "    G, scores = load_graph_and_scores(pkl)\n",
    "    N         = G.number_of_nodes()\n",
    "\n",
    "    # baseline EPC (no deletions)\n",
    "    epc_0 = epc_mc_deleted(G.copy(), set(), num_samples=mc_samples)\n",
    "\n",
    "    # top-K indices by descending score\n",
    "    topk   = scores.topk(K).indices.tolist()       # list[int]\n",
    "    epc_K  = epc_mc_deleted(G.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "    print(f\"{os.path.basename(pkl):<25}\"\n",
    "          f\" EPC0={epc_0:7.4f}  \"\n",
    "          f\"EPC-del={epc_K:7.4f}  \"\n",
    "          f\"Δ={epc_K-epc_0:+.4f}  \"\n",
    "          f\"K={K}\")\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2eef6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:   2%|▏         | 4/180 [00:00<00:05, 29.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.1_test100_0.pkl EPC0=41.8550  EPC-del=11.1960  Δ=-30.6590  K=10\n",
      "BA_sz100_sp2_rp0.1_test100_1.pkl EPC0=43.1600  EPC-del= 9.0720  Δ=-34.0880  K=10\n",
      "BA_sz100_sp2_rp0.1_test100_2.pkl EPC0=44.3950  EPC-del= 9.5535  Δ=-34.8415  K=10\n",
      "BA_sz100_sp2_rp0.2_test100_0.pkl EPC0=239.7600  EPC-del=25.2045  Δ=-214.5555  K=10\n",
      "BA_sz100_sp2_rp0.2_test100_1.pkl EPC0=234.3200  EPC-del=25.8660  Δ=-208.4540  K=10\n",
      "BA_sz100_sp2_rp0.2_test100_2.pkl EPC0=258.6100  EPC-del=22.4010  Δ=-236.2090  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:   6%|▌         | 11/180 [00:00<00:05, 30.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.3_test100_0.pkl EPC0=910.2500  EPC-del=50.0760  Δ=-860.1740  K=10\n",
      "BA_sz100_sp2_rp0.3_test100_1.pkl EPC0=903.3950  EPC-del=51.4845  Δ=-851.9105  K=10\n",
      "BA_sz100_sp2_rp0.3_test100_2.pkl EPC0=850.8100  EPC-del=43.2450  Δ=-807.5650  K=10\n",
      "BA_sz100_sp2_rp0.4_test100_0.pkl EPC0=1990.8750  EPC-del=80.4060  Δ=-1910.4690  K=10\n",
      "BA_sz100_sp2_rp0.4_test100_1.pkl EPC0=1997.6100  EPC-del=88.6815  Δ=-1908.9285  K=10\n",
      "BA_sz100_sp2_rp0.4_test100_2.pkl EPC0=1976.9650  EPC-del=93.0645  Δ=-1883.9005  K=10\n",
      "BA_sz100_sp2_rp0.5_test100_0.pkl EPC0=3005.4250  EPC-del=108.9630  Δ=-2896.4620  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  11%|█         | 19/180 [00:00<00:05, 30.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.5_test100_1.pkl EPC0=3015.4850  EPC-del=145.9215  Δ=-2869.5635  K=10\n",
      "BA_sz100_sp2_rp0.5_test100_2.pkl EPC0=3028.8750  EPC-del=122.8635  Δ=-2906.0115  K=10\n",
      "BA_sz100_sp2_rp0.6_test100_0.pkl EPC0=3820.7100  EPC-del=229.7610  Δ=-3590.9490  K=10\n",
      "BA_sz100_sp2_rp0.6_test100_1.pkl EPC0=3818.7250  EPC-del=248.7465  Δ=-3569.9785  K=10\n",
      "BA_sz100_sp2_rp0.6_test100_2.pkl EPC0=3802.4250  EPC-del=184.5495  Δ=-3617.8755  K=10\n",
      "BA_sz100_sp2_rp0.7_test100_0.pkl EPC0=4370.7200  EPC-del=680.3955  Δ=-3690.3245  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  13%|█▎        | 23/180 [00:00<00:05, 29.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.7_test100_1.pkl EPC0=4398.4300  EPC-del=1466.0910  Δ=-2932.3390  K=10\n",
      "BA_sz100_sp2_rp0.7_test100_2.pkl EPC0=4353.4550  EPC-del=683.2125  Δ=-3670.2425  K=10\n",
      "BA_sz100_sp2_rp0.8_test100_0.pkl EPC0=4696.5000  EPC-del=1546.9785  Δ=-3149.5215  K=10\n",
      "BA_sz100_sp2_rp0.8_test100_1.pkl EPC0=4694.9250  EPC-del=1495.2645  Δ=-3199.6605  K=10\n",
      "BA_sz100_sp2_rp0.8_test100_2.pkl EPC0=4714.7400  EPC-del=1611.8865  Δ=-3102.8535  K=10\n",
      "BA_sz100_sp2_rp0.9_test100_0.pkl EPC0=4898.1800  EPC-del=2531.8755  Δ=-2366.3045  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  16%|█▌        | 29/180 [00:00<00:05, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp2_rp0.9_test100_1.pkl EPC0=4903.9800  EPC-del=2504.0475  Δ=-2399.9325  K=10\n",
      "BA_sz100_sp2_rp0.9_test100_2.pkl EPC0=4893.9400  EPC-del=1408.5000  Δ=-3485.4400  K=10\n",
      "BA_sz100_sp2_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3171.7575  Δ=-1778.2425  K=10\n",
      "BA_sz100_sp2_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=2505.2625  Δ=-2444.7375  K=10\n",
      "BA_sz100_sp2_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=3249.1080  Δ=-1700.8920  K=10\n",
      "BA_sz100_sp3_rp0.1_test100_0.pkl EPC0=88.9650  EPC-del=23.3820  Δ=-65.5830  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  21%|██        | 37/180 [00:01<00:04, 30.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.1_test100_1.pkl EPC0=89.0400  EPC-del=22.6260  Δ=-66.4140  K=10\n",
      "BA_sz100_sp3_rp0.1_test100_2.pkl EPC0=94.2300  EPC-del=19.9755  Δ=-74.2545  K=10\n",
      "BA_sz100_sp3_rp0.2_test100_0.pkl EPC0=919.5350  EPC-del=53.2800  Δ=-866.2550  K=10\n",
      "BA_sz100_sp3_rp0.2_test100_1.pkl EPC0=893.5150  EPC-del=60.4350  Δ=-833.0800  K=10\n",
      "BA_sz100_sp3_rp0.2_test100_2.pkl EPC0=869.3700  EPC-del=64.4040  Δ=-804.9660  K=10\n",
      "BA_sz100_sp3_rp0.3_test100_0.pkl EPC0=2401.8550  EPC-del=194.1345  Δ=-2207.7205  K=10\n",
      "BA_sz100_sp3_rp0.3_test100_1.pkl EPC0=2405.4500  EPC-del=232.5510  Δ=-2172.8990  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  23%|██▎       | 41/180 [00:01<00:04, 30.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.3_test100_2.pkl EPC0=2420.4850  EPC-del=173.4030  Δ=-2247.0820  K=10\n",
      "BA_sz100_sp3_rp0.4_test100_0.pkl EPC0=3490.8950  EPC-del=456.2820  Δ=-3034.6130  K=10\n",
      "BA_sz100_sp3_rp0.4_test100_1.pkl EPC0=3478.3650  EPC-del=273.6270  Δ=-3204.7380  K=10\n",
      "BA_sz100_sp3_rp0.4_test100_2.pkl EPC0=3510.5500  EPC-del=469.7775  Δ=-3040.7725  K=10\n",
      "BA_sz100_sp3_rp0.5_test100_0.pkl EPC0=4264.9650  EPC-del=1898.0460  Δ=-2366.9190  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  27%|██▋       | 48/180 [00:01<00:05, 25.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.5_test100_1.pkl EPC0=4149.9900  EPC-del=1230.7095  Δ=-2919.2805  K=10\n",
      "BA_sz100_sp3_rp0.5_test100_2.pkl EPC0=4203.4050  EPC-del=1906.3125  Δ=-2297.0925  K=10\n",
      "BA_sz100_sp3_rp0.6_test100_0.pkl EPC0=4603.6400  EPC-del=2325.6135  Δ=-2278.0265  K=10\n",
      "BA_sz100_sp3_rp0.6_test100_1.pkl EPC0=4610.2500  EPC-del=2314.5750  Δ=-2295.6750  K=10\n",
      "BA_sz100_sp3_rp0.6_test100_2.pkl EPC0=4602.8650  EPC-del=2089.3905  Δ=-2513.4745  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  28%|██▊       | 51/180 [00:01<00:05, 24.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.7_test100_0.pkl EPC0=4800.2600  EPC-del=2266.1820  Δ=-2534.0780  K=10\n",
      "BA_sz100_sp3_rp0.7_test100_1.pkl EPC0=4823.6150  EPC-del=2728.3365  Δ=-2095.2785  K=10\n",
      "BA_sz100_sp3_rp0.7_test100_2.pkl EPC0=4822.1050  EPC-del=2967.0345  Δ=-1855.0705  K=10\n",
      "BA_sz100_sp3_rp0.8_test100_0.pkl EPC0=4922.7400  EPC-del=3563.7795  Δ=-1358.9605  K=10\n",
      "BA_sz100_sp3_rp0.8_test100_1.pkl EPC0=4919.5100  EPC-del=3490.9200  Δ=-1428.5900  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  32%|███▏      | 57/180 [00:02<00:05, 23.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp0.8_test100_2.pkl EPC0=4910.2600  EPC-del=3212.0730  Δ=-1698.1870  K=10\n",
      "BA_sz100_sp3_rp0.9_test100_0.pkl EPC0=4945.2300  EPC-del=3642.4620  Δ=-1302.7680  K=10\n",
      "BA_sz100_sp3_rp0.9_test100_1.pkl EPC0=4947.8500  EPC-del=3625.4205  Δ=-1322.4295  K=10\n",
      "BA_sz100_sp3_rp0.9_test100_2.pkl EPC0=4946.0000  EPC-del=3827.7990  Δ=-1118.2010  K=10\n",
      "BA_sz100_sp3_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3655.1700  Δ=-1294.8300  K=10\n",
      "BA_sz100_sp3_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3915.2520  Δ=-1034.7480  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  36%|███▌      | 64/180 [00:02<00:04, 27.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_sz100_sp3_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=3913.6680  Δ=-1036.3320  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_0.pkl EPC0=38.7300  EPC-del=22.7655  Δ=-15.9645  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_1.pkl EPC0=33.0550  EPC-del=17.7300  Δ=-15.3250  K=10\n",
      "ER_sz100_sp0.0443_rp0.1_test100_2.pkl EPC0=39.4600  EPC-del=21.5370  Δ=-17.9230  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_0.pkl EPC0=209.3800  EPC-del=74.4075  Δ=-134.9725  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_1.pkl EPC0=228.3750  EPC-del=63.6390  Δ=-164.7360  K=10\n",
      "ER_sz100_sp0.0443_rp0.2_test100_2.pkl EPC0=224.0950  EPC-del=64.6560  Δ=-159.4390  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  39%|███▉      | 71/180 [00:02<00:03, 29.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.3_test100_0.pkl EPC0=1066.3500  EPC-del=179.2890  Δ=-887.0610  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_1.pkl EPC0=1005.3950  EPC-del=179.4060  Δ=-825.9890  K=10\n",
      "ER_sz100_sp0.0443_rp0.3_test100_2.pkl EPC0=856.9550  EPC-del=127.2285  Δ=-729.7265  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_0.pkl EPC0=2533.8300  EPC-del=739.2600  Δ=-1794.5700  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_1.pkl EPC0=2755.4450  EPC-del=679.9275  Δ=-2075.5175  K=10\n",
      "ER_sz100_sp0.0443_rp0.4_test100_2.pkl EPC0=3100.5150  EPC-del=1009.6785  Δ=-2090.8365  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_0.pkl EPC0=3204.9650  EPC-del=1100.4210  Δ=-2104.5440  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  43%|████▎     | 77/180 [00:02<00:03, 27.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.5_test100_1.pkl EPC0=3009.8250  EPC-del=1307.9925  Δ=-1701.8325  K=10\n",
      "ER_sz100_sp0.0443_rp0.5_test100_2.pkl EPC0=3671.3400  EPC-del=2041.2810  Δ=-1630.0590  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_0.pkl EPC0=4447.3850  EPC-del=2953.8585  Δ=-1493.5265  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_1.pkl EPC0=3909.8950  EPC-del=2087.3385  Δ=-1822.5565  K=10\n",
      "ER_sz100_sp0.0443_rp0.6_test100_2.pkl EPC0=3910.6800  EPC-del=2000.3400  Δ=-1910.3400  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_0.pkl EPC0=4367.3950  EPC-del=2840.5440  Δ=-1526.8510  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  46%|████▌     | 83/180 [00:02<00:03, 28.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.7_test100_1.pkl EPC0=4195.6550  EPC-del=2311.7760  Δ=-1883.8790  K=10\n",
      "ER_sz100_sp0.0443_rp0.7_test100_2.pkl EPC0=4169.9500  EPC-del=2254.0500  Δ=-1915.9000  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_0.pkl EPC0=4782.9600  EPC-del=3399.1425  Δ=-1383.8175  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_1.pkl EPC0=4628.1800  EPC-del=2920.5990  Δ=-1707.5810  K=10\n",
      "ER_sz100_sp0.0443_rp0.8_test100_2.pkl EPC0=4542.8750  EPC-del=2866.5045  Δ=-1676.3705  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_0.pkl EPC0=4888.3700  EPC-del=3369.7215  Δ=-1518.6485  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  50%|█████     | 90/180 [00:03<00:02, 30.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0443_rp0.9_test100_1.pkl EPC0=4819.0900  EPC-del=3283.7310  Δ=-1535.3590  K=10\n",
      "ER_sz100_sp0.0443_rp0.9_test100_2.pkl EPC0=4804.1200  EPC-del=3293.9730  Δ=-1510.1470  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_0.pkl EPC0=4755.4250  EPC-del=3733.7760  Δ=-1021.6490  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3488.8635  Δ=-1461.1365  K=10\n",
      "ER_sz100_sp0.0443_rp1.0_test100_2.pkl EPC0=4755.4400  EPC-del=3410.5500  Δ=-1344.8900  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_0.pkl EPC0=89.1650  EPC-del=45.6120  Δ=-43.5530  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_1.pkl EPC0=91.3100  EPC-del=46.9125  Δ=-44.3975  K=10\n",
      "ER_sz100_sp0.0667_rp0.1_test100_2.pkl EPC0=83.0400  EPC-del=44.0055  Δ=-39.0345  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_0.pkl EPC0=832.0000  EPC-del=205.7940  Δ=-626.2060  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_1.pkl EPC0=889.3400  EPC-del=225.9765  Δ=-663.3635  K=10\n",
      "ER_sz100_sp0.0667_rp0.2_test100_2.pkl EPC0=936.7200  EPC-del=218.9385  Δ=-717.7815  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_0.pkl EPC0=2993.6100  EPC-del=1090.9485  Δ=-1902.6615  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_1.pkl EPC0=2385.6900  EPC-del=658.2465  Δ=-1727.4435  K=10\n",
      "ER_sz100_sp0.0667_rp0.3_test100_2.pkl EPC0=3274.2150  EPC-del=1442.3850  Δ=-1831.8300  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  56%|█████▌    | 100/180 [00:03<00:01, 47.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.4_test100_0.pkl EPC0=4142.0700  EPC-del=2685.8475  Δ=-1456.2225  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_1.pkl EPC0=4003.8850  EPC-del=2480.3820  Δ=-1523.5030  K=10\n",
      "ER_sz100_sp0.0667_rp0.4_test100_2.pkl EPC0=4198.6400  EPC-del=2922.2100  Δ=-1276.4300  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_0.pkl EPC0=4584.0050  EPC-del=3154.1220  Δ=-1429.8830  K=10\n",
      "ER_sz100_sp0.0667_rp0.5_test100_1.pkl EPC0=4503.3600  EPC-del=3066.7500  Δ=-1436.6100  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  61%|██████    | 109/180 [00:03<00:02, 30.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.5_test100_2.pkl EPC0=4600.9850  EPC-del=3309.0300  Δ=-1291.9550  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_0.pkl EPC0=4695.0700  EPC-del=3186.5715  Δ=-1508.4985  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_1.pkl EPC0=4705.5250  EPC-del=3301.4295  Δ=-1404.0955  K=10\n",
      "ER_sz100_sp0.0667_rp0.6_test100_2.pkl EPC0=4792.6750  EPC-del=3563.4510  Δ=-1229.2240  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_0.pkl EPC0=4845.8600  EPC-del=3565.8225  Δ=-1280.0375  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  63%|██████▎   | 113/180 [00:03<00:02, 27.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.7_test100_1.pkl EPC0=4887.1950  EPC-del=3747.8790  Δ=-1139.3160  K=10\n",
      "ER_sz100_sp0.0667_rp0.7_test100_2.pkl EPC0=4831.0750  EPC-del=3503.4795  Δ=-1327.5955  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_0.pkl EPC0=4899.1900  EPC-del=3559.2525  Δ=-1339.9375  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_1.pkl EPC0=4936.2650  EPC-del=3644.0865  Δ=-1292.1785  K=10\n",
      "ER_sz100_sp0.0667_rp0.8_test100_2.pkl EPC0=4914.6650  EPC-del=3619.3905  Δ=-1295.2745  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  65%|██████▌   | 117/180 [00:04<00:02, 26.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp0.9_test100_0.pkl EPC0=4922.5600  EPC-del=3575.2590  Δ=-1347.3010  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_1.pkl EPC0=4946.0250  EPC-del=3735.0045  Δ=-1211.0205  K=10\n",
      "ER_sz100_sp0.0667_rp0.9_test100_2.pkl EPC0=4922.9600  EPC-del=3394.9125  Δ=-1528.0475  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3913.6680  Δ=-1036.3320  K=10\n",
      "ER_sz100_sp0.0667_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3921.1920  Δ=-1028.8080  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  69%|██████▉   | 124/180 [00:04<00:02, 27.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER_sz100_sp0.0667_rp1.0_test100_2.pkl EPC0=4857.8600  EPC-del=3585.3300  Δ=-1272.5300  K=10\n",
      "SW_sz100_sp4_rp0.1_test100_0.pkl EPC0=30.2100  EPC-del=22.4685  Δ=-7.7415  K=10\n",
      "SW_sz100_sp4_rp0.1_test100_1.pkl EPC0=29.0000  EPC-del=21.6720  Δ=-7.3280  K=10\n",
      "SW_sz100_sp4_rp0.1_test100_2.pkl EPC0=29.7550  EPC-del=19.9170  Δ=-9.8380  K=10\n",
      "SW_sz100_sp4_rp0.2_test100_0.pkl EPC0=116.6100  EPC-del=55.9080  Δ=-60.7020  K=10\n",
      "SW_sz100_sp4_rp0.2_test100_1.pkl EPC0=119.6300  EPC-del=52.3845  Δ=-67.2455  K=10\n",
      "SW_sz100_sp4_rp0.2_test100_2.pkl EPC0=114.8000  EPC-del=55.6200  Δ=-59.1800  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  73%|███████▎  | 132/180 [00:04<00:01, 30.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp4_rp0.3_test100_0.pkl EPC0=473.6600  EPC-del=143.6220  Δ=-330.0380  K=10\n",
      "SW_sz100_sp4_rp0.3_test100_1.pkl EPC0=489.8150  EPC-del=134.3610  Δ=-355.4540  K=10\n",
      "SW_sz100_sp4_rp0.3_test100_2.pkl EPC0=467.6600  EPC-del=137.7045  Δ=-329.9555  K=10\n",
      "SW_sz100_sp4_rp0.4_test100_0.pkl EPC0=1917.9950  EPC-del=356.9850  Δ=-1561.0100  K=10\n",
      "SW_sz100_sp4_rp0.4_test100_1.pkl EPC0=1815.4300  EPC-del=361.6965  Δ=-1453.7335  K=10\n",
      "SW_sz100_sp4_rp0.4_test100_2.pkl EPC0=1882.7350  EPC-del=380.4390  Δ=-1502.2960  K=10\n",
      "SW_sz100_sp4_rp0.5_test100_0.pkl EPC0=3415.7650  EPC-del=938.7810  Δ=-2476.9840  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  76%|███████▌  | 136/180 [00:04<00:01, 29.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp4_rp0.5_test100_1.pkl EPC0=3413.2650  EPC-del=1256.5170  Δ=-2156.7480  K=10\n",
      "SW_sz100_sp4_rp0.5_test100_2.pkl EPC0=3437.0950  EPC-del=1176.2010  Δ=-2260.8940  K=10\n",
      "SW_sz100_sp4_rp0.6_test100_0.pkl EPC0=4294.5350  EPC-del=2201.7870  Δ=-2092.7480  K=10\n",
      "SW_sz100_sp4_rp0.6_test100_1.pkl EPC0=4262.8300  EPC-del=2386.2465  Δ=-1876.5835  K=10\n",
      "SW_sz100_sp4_rp0.6_test100_2.pkl EPC0=4216.4950  EPC-del=2210.4540  Δ=-2006.0410  K=10\n",
      "SW_sz100_sp4_rp0.7_test100_0.pkl EPC0=4695.3500  EPC-del=3151.3140  Δ=-1544.0360  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  79%|███████▉  | 143/180 [00:04<00:01, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp4_rp0.7_test100_1.pkl EPC0=4669.3650  EPC-del=3087.7290  Δ=-1581.6360  K=10\n",
      "SW_sz100_sp4_rp0.7_test100_2.pkl EPC0=4733.5000  EPC-del=3114.8640  Δ=-1618.6360  K=10\n",
      "SW_sz100_sp4_rp0.8_test100_0.pkl EPC0=4865.0950  EPC-del=3405.0645  Δ=-1460.0305  K=10\n",
      "SW_sz100_sp4_rp0.8_test100_1.pkl EPC0=4862.4350  EPC-del=3262.2075  Δ=-1600.2275  K=10\n",
      "SW_sz100_sp4_rp0.8_test100_2.pkl EPC0=4847.5100  EPC-del=3317.8950  Δ=-1529.6150  K=10\n",
      "SW_sz100_sp4_rp0.9_test100_0.pkl EPC0=4931.6050  EPC-del=3559.1760  Δ=-1372.4290  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  83%|████████▎ | 150/180 [00:05<00:01, 28.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp4_rp0.9_test100_1.pkl EPC0=4929.6450  EPC-del=3515.4675  Δ=-1414.1775  K=10\n",
      "SW_sz100_sp4_rp0.9_test100_2.pkl EPC0=4929.9100  EPC-del=3463.7985  Δ=-1466.1115  K=10\n",
      "SW_sz100_sp4_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3917.6280  Δ=-1032.3720  K=10\n",
      "SW_sz100_sp4_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3917.2320  Δ=-1032.7680  K=10\n",
      "SW_sz100_sp4_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=3915.2520  Δ=-1034.7480  K=10\n",
      "SW_sz100_sp5_rp0.1_test100_0.pkl EPC0=30.6250  EPC-del=21.0555  Δ=-9.5695  K=10\n",
      "SW_sz100_sp5_rp0.1_test100_1.pkl EPC0=30.4850  EPC-del=22.2120  Δ=-8.2730  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  88%|████████▊ | 158/180 [00:05<00:00, 31.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp5_rp0.1_test100_2.pkl EPC0=30.4850  EPC-del=22.3695  Δ=-8.1155  K=10\n",
      "SW_sz100_sp5_rp0.2_test100_0.pkl EPC0=114.1700  EPC-del=54.5760  Δ=-59.5940  K=10\n",
      "SW_sz100_sp5_rp0.2_test100_1.pkl EPC0=117.9000  EPC-del=53.6535  Δ=-64.2465  K=10\n",
      "SW_sz100_sp5_rp0.2_test100_2.pkl EPC0=119.8300  EPC-del=58.4505  Δ=-61.3795  K=10\n",
      "SW_sz100_sp5_rp0.3_test100_0.pkl EPC0=487.7050  EPC-del=145.8495  Δ=-341.8555  K=10\n",
      "SW_sz100_sp5_rp0.3_test100_1.pkl EPC0=509.3800  EPC-del=132.5430  Δ=-376.8370  K=10\n",
      "SW_sz100_sp5_rp0.3_test100_2.pkl EPC0=487.9750  EPC-del=136.4085  Δ=-351.5665  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  92%|█████████▏| 166/180 [00:05<00:00, 30.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp5_rp0.4_test100_0.pkl EPC0=1851.8050  EPC-del=318.0870  Δ=-1533.7180  K=10\n",
      "SW_sz100_sp5_rp0.4_test100_1.pkl EPC0=1809.4200  EPC-del=400.9095  Δ=-1408.5105  K=10\n",
      "SW_sz100_sp5_rp0.4_test100_2.pkl EPC0=1871.9850  EPC-del=374.0760  Δ=-1497.9090  K=10\n",
      "SW_sz100_sp5_rp0.5_test100_0.pkl EPC0=3415.1750  EPC-del=952.6320  Δ=-2462.5430  K=10\n",
      "SW_sz100_sp5_rp0.5_test100_1.pkl EPC0=3356.0850  EPC-del=880.0740  Δ=-2476.0110  K=10\n",
      "SW_sz100_sp5_rp0.5_test100_2.pkl EPC0=3486.3200  EPC-del=1193.0895  Δ=-2293.2305  K=10\n",
      "SW_sz100_sp5_rp0.6_test100_0.pkl EPC0=4255.8950  EPC-del=2374.2945  Δ=-1881.6005  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  94%|█████████▍| 170/180 [00:05<00:00, 29.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp5_rp0.6_test100_1.pkl EPC0=4159.8150  EPC-del=2230.6905  Δ=-1929.1245  K=10\n",
      "SW_sz100_sp5_rp0.6_test100_2.pkl EPC0=4250.8150  EPC-del=2278.0530  Δ=-1972.7620  K=10\n",
      "SW_sz100_sp5_rp0.7_test100_0.pkl EPC0=4659.0350  EPC-del=2874.0510  Δ=-1784.9840  K=10\n",
      "SW_sz100_sp5_rp0.7_test100_1.pkl EPC0=4698.6900  EPC-del=3193.1505  Δ=-1505.5395  K=10\n",
      "SW_sz100_sp5_rp0.7_test100_2.pkl EPC0=4645.7200  EPC-del=3035.2590  Δ=-1610.4610  K=10\n",
      "SW_sz100_sp5_rp0.8_test100_0.pkl EPC0=4852.9750  EPC-del=3348.0315  Δ=-1504.9435  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity:  98%|█████████▊| 177/180 [00:06<00:00, 28.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp5_rp0.8_test100_1.pkl EPC0=4841.6550  EPC-del=3243.8880  Δ=-1597.7670  K=10\n",
      "SW_sz100_sp5_rp0.8_test100_2.pkl EPC0=4851.8250  EPC-del=3440.3085  Δ=-1411.5165  K=10\n",
      "SW_sz100_sp5_rp0.9_test100_0.pkl EPC0=4936.8650  EPC-del=3441.6585  Δ=-1495.2065  K=10\n",
      "SW_sz100_sp5_rp0.9_test100_1.pkl EPC0=4942.9050  EPC-del=3628.2825  Δ=-1314.6225  K=10\n",
      "SW_sz100_sp5_rp0.9_test100_2.pkl EPC0=4933.0800  EPC-del=3507.8085  Δ=-1425.2715  K=10\n",
      "SW_sz100_sp5_rp1.0_test100_0.pkl EPC0=4950.0000  EPC-del=3826.1295  Δ=-1123.8705  K=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sanity: 100%|██████████| 180/180 [00:06<00:00, 28.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_sz100_sp5_rp1.0_test100_1.pkl EPC0=4950.0000  EPC-del=3915.6480  Δ=-1034.3520  K=10\n",
      "SW_sz100_sp5_rp1.0_test100_2.pkl EPC0=4950.0000  EPC-del=3835.9170  Δ=-1114.0830  K=10\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 0. CONFIGURATION\n",
    "# ----------------------------------------------------------------------\n",
    "graphs_dir  = \"data/graphs/test_100\"\n",
    "labels_dir  = \"data/graphs_labels/test_100\"          # same sub-folder structure\n",
    "K           = 10                            # how many nodes to delete\n",
    "mc_samples  = 10_000                        # EPC Monte-Carlo samples\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. HELPER: load graph + label file\n",
    "# ----------------------------------------------------------------------\n",
    "def load_graph_and_scores(pkl_path):\n",
    "    G   = pickle.load(open(pkl_path, \"rb\"))[\"graph\"]\n",
    "\n",
    "    # label file has the same stem plus '_labels.pt'\n",
    "    lbl_path = os.path.join(\n",
    "        labels_dir,\n",
    "        os.path.basename(pkl_path).replace(\".pkl\", \"_labels.pt\")\n",
    "    )\n",
    "    if not os.path.exists(lbl_path):\n",
    "        raise FileNotFoundError(f\"label file not found for {pkl_path}\")\n",
    "\n",
    "    # tensor shape [N], dtype=float\n",
    "    scores = torch.load(lbl_path)\n",
    "    # undo stabilisation: score = exp(label) - 1\n",
    "    # scores = log1_scores.exp() - 1.0\n",
    "\n",
    "    return G, scores\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. MAIN LOOP\n",
    "# ----------------------------------------------------------------------\n",
    "for pkl in tqdm(sorted(glob.glob(os.path.join(graphs_dir, \"*.pkl\"))),\n",
    "                desc=\"sanity\"):\n",
    "\n",
    "    G, scores = load_graph_and_scores(pkl)\n",
    "    N         = G.number_of_nodes()\n",
    "\n",
    "    # baseline EPC (no deletions)\n",
    "    epc_0 = epc_mc_deleted(G.copy(), set(), num_samples=mc_samples)\n",
    "\n",
    "    # top-K indices by descending score\n",
    "    # print(scores)\n",
    "    topk   = scores.topk(K).indices.tolist()       # list[int]\n",
    "    epc_K  = epc_mc_deleted(G.copy(), set(topk), num_samples=mc_samples)\n",
    "\n",
    "    print(f\"{os.path.basename(pkl):<25}\"\n",
    "          f\" EPC0={epc_0:7.4f}  \"\n",
    "          f\"EPC-del={epc_K:7.4f}  \"\n",
    "          f\"Δ={epc_K-epc_0:+.4f}  \"\n",
    "          f\"K={K}\")\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd992e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
