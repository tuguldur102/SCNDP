{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Library ===\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import heapq\n",
    "import itertools\n",
    "from collections import defaultdict, deque\n",
    "from itertools import combinations\n",
    "from typing import Any, Tuple, Dict, List, Set, Sequence, Union, Callable\n",
    "\n",
    "# === Third-Party Libraries ===\n",
    "\n",
    "# --- Scientific Computing ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# --- Plotting ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parallel Processing ---\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Graph Processing ---\n",
    "import networkx as nx\n",
    "\n",
    "# --- JIT Compilation ---\n",
    "from numba import njit, prange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffde812",
   "metadata": {},
   "source": [
    "# 1. EPC (parallelized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0630a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_to_csr(G: nx.Graph) -> Tuple[List[int], Dict[int, int], np.ndarray, np.ndarray, np.ndarray]:\n",
    "     \"\"\"Convert an undirected NetworkX graph (edge attr `'p'`) to CSR arrays.\"\"\"\n",
    "     nodes: List[int] = list(G.nodes())\n",
    "     idx_of: Dict[int, int] = {u: i for i, u in enumerate(nodes)}\n",
    "\n",
    "     indptr: List[int] = [0]\n",
    "     indices: List[int] = []\n",
    "     probs: List[float] = []\n",
    "\n",
    "     for u in nodes:\n",
    "         for v in G.neighbors(u):\n",
    "             indices.append(idx_of[v])\n",
    "             probs.append(G.edges[u, v]['p'])\n",
    "         indptr.append(len(indices))\n",
    "\n",
    "     return (\n",
    "         nodes,\n",
    "         idx_of,\n",
    "         np.asarray(indptr, dtype=np.int32),\n",
    "         np.asarray(indices, dtype=np.int32),\n",
    "         np.asarray(probs, dtype=np.float32),\n",
    "     )\n",
    "\n",
    "@njit(inline=\"always\")\n",
    "def _bfs_component_size(start: int,\n",
    "                    indptr: np.ndarray,\n",
    "                    indices: np.ndarray,\n",
    "                    probs: np.ndarray,\n",
    "                    deleted: np.ndarray) -> int:\n",
    "    \"\"\"Return |C_u|−1 for **one** random realisation (stack BFS).\"\"\"\n",
    "    n = deleted.size\n",
    "    stack = np.empty(n, dtype=np.int32)\n",
    "    visited = np.zeros(n, dtype=np.uint8)\n",
    "\n",
    "    size = 1\n",
    "    top = 0\n",
    "    stack[top] = start\n",
    "    top += 1\n",
    "    visited[start] = 1\n",
    "\n",
    "    while top:\n",
    "        top -= 1\n",
    "        v = stack[top]\n",
    "        for eid in range(indptr[v], indptr[v + 1]):\n",
    "            w = indices[eid]\n",
    "            if deleted[w]:\n",
    "                continue\n",
    "            if np.random.random() >= probs[eid]:\n",
    "                continue\n",
    "            if visited[w]:\n",
    "                continue\n",
    "            visited[w] = 1\n",
    "            stack[top] = w\n",
    "            top += 1\n",
    "            size += 1\n",
    "    return size - 1\n",
    "\n",
    "@njit(parallel=True)\n",
    "def epc_mc(indptr: np.ndarray,\n",
    "            indices: np.ndarray,\n",
    "            probs: np.ndarray,\n",
    "            deleted: np.ndarray,\n",
    "            num_samples: int) -> float:\n",
    "    \"\"\"Monte‑Carlo estimator of **expected pairwise connectivity** (EPC).\"\"\"\n",
    "    surv = np.where(~deleted)[0]\n",
    "    m = surv.size\n",
    "    if m < 2:\n",
    "        return 0.0\n",
    "\n",
    "    acc = 0.0\n",
    "    for _ in prange(num_samples):\n",
    "        u = surv[np.random.randint(m)]\n",
    "        acc += _bfs_component_size(u, indptr, indices, probs, deleted)\n",
    "\n",
    "    return (m * acc) / (2.0 * num_samples)\n",
    "\n",
    "def epc_mc_deleted(\n",
    "  G: nx.Graph,\n",
    "  S: set,\n",
    "  num_samples: int = 100_000,\n",
    ") -> float:\n",
    "  # build csr once\n",
    "  nodes, idx_of, indptr, indices, probs = nx_to_csr(G)\n",
    "  n = len(nodes)\n",
    "\n",
    "  # turn python set S into a mask (node-IDs to delete)\n",
    "  deleted = np.zeros(n, dtype=np.bool_)\n",
    "  for u in S:\n",
    "    deleted[idx_of[u]] = True\n",
    "\n",
    "  epc = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "\n",
    "  return epc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263d1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search_swap(\n",
    "  # G: nx.Graph,\n",
    "  S: Set[int],\n",
    "  *,\n",
    "  csr: Tuple[List[int], Dict[int,int], np.ndarray, np.ndarray, np.ndarray],\n",
    "  num_samples: int = 100_000,\n",
    "  max_iter: int = 5\n",
    ") -> Set[int]:\n",
    "  \"\"\"\n",
    "  Given initial delete-set S, try 1-for-1 swaps to reduce EPC.\n",
    "  csr = (nodes, idx_of, indptr, indices, probs).\n",
    "  \"\"\"\n",
    "  nodes, idx_of, indptr, indices, probs = csr\n",
    "  n = len(nodes)\n",
    "\n",
    "  # build boolean mask from S\n",
    "  deleted = np.zeros(n, dtype=np.bool_)\n",
    "  for u in S:\n",
    "    deleted[idx_of[u]] = True\n",
    "\n",
    "  # current EPC\n",
    "  curr = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "\n",
    "  for it in range(max_iter):\n",
    "    best_delta = 0.0\n",
    "    best_swap = None\n",
    "\n",
    "    # try swapping each i in S with each j not in S\n",
    "    for i in list(S):\n",
    "      ii = idx_of[i]\n",
    "      # undelete i\n",
    "      deleted[ii] = False\n",
    "\n",
    "      for j in nodes:\n",
    "        jj = idx_of[j]\n",
    "        if deleted[jj]:\n",
    "          continue\n",
    "        # delete j\n",
    "        deleted[jj] = True\n",
    "\n",
    "        sigma = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "        delta = curr - sigma\n",
    "        if delta > best_delta:\n",
    "          best_delta = delta\n",
    "          best_swap = (ii, jj, sigma)\n",
    "\n",
    "        # revert j\n",
    "        deleted[jj] = False\n",
    "\n",
    "      # revert i\n",
    "\n",
    "      deleted[ii] = True\n",
    "\n",
    "    if best_swap is None:\n",
    "      break   \n",
    "\n",
    "    # commit the best swap\n",
    "    ii, jj, new_sigma = best_swap\n",
    "    deleted[ii] = False\n",
    "    deleted[jj] = True\n",
    "    curr = new_sigma\n",
    "\n",
    "    # update S\n",
    "    S.remove(nodes[ii])\n",
    "    S.add(nodes[jj])\n",
    "\n",
    "  return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bff75a",
   "metadata": {},
   "source": [
    "# 2. LNS + 1-swap LS + tiny tabu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861beb6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# component_sampling_epc_mc(G, S, num_samples) → float\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# local_search_swap(S, csr, num_samples, max_iter) → Set[int]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# build_csr(G) → (nodes, idx_of, indptr, indices, probs)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# epc_mc(indptr, indices, probs, deleted_mask, num_samples) → float\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# greedy EC\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgreedy_rebuild\u001b[39m(\n\u001b[0;32m      8\u001b[0m     G: nx\u001b[38;5;241m.\u001b[39mGraph,\n\u001b[0;32m      9\u001b[0m     K: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m---> 10\u001b[0m     sigma_mc: \u001b[43mCallable\u001b[49m[[Set[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m     11\u001b[0m     S_start: Set[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Set[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m     13\u001b[0m     S \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(S_start) \u001b[38;5;28;01mif\u001b[39;00m S_start \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     14\u001b[0m     σ_S \u001b[38;5;241m=\u001b[39m sigma_mc(S)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Callable' is not defined"
     ]
    }
   ],
   "source": [
    "# component_sampling_epc_mc(G, S, num_samples) → float\n",
    "# local_search_swap(S, csr, num_samples, max_iter) → Set[int]\n",
    "# build_csr(G) → (nodes, idx_of, indptr, indices, probs)\n",
    "# epc_mc(indptr, indices, probs, deleted_mask, num_samples) → float\n",
    "\n",
    "# greedy EC\n",
    "def greedy_rebuild(\n",
    "    G: nx.Graph,\n",
    "    K: int,\n",
    "    sigma_mc: Any,\n",
    "    S_start: Set[int] = None\n",
    ") -> Set[int]:\n",
    "    S = set(S_start) if S_start else set()\n",
    "    σ_S = sigma_mc(S)\n",
    "    while len(S) < K:\n",
    "        # compute single‐node gains\n",
    "        best_gain, best_v = -math.inf, None\n",
    "        for v in G.nodes():\n",
    "            if v in S:\n",
    "                continue\n",
    "            gain = σ_S - sigma_mc(S | {v})\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_v = gain, v\n",
    "        S.add(best_v)\n",
    "        σ_S = sigma_mc(S)\n",
    "    return S\n",
    "\n",
    "# destroy-recreate lns\n",
    "def destroy_recreate_lns(\n",
    "    G: nx.Graph,\n",
    "    K: int,\n",
    "    S_init: Set[int],\n",
    "    rho: float,\n",
    "    iterations: int,\n",
    "    sigma_mc: Any\n",
    ") -> Set[int]:\n",
    "    S_best = set(S_init)\n",
    "    score_best = sigma_mc(S_best)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # destroy\n",
    "        to_remove = random.sample(S_best, max(1, int(math.ceil(rho * K))))\n",
    "        S_partial = S_best - set(to_remove)\n",
    "\n",
    "        # recreate\n",
    "        S_new = greedy_rebuild(G, K, sigma_mc, S_partial)\n",
    "        score_new = sigma_mc(S_new)\n",
    "\n",
    "        if score_new < score_best:\n",
    "            S_best, score_best = S_new, score_new\n",
    "\n",
    "    return S_best\n",
    "\n",
    "# tabu search\n",
    "def tabu_search_swap(\n",
    "    S: Set[int],\n",
    "    csr: Tuple[List[int], Dict[int,int], np.ndarray, np.ndarray, np.ndarray],\n",
    "    num_samples: int,\n",
    "    max_iter: int,\n",
    "    tabu_tenure: int\n",
    ") -> Set[int]:\n",
    "    nodes, idx_of, indptr, indices, probs = csr\n",
    "    n = len(nodes)\n",
    "    deleted = np.zeros(n, dtype=bool)\n",
    "    for u in S:\n",
    "        deleted[idx_of[u]] = True\n",
    "\n",
    "    curr = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "    tabu: Dict[int,int] = {}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        best_delta = 0.0\n",
    "        best_move = None\n",
    "\n",
    "        # examine all 1‐for‐1 swaps, skipping tabu nodes\n",
    "        for i in list(S):\n",
    "            if tabu.get(i, 0) > 0:\n",
    "                continue\n",
    "            ii = idx_of[i]\n",
    "            deleted[ii] = False\n",
    "            for j in nodes:\n",
    "                if j in S or tabu.get(j, 0) > 0:\n",
    "                    continue\n",
    "                jj = idx_of[j]\n",
    "                deleted[jj] = True\n",
    "                new_score = epc_mc(indptr, indices, probs, deleted, num_samples)\n",
    "                delta = curr - new_score\n",
    "                if delta > best_delta:\n",
    "                    best_delta = delta\n",
    "                    best_move = (i, j, new_score)\n",
    "                deleted[jj] = False\n",
    "            deleted[ii] = True\n",
    "\n",
    "        # decrement tabu tenures\n",
    "        for node in list(tabu):\n",
    "            tabu[node] -= 1\n",
    "            if tabu[node] <= 0:\n",
    "                del tabu[node]\n",
    "\n",
    "        if best_move is None:\n",
    "            break\n",
    "\n",
    "        # commit best swap\n",
    "        i_out, j_in, curr = best_move\n",
    "        ii, jj = idx_of[i_out], idx_of[j_in]\n",
    "        deleted[ii], deleted[jj] = False, True\n",
    "        S.remove(i_out); S.add(j_in)\n",
    "\n",
    "        # mark both nodes tabu\n",
    "        tabu[i_out] = tabu_tenure\n",
    "        tabu[j_in]  = tabu_tenure\n",
    "\n",
    "    return S\n",
    "\n",
    "def build_csr(G: nx.Graph):\n",
    "\n",
    "    nodes = sorted(G.nodes())\n",
    "    idx_of = {u: i for i, u in enumerate(nodes)}\n",
    "    n = len(nodes)\n",
    "\n",
    "    degs = [len(list(G.neighbors(u))) for u in nodes]\n",
    "    indptr = np.zeros(n + 1, dtype=int)\n",
    "    indptr[1:] = np.cumsum(degs)\n",
    "\n",
    "    indices = np.empty(indptr[-1], dtype=int)\n",
    "    probs = np.empty(indptr[-1], dtype=float)\n",
    "    ptr = 0\n",
    "\n",
    "    for u in nodes:\n",
    "        for v in G.neighbors(u):\n",
    "            indices[ptr] = idx_of[v]\n",
    "            probs[ptr] = G.edges[u, v]['p']\n",
    "            ptr += 1\n",
    "            \n",
    "    return nodes, idx_of, indptr, indices, probs\n",
    "\n",
    "# hybrid approach\n",
    "def lns_tabu_pipeline(\n",
    "    G: nx.Graph,\n",
    "    K: int,\n",
    "    rho: float = 0.3,\n",
    "    lns_iters: int = 20,\n",
    "    mc_samples_construct: int = 500,\n",
    "    mc_samples_ls: int = 2000,\n",
    "    mc_samples_eval: int = 2000,\n",
    "    max_ls_iter: int = 3,\n",
    "    tabu_tenure: int = 5,\n",
    "    tabu_iter: int = 5\n",
    ") -> Tuple[Set[int], float]:\n",
    "    # build CSR once\n",
    "    csr = build_csr(G)\n",
    "    # prepare a cached sigma for fast destroy‐recreate\n",
    "    cache: Dict[frozenset, float] = {}\n",
    "    def sigma_construct(S: Set[int]) -> float:\n",
    "        key = frozenset(S)\n",
    "        if key not in cache:\n",
    "            cache[key] = epc_mc_deleted(G, S, num_samples=mc_samples_construct)\n",
    "        return cache[key]\n",
    "\n",
    "    # initial solution (pure greedy)\n",
    "    S0 = greedy_rebuild(G, K, sigma_construct)\n",
    "\n",
    "    best_S, best_score = set(S0), sigma_construct(S0)\n",
    "\n",
    "    # LNS + LS + Tabu\n",
    "    for _ in range(1): \n",
    "        # destroy‐recreate\n",
    "        S1 = destroy_recreate_lns(G, K, best_S, rho, lns_iters, sigma_construct)\n",
    "        # polish with your 1‐swap local search\n",
    "        S2 = local_search_swap(S1, csr=csr, num_samples=mc_samples_ls, max_iter=max_ls_iter)\n",
    "        # tiny tabu around 1‐swap\n",
    "        S3 = tabu_search_swap(S2, csr=csr, num_samples=mc_samples_ls,\n",
    "                              max_iter=tabu_iter, tabu_tenure=tabu_tenure)\n",
    "        # final evaluation\n",
    "        score3 = epc_mc_deleted(G, S3, num_samples=mc_samples_eval)\n",
    "        if score3 < best_score:\n",
    "            best_S, best_score = set(S3), score3\n",
    "\n",
    "    return best_S, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.erdos_renyi_graph(100, 0.0443, 42)\n",
    "p_edge = 1.0\n",
    "K = 10\n",
    "# alpha = 0.4\n",
    "\n",
    "# assign uniform edge‐survival 0.8\n",
    "for u, v in G.edges():\n",
    "  G[u][v]['p'] = p_edge\n",
    "\n",
    "S_star, score_star = lns_tabu_pipeline(\n",
    "    G.copy(), K=K,\n",
    "    rho=0.3,\n",
    "    lns_iters=30,\n",
    "    mc_samples_construct=1000,\n",
    "    mc_samples_ls=5000,\n",
    "    mc_samples_eval=2000,\n",
    "    max_ls_iter=2,\n",
    "    tabu_tenure=7,\n",
    "    tabu_iter=5\n",
    ")\n",
    "\n",
    "epc_grasp_final = epc_mc_deleted(G.copy(), S_star, 100_000)\n",
    "\n",
    "print(\"Best S:\", S_star)\n",
    "print(\"EPC(S):\", score_star)\n",
    "print(\"Estimated final sigma(S*)  :\", epc_grasp_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
